import{_ as h,C as c,c as d,o as i,a2 as n,b as r,w as e,a as o,G as s,a3 as l}from"./chunks/framework.nRfFlDZQ.js";const g="/sen_doc/assets/mae_page1_fig1.DSPvGypb.png",p="/sen_doc/assets/moco-v2_page1.fatS8kqy.png",u="/sen_doc/assets/dino_page1_fig1.BuQnfeyU.png",m="/sen_doc/assets/mae_page1_fig2.Cfnnlzw9.png",b="/sen_doc/assets/ssl4eo_page2_fig1.Cym1N-uR.jpeg",v="/sen_doc/assets/satmae_page4_fig1.DbcWMUrV.jpeg",f="/sen_doc/assets/prithvi_page3_fig3.xhQhU_FR.jpeg",C=JSON.parse('{"title":"Mục 3.5: Pre-trained Weights và Transfer Learning trong TorchGeo","description":"","frontmatter":{},"headers":[],"relativePath":"chuong-03-kien-truc-model/muc-05-pretrained-weights/01-pretrained-weights.md","filePath":"chuong-03-kien-truc-model/muc-05-pretrained-weights/01-pretrained-weights.md","lastUpdated":1766302189000}'),S={name:"chuong-03-kien-truc-model/muc-05-pretrained-weights/01-pretrained-weights.md"};function _(E,t,k,A,T,P){const a=c("Mermaid");return i(),d("div",null,[t[5]||(t[5]=n('<h1 id="muc-3-5-pre-trained-weights-va-transfer-learning-trong-torchgeo" tabindex="-1">Mục 3.5: Pre-trained Weights và Transfer Learning trong TorchGeo <a class="header-anchor" href="#muc-3-5-pre-trained-weights-va-transfer-learning-trong-torchgeo" aria-label="Permalink to &quot;Mục 3.5: Pre-trained Weights và Transfer Learning trong TorchGeo&quot;">​</a></h1><h2 id="_3-5-1-gioi-thieu" tabindex="-1">3.5.1. Giới Thiệu <a class="header-anchor" href="#_3-5-1-gioi-thieu" aria-label="Permalink to &quot;3.5.1. Giới Thiệu&quot;">​</a></h2><p>Sau khi tìm hiểu các kiến trúc mô hình từ <strong>Mục 3.2-3.4</strong> (classification, segmentation, change detection), phần này trình bày yếu tố then chốt giúp các kiến trúc đó đạt hiệu quả cao: pre-trained weights và transfer learning. Trong deep learning cho viễn thám, việc huấn luyện mô hình từ đầu (training from scratch) đối mặt với hai thách thức lớn: (1) thiếu hụt dữ liệu có nhãn quy mô lớn, và (2) chi phí tính toán cao cho các kiến trúc phức tạp. Pre-trained weights - trọng số đã được huấn luyện trước trên các tập dữ liệu quy mô lớn - cung cấp giải pháp hiệu quả cho cả hai vấn đề này.</p><p>TorchGeo đóng vai trò tiên phong trong việc cung cấp pre-trained weights được thiết kế đặc biệt cho dữ liệu vệ tinh. Không giống như ImageNet weights—vốn được tối ưu cho ảnh tự nhiên RGB—các weights của TorchGeo được huấn luyện trên dữ liệu đa phổ (multispectral) từ các vệ tinh như Sentinel-1, Sentinel-2, giúp mô hình học được các đặc trưng phù hợp với đặc thù của remote sensing imagery.</p><p>Mục này trình bày chi tiết về các pre-trained weights có sẵn trong TorchGeo, phương pháp self-supervised learning được sử dụng để tạo ra chúng, và các foundation models thế hệ mới như Prithvi-EO đang định hình tương lai của lĩnh vực.</p><hr><h2 id="_5-5-2-tai-sao-can-domain-specific-pre-training" tabindex="-1">5.5.2. Tại Sao Cần Domain-Specific Pre-training? <a class="header-anchor" href="#_5-5-2-tai-sao-can-domain-specific-pre-training" aria-label="Permalink to &quot;5.5.2. Tại Sao Cần Domain-Specific Pre-training?&quot;">​</a></h2><h3 id="_5-5-2-1-han-che-cua-imagenet-pre-training" tabindex="-1">5.5.2.1. Hạn Chế của ImageNet Pre-training <a class="header-anchor" href="#_5-5-2-1-han-che-cua-imagenet-pre-training" aria-label="Permalink to &quot;5.5.2.1. Hạn Chế của ImageNet Pre-training&quot;">​</a></h3><p>ImageNet—tập dữ liệu chuẩn cho pre-training trong computer vision—chứa hơn 14 triệu ảnh thuộc 1000 class như chó, mèo, xe cộ. Mặc dù các weights từ ImageNet đã chứng minh khả năng transfer tốt sang nhiều domain, chúng có những hạn chế rõ rệt khi áp dụng cho ảnh vệ tinh:</p><p><strong>Sự khác biệt về modality:</strong></p><ul><li>ImageNet chỉ có 3 kênh RGB</li><li>Sentinel-2 có 13 băng phổ bao gồm near-infrared, shortwave-infrared</li><li>Sentinel-1 SAR có dữ liệu radar hoàn toàn khác về bản chất</li></ul><p><strong>Sự khác biệt về perspective:</strong></p><ul><li>ImageNet: ảnh chụp từ mặt đất, góc nhìn ngang</li><li>Satellite imagery: góc nhìn từ trên xuống (nadir view)</li><li>Các đặc trưng hình học hoàn toàn khác biệt</li></ul><p><strong>Sự khác biệt về phân bố pixel:</strong></p><ul><li>ImageNet: giá trị pixel 0-255, RGB normalized</li><li>Satellite data: giá trị reflectance hoặc backscatter coefficient</li><li>Dynamic range và distribution khác nhau đáng kể</li></ul><h3 id="_5-5-2-2-loi-ich-cua-domain-specific-weights" tabindex="-1">5.5.2.2. Lợi Ích của Domain-Specific Weights <a class="header-anchor" href="#_5-5-2-2-loi-ich-cua-domain-specific-weights" aria-label="Permalink to &quot;5.5.2.2. Lợi Ích của Domain-Specific Weights&quot;">​</a></h3><p>Nghiên cứu trên benchmark EuroSAT cho thấy hiệu quả rõ rệt của domain-specific pre-training:</p><table tabindex="0"><thead><tr><th>Phương pháp Pre-training</th><th>Accuracy trên EuroSAT</th></tr></thead><tbody><tr><td>Random Initialization</td><td>89.2%</td></tr><tr><td>ImageNet Pre-trained</td><td>95.5%</td></tr><tr><td>SSL4EO MoCo</td><td>97.2%</td></tr><tr><td>SSL4EO MAE (ViT)</td><td>97.8%</td></tr></tbody></table><p>Kết quả cho thấy domain-specific weights cải thiện 1.5-2% so với ImageNet, một mức độ đáng kể trong các bài toán benchmark đã gần đạt trần hiệu năng.</p><hr><h2 id="_5-5-3-self-supervised-learning-cho-remote-sensing" tabindex="-1">5.5.3. Self-Supervised Learning cho Remote Sensing <a class="header-anchor" href="#_5-5-3-self-supervised-learning-cho-remote-sensing" aria-label="Permalink to &quot;5.5.3. Self-Supervised Learning cho Remote Sensing&quot;">​</a></h2><p>Self-supervised learning (SSL) đã cách mạng hóa việc pre-training bằng cách tận dụng lượng dữ liệu không nhãn khổng lồ. Trong remote sensing, SSL đặc biệt quan trọng vì dữ liệu vệ tinh có sẵn ở quy mô petabyte nhưng việc tạo nhãn lại tốn kém và đòi hỏi chuyên môn.</p><p><img src="'+g+'" alt="Masked Autoencoder Concept"><em>Hình 5.5.1: Minh họa khái niệm Masked Autoencoder (MAE) - mask ngẫu nhiên 75% patches và reconstruct lại.</em></p><h3 id="_5-5-3-1-momentum-contrast-moco-v2" tabindex="-1">5.5.3.1. Momentum Contrast (MoCo v2) <a class="header-anchor" href="#_5-5-3-1-momentum-contrast-moco-v2" aria-label="Permalink to &quot;5.5.3.1. Momentum Contrast (MoCo v2)&quot;">​</a></h3><p>MoCo v2 là framework contrastive learning sử dụng momentum encoder để tạo large negative sample queue, cho phép học với batch size nhỏ mà vẫn đạt hiệu quả cao.</p><p><strong>Nguyên lý hoạt động:</strong></p>',26)),(i(),r(l,null,{default:e(()=>[s(a,{id:"mermaid-159",class:"mermaid",graph:"graph%20LR%0A%20%20%20%20subgraph%20%22MoCo%20v2%20Framework%22%0A%20%20%20%20%20%20%20%20A%5BInput%20Image%5D%20--%3E%20B%5BQuery%20Encoder%5D%0A%20%20%20%20%20%20%20%20A%20--%3E%20C%5BKey%20Encoder%3Cbr%2F%3EMomentum%20Updated%5D%0A%20%20%20%20%20%20%20%20B%20--%3E%20D%5BQuery%20Embedding%20q%5D%0A%20%20%20%20%20%20%20%20C%20--%3E%20E%5BKey%20Embedding%20k%5D%0A%20%20%20%20%20%20%20%20D%20--%3E%20F%5BContrastive%20Loss%5D%0A%20%20%20%20%20%20%20%20E%20--%3E%20F%0A%20%20%20%20%20%20%20%20G%5BQueue%20of%20Keys%5D%20--%3E%20F%0A%20%20%20%20end%0A"})]),fallback:e(()=>[...t[0]||(t[0]=[o(" Loading... ",-1)])]),_:1})),t[6]||(t[6]=n('<p><strong>Đặc điểm nổi bật:</strong></p><ul><li>Dynamic queue với momentum encoder giúp decouple batch size khỏi số negative samples</li><li>MLP projection head + data augmentation mạnh</li><li>Hiệu quả trên setup 8-GPU tiêu chuẩn</li><li>Đạt 71.1% linear eval accuracy trên ImageNet</li></ul><p><img src="'+p+'" alt="MoCo v2 Improvements"></p><p><em>Hình 5.5.1b: MoCo v2 improvements so với v1 - thêm MLP head và stronger augmentations [Chen et al., 2020]</em></p><p><strong>Ứng dụng trong TorchGeo:</strong> MoCo v2 là nền tảng cho SSL4EO weights trên cả Sentinel-1 và Sentinel-2, cung cấp các weights ResNet cho các task classification và segmentation.</p><h3 id="_5-5-3-2-self-distillation-with-no-labels-dino" tabindex="-1">5.5.3.2. Self-Distillation with No Labels (DINO) <a class="header-anchor" href="#_5-5-3-2-self-distillation-with-no-labels-dino" aria-label="Permalink to &quot;5.5.3.2. Self-Distillation with No Labels (DINO)&quot;">​</a></h3><p>DINO sử dụng framework student-teacher với self-distillation, tạo ra các đặc trưng có semantic meaning mà không cần nhãn thủ công.</p><p><strong>Kiến trúc DINO:</strong></p>',8)),(i(),r(l,null,{default:e(()=>[s(a,{id:"mermaid-203",class:"mermaid",graph:"graph%20TB%0A%20%20%20%20subgraph%20%22DINO%20Training%22%0A%20%20%20%20%20%20%20%20A%5BImage%20x%5D%20--%3E%20B%5BGlobal%20Crops%5D%0A%20%20%20%20%20%20%20%20A%20--%3E%20C%5BLocal%20Crops%5D%0A%20%20%20%20%20%20%20%20B%20--%3E%20D%5BTeacher%20Network%5D%0A%20%20%20%20%20%20%20%20B%20--%3E%20E%5BStudent%20Network%5D%0A%20%20%20%20%20%20%20%20C%20--%3E%20E%0A%20%20%20%20%20%20%20%20D%20--%3E%20F%5BTeacher%20Output%5D%0A%20%20%20%20%20%20%20%20E%20--%3E%20G%5BStudent%20Output%5D%0A%20%20%20%20%20%20%20%20F%20--%3E%20H%5BCross-Entropy%20Loss%5D%0A%20%20%20%20%20%20%20%20G%20--%3E%20H%0A%20%20%20%20%20%20%20%20D%20-.-%3E%20%7CMomentum%20Update%7C%20E%0A%20%20%20%20end%0A"})]),fallback:e(()=>[...t[1]||(t[1]=[o(" Loading... ",-1)])]),_:1})),t[7]||(t[7]=n('<p><strong>Điểm mạnh:</strong></p><ul><li>Multi-crop strategy: 2 global views + nhiều local views</li><li>Student xử lý tất cả crops; teacher chỉ xử lý global views</li><li>Centering và sharpening ngăn collapse</li><li>Learned features có implicit semantic segmentation</li></ul><p><img src="'+u+'" alt="DINO Self-Attention"></p><p><em>Hình 5.5.1c: DINO self-attention maps - cho thấy model tự động học implicit semantic segmentation mà không cần nhãn [Caron et al., 2021]</em></p><p><strong>Biến thể cho Remote Sensing:</strong></p><ul><li><strong>DINO-MC:</strong> Xử lý varying object sizes trong ảnh vệ tinh</li><li><strong>SatDINO:</strong> Thêm Ground Sample Distance (GSD) encoding và adaptive view sampling</li></ul><h3 id="_5-5-3-3-masked-autoencoder-mae" tabindex="-1">5.5.3.3. Masked Autoencoder (MAE) <a class="header-anchor" href="#_5-5-3-3-masked-autoencoder-mae" aria-label="Permalink to &quot;5.5.3.3. Masked Autoencoder (MAE)&quot;">​</a></h3><p>MAE là phương pháp self-supervised reconstruction dựa trên việc mask và reconstruct lại các patches của ảnh. Với tỷ lệ masking cao (75%), MAE buộc mô hình học các đặc trưng có ý nghĩa để có thể dự đoán nội dung bị ẩn.</p><p><strong>Kiến trúc MAE:</strong></p>',9)),(i(),r(l,null,{default:e(()=>[s(a,{id:"mermaid-259",class:"mermaid",graph:"graph%20LR%0A%20%20%20%20subgraph%20%22MAE%20Architecture%22%0A%20%20%20%20%20%20%20%20A%5BInput%20Image%5D%20--%3E%20B%5BPatch%20%2B%20Mask%3Cbr%2F%3E75%25%20masked%5D%0A%20%20%20%20%20%20%20%20B%20--%3E%20C%5BVisible%20Patches%3Cbr%2F%3E25%25%5D%0A%20%20%20%20%20%20%20%20C%20--%3E%20D%5BEncoder%3Cbr%2F%3EViT%5D%0A%20%20%20%20%20%20%20%20D%20--%3E%20E%5BLatent%20%2B%20Mask%20Tokens%5D%0A%20%20%20%20%20%20%20%20E%20--%3E%20F%5BLightweight%20Decoder%5D%0A%20%20%20%20%20%20%20%20F%20--%3E%20G%5BReconstructed%20Image%5D%0A%20%20%20%20end%0A"})]),fallback:e(()=>[...t[2]||(t[2]=[o(" Loading... ",-1)])]),_:1})),t[8]||(t[8]=n('<p><strong>Hiệu quả:</strong></p><ul><li>Encoder chỉ xử lý visible patches (25%) → tiết kiệm compute</li><li>Decoder nhẹ để reconstruct full image</li><li>Strong benchmark results trên ImageNet</li><li>Effective transfer to downstream tasks</li></ul><p><img src="'+m+'" alt="MAE Reconstruction"></p><p><em>Hình 5.5.1d: MAE reconstruction example - từ 25% visible patches reconstruct lại 75% masked patches [He et al., 2022]</em></p><p><strong>Relevance cho Remote Sensing:</strong> MAE đặc biệt phù hợp với RS vì lượng dữ liệu không nhãn khổng lồ. Variants như SatMAE mở rộng thêm temporal và multi-spectral capabilities.</p><hr><h2 id="_5-5-4-ssl4eo-benchmark-dataset-va-pre-trained-weights" tabindex="-1">5.5.4. SSL4EO: Benchmark Dataset và Pre-trained Weights <a class="header-anchor" href="#_5-5-4-ssl4eo-benchmark-dataset-va-pre-trained-weights" aria-label="Permalink to &quot;5.5.4. SSL4EO: Benchmark Dataset và Pre-trained Weights&quot;">​</a></h2><h3 id="_5-5-4-1-tong-quan-ssl4eo-s12" tabindex="-1">5.5.4.1. Tổng Quan SSL4EO-S12 <a class="header-anchor" href="#_5-5-4-1-tong-quan-ssl4eo-s12" aria-label="Permalink to &quot;5.5.4.1. Tổng Quan SSL4EO-S12&quot;">​</a></h3><p>SSL4EO-S12 (Self-Supervised Learning for Earth Observation on Sentinel-1/2) là initiative quan trọng cung cấp cả dataset và pre-trained weights cho cộng đồng remote sensing.</p><p><img src="'+b+'" alt="SSL4EO Dataset Overview"><em>Hình 5.5.2: Tổng quan dataset SSL4EO với phạm vi phủ toàn cầu và đa thời điểm.</em></p><p><strong>Thành phần chính:</strong></p><ul><li><strong>Dataset:</strong> 200,000+ image triplets từ Sentinel-1 và Sentinel-2</li><li><strong>Coverage:</strong> European và global coverage với seasonal variations</li><li><strong>Methods:</strong> Benchmarks bốn SSL methods: MoCo-v2, DINO, MAE, data2vec</li><li><strong>Availability:</strong> Public dataset, code, và pre-trained models</li></ul><h3 id="_5-5-4-2-cac-weights-co-san" tabindex="-1">5.5.4.2. Các Weights Có Sẵn <a class="header-anchor" href="#_5-5-4-2-cac-weights-co-san" aria-label="Permalink to &quot;5.5.4.2. Các Weights Có Sẵn&quot;">​</a></h3><p><strong>MoCo Pre-trained Weights:</strong></p><table tabindex="0"><thead><tr><th>Model</th><th>Input Sensor</th><th>Weight Identifier</th></tr></thead><tbody><tr><td>ResNet-18</td><td>Sentinel-2 All Bands</td><td><code>SENTINEL2_ALL_MOCO</code></td></tr><tr><td>ResNet-50</td><td>Sentinel-2 All Bands</td><td><code>SENTINEL2_ALL_MOCO</code></td></tr><tr><td>ResNet-50</td><td>Sentinel-2 RGB Only</td><td><code>SENTINEL2_RGB_MOCO</code></td></tr><tr><td>ResNet-50</td><td>Sentinel-1 All Bands</td><td><code>SENTINEL1_ALL_MOCO</code></td></tr></tbody></table><p><strong>MAE và DINO Weights:</strong></p><table tabindex="0"><thead><tr><th>Model</th><th>Method</th><th>Input Sensor</th><th>Weight Identifier</th></tr></thead><tbody><tr><td>ViT-Small</td><td>MAE</td><td>Sentinel-2</td><td><code>SENTINEL2_ALL_MAE</code></td></tr><tr><td>ViT-Base</td><td>MAE</td><td>Sentinel-2</td><td><code>SENTINEL2_ALL_MAE</code></td></tr><tr><td>ViT-Small</td><td>DINO</td><td>Sentinel-2</td><td><code>SENTINEL2_ALL_DINO</code></td></tr><tr><td>ViT-Base</td><td>DINO</td><td>Sentinel-2</td><td><code>SENTINEL2_ALL_DINO</code></td></tr></tbody></table><h3 id="_5-5-4-3-downstream-performance" tabindex="-1">5.5.4.3. Downstream Performance <a class="header-anchor" href="#_5-5-4-3-downstream-performance" aria-label="Permalink to &quot;5.5.4.3. Downstream Performance&quot;">​</a></h3><p>Kết quả benchmark cho thấy SSL4EO weights đạt hoặc vượt supervised ImageNet baseline:</p><table tabindex="0"><thead><tr><th>Task</th><th>Dataset</th><th>SSL4EO Performance</th></tr></thead><tbody><tr><td>Scene Classification</td><td>EuroSAT</td><td>97.2-97.8%</td></tr><tr><td>Land Cover</td><td>BigEarthNet</td><td>Competitive</td></tr><tr><td>Object Detection</td><td>xView</td><td>Improved mAP</td></tr></tbody></table><p><strong>Key Insight:</strong> Generic SSL methods (MoCo, MAE, DINO) transfer effectively to RS domain khi được huấn luyện trên dữ liệu phù hợp.</p><hr><h2 id="_5-5-5-satmae-domain-adapted-masked-autoencoder" tabindex="-1">5.5.5. SatMAE: Domain-Adapted Masked Autoencoder <a class="header-anchor" href="#_5-5-5-satmae-domain-adapted-masked-autoencoder" aria-label="Permalink to &quot;5.5.5. SatMAE: Domain-Adapted Masked Autoencoder&quot;">​</a></h2><h3 id="_5-5-5-1-innovations-cua-satmae" tabindex="-1">5.5.5.1. Innovations của SatMAE <a class="header-anchor" href="#_5-5-5-1-innovations-cua-satmae" aria-label="Permalink to &quot;5.5.5.1. Innovations của SatMAE&quot;">​</a></h3><p>SatMAE mở rộng MAE với các adaptation đặc biệt cho satellite imagery:</p><p><img src="'+v+'" alt="SatMAE Architecture"><em>Hình 5.5.3: Kiến trúc SatMAE với temporal encoding và multi-spectral handling.</em></p><p><strong>Các cải tiến chính:</strong></p><ol><li><strong>Temporal Embedding:</strong> Encode thông tin thời gian cho time-series satellite data</li><li><strong>Independent Masking:</strong> Masking độc lập qua temporal dimension</li><li><strong>Spectral-aware Positional Encoding:</strong> Xử lý riêng biệt các nhóm băng phổ</li><li><strong>Multi-spectral Support:</strong> Hỗ trợ native cho N-band imagery</li></ol><h3 id="_5-5-5-2-transfer-learning-performance" tabindex="-1">5.5.5.2. Transfer Learning Performance <a class="header-anchor" href="#_5-5-5-2-transfer-learning-performance" aria-label="Permalink to &quot;5.5.5.2. Transfer Learning Performance&quot;">​</a></h3><p>SatMAE đạt cải thiện đáng kể so với generic MAE:</p><table tabindex="0"><thead><tr><th>Task</th><th>Metric</th><th>Improvement</th></tr></thead><tbody><tr><td>Land Cover Classification</td><td>Accuracy</td><td>+14% vs baseline</td></tr><tr><td>Supervised Learning Comparison</td><td>Various</td><td>+7% vs SOTA</td></tr><tr><td>Semantic Segmentation</td><td>mIoU</td><td>Significant gains</td></tr></tbody></table><h3 id="_5-5-5-3-temporal-capabilities" tabindex="-1">5.5.5.3. Temporal Capabilities <a class="header-anchor" href="#_5-5-5-3-temporal-capabilities" aria-label="Permalink to &quot;5.5.5.3. Temporal Capabilities&quot;">​</a></h3><p>SatMAE đặc biệt mạnh cho các task yêu cầu phân tích temporal:</p><ul><li><strong>Change Detection:</strong> Exploit temporal differences</li><li><strong>Crop Monitoring:</strong> Track phenological stages</li><li><strong>Seasonal Analysis:</strong> Capture seasonal patterns</li></ul><hr><h2 id="_5-5-6-foundation-models-prithvi-eo" tabindex="-1">5.5.6. Foundation Models: Prithvi-EO <a class="header-anchor" href="#_5-5-6-foundation-models-prithvi-eo" aria-label="Permalink to &quot;5.5.6. Foundation Models: Prithvi-EO&quot;">​</a></h2><h3 id="_5-5-6-1-gioi-thieu-prithvi" tabindex="-1">5.5.6.1. Giới Thiệu Prithvi <a class="header-anchor" href="#_5-5-6-1-gioi-thieu-prithvi" aria-label="Permalink to &quot;5.5.6.1. Giới Thiệu Prithvi&quot;">​</a></h3><p>Prithvi-EO, được phát triển bởi IBM và NASA, đại diện cho bước tiến lớn trong geospatial AI. Với kiến trúc transformer quy mô lớn (100M-600M parameters) được pre-train trên hơn 1TB dữ liệu multispectral, Prithvi đạt state-of-the-art trên nhiều downstream tasks.</p><p><img src="'+f+'" alt="Prithvi Architecture"></p><p><em>Hình 5.5.4: Minh họa multi-temporal satellite patches làm input cho Prithvi model [Jakubik et al., 2023]</em></p><p><strong>Đặc điểm nổi bật:</strong></p><ul><li><strong>Scale:</strong> 300M (ViT-L) và 600M (ViT-H) parameter variants</li><li><strong>Data:</strong> Pre-trained on NASA Harmonized Landsat-Sentinel (HLS) data</li><li><strong>Temporal:</strong> Native support cho video format input (B,C,T,H,W)</li><li><strong>Open Source:</strong> Available via Hugging Face; integrated với TerraTorch</li></ul><h3 id="_5-5-6-2-kien-truc-chi-tiet" tabindex="-1">5.5.6.2. Kiến Trúc Chi Tiết <a class="header-anchor" href="#_5-5-6-2-kien-truc-chi-tiet" aria-label="Permalink to &quot;5.5.6.2. Kiến Trúc Chi Tiết&quot;">​</a></h3>',43)),(i(),r(l,null,{default:e(()=>[s(a,{id:"mermaid-698",class:"mermaid",graph:"graph%20TB%0A%20%20%20%20subgraph%20%22Prithvi-EO%20Architecture%22%0A%20%20%20%20%20%20%20%20A%5BMulti-temporal%20Input%3Cbr%2F%3EB%2CC%2CT%2CH%2CW%5D%20--%3E%20B%5B3D%20Patch%20Embedding%5D%0A%20%20%20%20%20%20%20%20B%20--%3E%20C%5BSpatial%20%2B%20Temporal%3Cbr%2F%3EPositional%20Encoding%5D%0A%20%20%20%20%20%20%20%20D%5BLat%2FLon%20%2B%20Date%5D%20--%3E%20E%5B2D%20Sin%2FCos%20Encoding%5D%0A%20%20%20%20%20%20%20%20C%20--%3E%20F%5BVision%20Transformer%3Cbr%2F%3EViT-L%2FViT-H%5D%0A%20%20%20%20%20%20%20%20E%20--%3E%20F%0A%20%20%20%20%20%20%20%20F%20--%3E%20G%5BMAE%20Decoder%5D%0A%20%20%20%20%20%20%20%20G%20--%3E%20H%5BReconstructed%20Output%5D%0A%20%20%20%20end%0A"})]),fallback:e(()=>[...t[3]||(t[3]=[o(" Loading... ",-1)])]),_:1})),t[9]||(t[9]=n('<p><strong>Geospatial Context:</strong></p><ul><li>Lat/lon và date metadata được encode qua 2D sin/cos encoding</li><li>Weighted sum fusion integrate location và temporal information</li><li>Drop mechanism handles missing data during pre-training</li></ul><h3 id="_5-5-6-3-benchmark-results" tabindex="-1">5.5.6.3. Benchmark Results <a class="header-anchor" href="#_5-5-6-3-benchmark-results" aria-label="Permalink to &quot;5.5.6.3. Benchmark Results&quot;">​</a></h3><p>Prithvi-EO đạt kết quả ấn tượng trên GEO-Bench evaluation:</p><table tabindex="0"><thead><tr><th>Task</th><th>Dataset</th><th>Prithvi-EO v1</th><th>Prithvi-EO 600M</th></tr></thead><tbody><tr><td>Flood Detection</td><td>HLS</td><td>79.6% IoU</td><td>83.1% IoU</td></tr><tr><td>Wildfire Scars</td><td>HLS</td><td>76.8% IoU</td><td>83.2% IoU</td></tr><tr><td>Crop Segmentation</td><td>Various</td><td>-</td><td>50.7% mIoU</td></tr><tr><td>GPP Estimation</td><td>MERRA-2</td><td>baseline</td><td>+20% R²</td></tr></tbody></table><p><strong>Kết luận:</strong> Prithvi-EO 600M outperforms 6+ competing foundation models across resolutions 0.1m-15m.</p><h3 id="_5-5-6-4-multi-modal-capabilities" tabindex="-1">5.5.6.4. Multi-modal Capabilities <a class="header-anchor" href="#_5-5-6-4-multi-modal-capabilities" aria-label="Permalink to &quot;5.5.6.4. Multi-modal Capabilities&quot;">​</a></h3><p>Prithvi có khả năng mở rộng đa modal:</p><ul><li><strong>Primary:</strong> Optical HLS (30m resolution, 10-year coverage)</li><li><strong>SAR Integration:</strong> Demonstrated với Radar Vegetation Index</li><li><strong>Atmospheric Data:</strong> MERRA-2 variables cho GPP prediction</li><li><strong>Flexibility:</strong> Adaptable architecture cho various sensor types</li></ul><hr><h2 id="_5-5-7-sensor-specific-weights-va-best-practices" tabindex="-1">5.5.7. Sensor-Specific Weights và Best Practices <a class="header-anchor" href="#_5-5-7-sensor-specific-weights-va-best-practices" aria-label="Permalink to &quot;5.5.7. Sensor-Specific Weights và Best Practices&quot;">​</a></h2><h3 id="_5-5-7-1-sentinel-2-weights" tabindex="-1">5.5.7.1. Sentinel-2 Weights <a class="header-anchor" href="#_5-5-7-1-sentinel-2-weights" aria-label="Permalink to &quot;5.5.7.1. Sentinel-2 Weights&quot;">​</a></h3><p><strong>Các options có sẵn:</strong></p><table tabindex="0"><thead><tr><th>Pre-training Method</th><th>Model</th><th>Use Case</th></tr></thead><tbody><tr><td>SSL4EO MoCo</td><td>ResNet</td><td>General classification</td></tr><tr><td>SSL4EO MAE</td><td>ViT</td><td>High performance tasks</td></tr><tr><td>SSL4EO DINO</td><td>ViT</td><td>Semantic-rich features</td></tr><tr><td>SatMAE</td><td>ViT</td><td>Temporal analysis</td></tr><tr><td>BigEarthNet Supervised</td><td>Various</td><td>Quick baseline</td></tr></tbody></table><p><strong>Band Configuration:</strong></p><ul><li><strong>All 13 bands:</strong> Complete spectral information</li><li><strong>RGB only (B4, B3, B2):</strong> 10m resolution, ImageNet-compatible</li><li><strong>RGB + NIR (B4, B3, B2, B8):</strong> Vegetation analysis</li></ul><h3 id="_5-5-7-2-sentinel-1-sar-weights" tabindex="-1">5.5.7.2. Sentinel-1 SAR Weights <a class="header-anchor" href="#_5-5-7-2-sentinel-1-sar-weights" aria-label="Permalink to &quot;5.5.7.2. Sentinel-1 SAR Weights&quot;">​</a></h3><p><strong>Đặc thù của SAR data:</strong></p><ul><li>Polarization channels: VV, VH</li><li>Log-scale normalization required</li><li>Different value distribution than optical</li><li>Speckle noise considerations</li></ul><p><strong>Recommended:</strong></p><ul><li>SSL4EO Sentinel-1 MoCo weights</li><li>Match polarization configuration với training data</li></ul><h3 id="_5-5-7-3-high-resolution-imagery" tabindex="-1">5.5.7.3. High-Resolution Imagery <a class="header-anchor" href="#_5-5-7-3-high-resolution-imagery" aria-label="Permalink to &quot;5.5.7.3. High-Resolution Imagery&quot;">​</a></h3><p>Cho aerial và very-high-resolution satellite:</p><ul><li><strong>NAIP-specific:</strong> Domain-adapted weights</li><li><strong>ImageNet:</strong> Often sufficient cho RGB</li><li><strong>Million-AID:</strong> Pre-training cho aerial imagery</li></ul><hr><h2 id="_5-5-8-fine-tuning-strategies" tabindex="-1">5.5.8. Fine-tuning Strategies <a class="header-anchor" href="#_5-5-8-fine-tuning-strategies" aria-label="Permalink to &quot;5.5.8. Fine-tuning Strategies&quot;">​</a></h2><h3 id="_5-5-8-1-strategy-selection-theo-data-amount" tabindex="-1">5.5.8.1. Strategy Selection theo Data Amount <a class="header-anchor" href="#_5-5-8-1-strategy-selection-theo-data-amount" aria-label="Permalink to &quot;5.5.8.1. Strategy Selection theo Data Amount&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Data Amount</th><th>Recommended Strategy</th></tr></thead><tbody><tr><td>&lt; 100 samples</td><td>Linear Probing only</td></tr><tr><td>100 - 1,000</td><td>Progressive Unfreezing</td></tr><tr><td>1,000 - 10,000</td><td>Full Fine-tuning với regularization</td></tr><tr><td>&gt; 10,000</td><td>Full Fine-tuning</td></tr></tbody></table><h3 id="_5-5-8-2-linear-probing" tabindex="-1">5.5.8.2. Linear Probing <a class="header-anchor" href="#_5-5-8-2-linear-probing" aria-label="Permalink to &quot;5.5.8.2. Linear Probing&quot;">​</a></h3><p>Freeze toàn bộ backbone, chỉ train classifier head:</p><ul><li>Nhanh và stable</li><li>Test feature quality</li><li>Baseline cho so sánh</li></ul><h3 id="_5-5-8-3-progressive-unfreezing" tabindex="-1">5.5.8.3. Progressive Unfreezing <a class="header-anchor" href="#_5-5-8-3-progressive-unfreezing" aria-label="Permalink to &quot;5.5.8.3. Progressive Unfreezing&quot;">​</a></h3><p>Dần dần unfreeze từ top layers xuống:</p><ol><li>Train classifier only (1-2 epochs)</li><li>Unfreeze last block (2-3 epochs)</li><li>Unfreeze more blocks progressively</li><li>Full fine-tuning (remaining epochs)</li></ol><p><strong>Lợi ích:</strong></p><ul><li>Preserves pre-trained features</li><li>Stable training trajectory</li><li>Phù hợp cho limited data scenarios</li></ul><h3 id="_5-5-8-4-layer-wise-learning-rates" tabindex="-1">5.5.8.4. Layer-wise Learning Rates <a class="header-anchor" href="#_5-5-8-4-layer-wise-learning-rates" aria-label="Permalink to &quot;5.5.8.4. Layer-wise Learning Rates&quot;">​</a></h3><p>Áp dụng learning rates khác nhau cho different depths:</p><ul><li>Lower LR cho early layers (pre-trained features)</li><li>Higher LR cho later layers và new heads</li><li>Typical ratio: 10x between groups</li></ul><hr><h2 id="_5-5-9-practical-considerations" tabindex="-1">5.5.9. Practical Considerations <a class="header-anchor" href="#_5-5-9-practical-considerations" aria-label="Permalink to &quot;5.5.9. Practical Considerations&quot;">​</a></h2><h3 id="_5-5-9-1-channel-mismatch-handling" tabindex="-1">5.5.9.1. Channel Mismatch Handling <a class="header-anchor" href="#_5-5-9-1-channel-mismatch-handling" aria-label="Permalink to &quot;5.5.9.1. Channel Mismatch Handling&quot;">​</a></h3><p>Khi input channels khác với pre-trained weights:</p><p><strong>Fewer channels than pre-training:</strong></p><ul><li>Select matching weight channels</li><li>Average redundant channel weights</li></ul><p><strong>More channels than pre-training:</strong></p><ul><li>Duplicate và tile existing weights</li><li>Initialize extra channels với random weights</li><li>TorchGeo <code>in_chans</code> parameter tự động handle</li></ul><h3 id="_5-5-9-2-model-size-va-deployment" tabindex="-1">5.5.9.2. Model Size và Deployment <a class="header-anchor" href="#_5-5-9-2-model-size-va-deployment" aria-label="Permalink to &quot;5.5.9.2. Model Size và Deployment&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Model</th><th>Parameters</th><th>Memory</th><th>Use Case</th></tr></thead><tbody><tr><td>ResNet-18</td><td>11M</td><td>~200MB</td><td>Edge deployment</td></tr><tr><td>ResNet-50</td><td>25M</td><td>~400MB</td><td>Production server</td></tr><tr><td>ViT-Base</td><td>86M</td><td>~700MB</td><td>High accuracy</td></tr><tr><td>ViT-Large</td><td>307M</td><td>~2.5GB</td><td>Research</td></tr></tbody></table><h3 id="_5-5-9-3-common-mistakes-to-avoid" tabindex="-1">5.5.9.3. Common Mistakes to Avoid <a class="header-anchor" href="#_5-5-9-3-common-mistakes-to-avoid" aria-label="Permalink to &quot;5.5.9.3. Common Mistakes to Avoid&quot;">​</a></h3><ol><li><strong>Sử dụng ImageNet weights</strong> trực tiếp cho multi-spectral data mà không adaptation</li><li><strong>Bỏ qua normalization</strong> differences giữa sensors</li><li><strong>Full fine-tuning với insufficient data</strong> → overfitting</li><li><strong>Wrong channel configuration</strong> không match với weights</li><li><strong>Mismatched transforms</strong> giữa pre-training và fine-tuning</li></ol><hr><h2 id="_5-5-10-torchgeo-prithvi-integration-workflow" tabindex="-1">5.5.10. TorchGeo + Prithvi Integration Workflow <a class="header-anchor" href="#_5-5-10-torchgeo-prithvi-integration-workflow" aria-label="Permalink to &quot;5.5.10. TorchGeo + Prithvi Integration Workflow&quot;">​</a></h2><p>Việc kết hợp TorchGeo (infrastructure) với Prithvi (foundation model) tạo thành pipeline hoàn chỉnh:</p>',54)),(i(),r(l,null,{default:e(()=>[s(a,{id:"mermaid-1308",class:"mermaid",graph:"graph%20TB%0A%20%20%20%20subgraph%20%22Complete%20Workflow%22%0A%20%20%20%20%20%20%20%20A%5B1.%20Data%20Ingestion%3Cbr%2F%3ETorchGeo%20Datasets%5D%20--%3E%20B%5B2.%20Preprocessing%3Cbr%2F%3ETorchGeo%20Transforms%5D%0A%20%20%20%20%20%20%20%20B%20--%3E%20C%5B3.%20Model%20Selection%3Cbr%2F%3EPrithvi%20from%20HuggingFace%5D%0A%20%20%20%20%20%20%20%20C%20--%3E%20D%5B4.%20Fine-tuning%3Cbr%2F%3ETerraTorch%20Toolkit%5D%0A%20%20%20%20%20%20%20%20D%20--%3E%20E%5B5.%20Evaluation%3Cbr%2F%3ETorchGeo%20Benchmarks%5D%0A%20%20%20%20end%0A"})]),fallback:e(()=>[...t[4]||(t[4]=[o(" Loading... ",-1)])]),_:1})),t[10]||(t[10]=n('<p><strong>Quy trình chi tiết:</strong></p><ol><li><strong>TorchGeo</strong> load và preprocess geospatial data với on-the-fly transforms</li><li><strong>Prithvi</strong> cung cấp pre-trained backbone đã học rich representations</li><li><strong>TerraTorch</strong> simplifies task-specific adaptation với minimal code</li><li><strong>Evaluation</strong> trên standard benchmarks đảm bảo reproducibility</li></ol><hr><h2 id="_5-5-11-tuong-lai-cua-pre-trained-weights" tabindex="-1">5.5.11. Tương Lai của Pre-trained Weights <a class="header-anchor" href="#_5-5-11-tuong-lai-cua-pre-trained-weights" aria-label="Permalink to &quot;5.5.11. Tương Lai của Pre-trained Weights&quot;">​</a></h2><h3 id="_5-5-11-1-xu-huong-phat-trien" tabindex="-1">5.5.11.1. Xu Hướng Phát Triển <a class="header-anchor" href="#_5-5-11-1-xu-huong-phat-trien" aria-label="Permalink to &quot;5.5.11.1. Xu Hướng Phát Triển&quot;">​</a></h3><p><strong>Mở rộng sensor support:</strong></p><ul><li>MODIS, VIIRS cho global monitoring</li><li>Pléiades, WorldView cho very-high-resolution</li><li>Commercial SAR: ICEYE, Capella</li><li>Hyperspectral sensors</li></ul><p><strong>Foundation models lớn hơn:</strong></p><ul><li>Billion-parameter models</li><li>Multi-modal pre-training (optical + SAR + text)</li><li>Zero-shot và few-shot capabilities</li></ul><p><strong>Efficiency improvements:</strong></p><ul><li>Knowledge distillation cho smaller models</li><li>Efficient architectures (MobileViT, EfficientViT)</li><li>Progressive và curriculum training</li></ul><h3 id="_5-5-11-2-open-questions" tabindex="-1">5.5.11.2. Open Questions <a class="header-anchor" href="#_5-5-11-2-open-questions" aria-label="Permalink to &quot;5.5.11.2. Open Questions&quot;">​</a></h3><p>Một số câu hỏi còn mở trong lĩnh vực:</p><ul><li>Optimal masking ratio cho multi-spectral Sentinel data?</li><li>Geo-location clustering scaling cho global high-resolution datasets?</li><li>Transfer performance từ Sentinel sang non-Sentinel sensors?</li><li>Edge deployment strategies cho foundation models?</li></ul><hr><h2 id="_3-5-12-ket-luan-muc-va-chuong" tabindex="-1">3.5.12. Kết Luận Mục và Chương <a class="header-anchor" href="#_3-5-12-ket-luan-muc-va-chuong" aria-label="Permalink to &quot;3.5.12. Kết Luận Mục và Chương&quot;">​</a></h2><p>Pre-trained weights đại diện cho một trong những đóng góp quan trọng nhất của TorchGeo cho cộng đồng remote sensing. Từ <strong>SSL4EO</strong> với các phương pháp MoCo, MAE, DINO đến <strong>foundation models</strong> như Prithvi-EO, các weights này cho phép practitioners đạt được hiệu năng cao với lượng dữ liệu có nhãn hạn chế.</p><p><strong>Những điểm then chốt:</strong></p><ol><li><strong>Domain-specific weights</strong> vượt trội ImageNet weights 1.5-2% trên RS tasks</li><li><strong>Self-supervised learning</strong> tận dụng hiệu quả unlabeled satellite data (petabyte-scale)</li><li><strong>Foundation models</strong> như Prithvi (600M params) đang mở ra paradigm mới</li><li><strong>Fine-tuning strategy</strong> cần phù hợp với data amount và task complexity</li><li><strong>TorchGeo ecosystem</strong> cung cấp complete pipeline từ data đến deployment</li></ol><hr><h2 id="ket-chuong-3" tabindex="-1">Kết Chương 3 <a class="header-anchor" href="#ket-chuong-3" aria-label="Permalink to &quot;Kết Chương 3&quot;">​</a></h2><p><strong>Chương 3</strong> đã trình bày toàn diện hệ sinh thái TorchGeo và các kiến trúc mô hình cho remote sensing:</p><ul><li><strong>Mục 3.1:</strong> TorchGeo framework với GeoDataset, samplers, transforms cho dữ liệu địa không gian</li><li><strong>Mục 3.2:</strong> Backbone networks (ResNet, ViT, Swin, EfficientNet) cho classification</li><li><strong>Mục 3.3:</strong> Segmentation architectures (U-Net, DeepLabV3+, FPN, PSPNet, HRNet)</li><li><strong>Mục 3.4:</strong> Change detection models (FC-Siam, BIT-Transformer, STANet)</li><li><strong>Mục 3.5:</strong> Pre-trained weights (SSL4EO, SatMAE, Prithvi) và transfer learning strategies</li></ul><p>Các kiến trúc mô hình được trình bày trong chương này sẽ được minh họa qua các giải pháp chiến thắng trong <strong>Chương 4 - xView Challenges</strong>, và ứng dụng thực tế trong <strong>Chương 5</strong> (phát hiện tàu biển) và <strong>Chương 6</strong> (phát hiện dầu loang).</p><hr><h2 id="tai-lieu-tham-khao" tabindex="-1">Tài Liệu Tham Khảo <a class="header-anchor" href="#tai-lieu-tham-khao" aria-label="Permalink to &quot;Tài Liệu Tham Khảo&quot;">​</a></h2><p>[1] Chen, X., et al. &quot;Improved Baselines with Momentum Contrastive Learning.&quot; arXiv:2003.04297, 2020.</p><p>[2] Caron, M., et al. &quot;Emerging Properties in Self-Supervised Vision Transformers.&quot; ICCV, 2021.</p><p>[3] He, K., et al. &quot;Masked Autoencoders Are Scalable Vision Learners.&quot; CVPR, 2022.</p><p>[4] Wang, Y., et al. &quot;SSL4EO-S12: A Large-Scale Multi-Modal, Multi-Temporal Dataset for Self-Supervised Learning in Earth Observation.&quot; IEEE GRSM, 2023.</p><p>[5] Cong, Y., et al. &quot;SatMAE: Pre-training Transformers for Temporal and Multi-Spectral Satellite Imagery.&quot; NeurIPS, 2022.</p><p>[6] Ayush, K., et al. &quot;Geography-Aware Self-Supervised Learning.&quot; ICCV, 2021.</p><p>[7] Jakubik, J., et al. &quot;Foundation Models for Generalist Geospatial Artificial Intelligence.&quot; arXiv:2310.18660, 2023.</p><p>[8] Stewart, A., et al. &quot;TorchGeo: Deep Learning with Geospatial Data.&quot; ACM SIGSPATIAL, 2022.</p>',34))])}const q=h(S,[["render",_]]);export{C as __pageData,q as default};
