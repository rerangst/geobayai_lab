import{_ as e,c as t,o as i,a2 as s}from"./chunks/framework.nRfFlDZQ.js";const p=JSON.parse('{"title":"xView2/xBD Dataset Image Sources & Documentation","description":"","frontmatter":{},"headers":[],"relativePath":"assets/images/chuong-04-xview2/image-sources.md","filePath":"assets/images/chuong-04-xview2/image-sources.md","lastUpdated":1766302189000}'),r={name:"assets/images/chuong-04-xview2/image-sources.md"};function l(n,a,o,d,h,c){return i(),t("div",null,[...a[0]||(a[0]=[s(`<h1 id="xview2-xbd-dataset-image-sources-documentation" tabindex="-1">xView2/xBD Dataset Image Sources &amp; Documentation <a class="header-anchor" href="#xview2-xbd-dataset-image-sources-documentation" aria-label="Permalink to &quot;xView2/xBD Dataset Image Sources &amp; Documentation&quot;">​</a></h1><p>Research conducted: 2025-12-19</p><h2 id="overview" tabindex="-1">Overview <a class="header-anchor" href="#overview" aria-label="Permalink to &quot;Overview&quot;">​</a></h2><p>The xView2 Challenge dataset (xBD - xView Building Damage) contains satellite imagery before and after natural disasters with building damage classification. This document catalogs image sources, visualization resources, and download locations.</p><hr><h2 id="primary-research-papers-figures" tabindex="-1">Primary Research Papers &amp; Figures <a class="header-anchor" href="#primary-research-papers-figures" aria-label="Permalink to &quot;Primary Research Papers &amp; Figures&quot;">​</a></h2><h3 id="_1-cvpr-2019-paper-creating-xbd-a-dataset-for-assessing-building-damage-from-satellite-imagery" tabindex="-1">1. CVPR 2019 Paper: &quot;Creating xBD: A Dataset for Assessing Building Damage from Satellite Imagery&quot; <a class="header-anchor" href="#_1-cvpr-2019-paper-creating-xbd-a-dataset-for-assessing-building-damage-from-satellite-imagery" aria-label="Permalink to &quot;1. CVPR 2019 Paper: &quot;Creating xBD: A Dataset for Assessing Building Damage from Satellite Imagery&quot;&quot;">​</a></h3><p><strong>Paper Details</strong></p><ul><li>Authors: Ritwik Gupta, Bryce Goodman, Nirav Patel, Ricky Hosfelt, Sandra Sajeev, Eric Heim, Jigar Doshi, Keane Lucas, Howie Choset, Matthew Gaston</li><li>Published: CVPR 2019 Workshops (cv4gc)</li><li>Date: November 21, 2019</li></ul><p><strong>Available Formats</strong></p><table tabindex="0"><thead><tr><th>Format</th><th>URL</th><th>License</th></tr></thead><tbody><tr><td>PDF (Direct)</td><td><a href="https://arxiv.org/pdf/1911.09296" target="_blank" rel="noreferrer">https://arxiv.org/pdf/1911.09296</a></td><td>arXiv Open Access</td></tr><tr><td>PDF (CVF)</td><td><a href="https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/Gupta_Creating_xBD_A_Dataset_for_Assessing_Building_Damage_from_Satellite_CVPRW_2019_paper.pdf" target="_blank" rel="noreferrer">https://openaccess.thecvf.com/content_CVPRW_2019/papers/cv4gc/Gupta_Creating_xBD_A_Dataset_for_Assessing_Building_Damage_from_Satellite_CVPRW_2019_paper.pdf</a></td><td>Open Access</td></tr><tr><td>HTML (ar5iv)</td><td><a href="https://ar5iv.labs.arxiv.org/html/1911.09296" target="_blank" rel="noreferrer">https://ar5iv.labs.arxiv.org/html/1911.09296</a></td><td>Open Access</td></tr><tr><td>HTML (CVF)</td><td><a href="https://openaccess.thecvf.com/content_CVPRW_2019/html/cv4gc/Gupta_Creating_xBD_A_Dataset_for_Assessing_Building_Damage_from_Satellite_CVPRW_2019_paper.html" target="_blank" rel="noreferrer">https://openaccess.thecvf.com/content_CVPRW_2019/html/cv4gc/Gupta_Creating_xBD_A_Dataset_for_Assessing_Building_Damage_from_Satellite_CVPRW_2019_paper.html</a></td><td>Open Access</td></tr></tbody></table><p><strong>Paper Contains</strong></p><ul><li>Figure 1: Example disaster events (Hurricane Harvey, Palu Tsunami, Mexico City Earthquake, Santa Rosa Fire)</li><li>Figure 2: Pre- and post-disaster satellite imagery pairs (4 events)</li><li>Figure 3: Geographic distribution map - disaster types and locations worldwide</li><li>Figure 4: Joint Damage Scale classification descriptions (0-3 scale)</li><li>Figure 5: Building polygon annotations on pre-disaster imagery</li><li>Figure 6: Area coverage distribution across disasters</li><li>Figure 7: Positive vs. negative imagery counts by disaster</li><li>Figure 8: Building polygon density per disaster event</li><li>Figure 9: Damage classification label distribution (heavily imbalanced - 8x more &quot;no damage&quot;)</li><li>Figure 10: Baseline classification model architecture diagram</li></ul><p><strong>Image Accessibility</strong>: Figures embedded in PDF/HTML. Use ar5iv HTML version for responsive viewing.</p><hr><h2 id="official-dataset-source" tabindex="-1">Official Dataset Source <a class="header-anchor" href="#official-dataset-source" aria-label="Permalink to &quot;Official Dataset Source&quot;">​</a></h2><h3 id="xview2-challenge-website" tabindex="-1">xView2 Challenge Website <a class="header-anchor" href="#xview2-challenge-website" aria-label="Permalink to &quot;xView2 Challenge Website&quot;">​</a></h3><p><strong>Main URL</strong>: <a href="https://xview2.org/" target="_blank" rel="noreferrer">https://xview2.org/</a></p><p><strong>Dataset Page</strong>: <a href="https://xview2.org/dataset" target="_blank" rel="noreferrer">https://xview2.org/dataset</a></p><p><strong>Status</strong>: Official repository (requires JavaScript, registration for download)</p><p><strong>Dataset Contents</strong></p><ul><li>22,068 satellite images at 1024×1024 resolution</li><li>11,034 pre/post-disaster image pairs</li><li>850,736 annotated building polygons</li><li>Coverage: 45,362 km² across 15 countries</li><li>19 natural disasters, 6 disaster types</li><li>Ground sampling distance (GSD): &lt;0.8 meters</li><li>Source: Maxar/DigitalGlobe Open Data Program</li></ul><p><strong>Download Requirements</strong></p><ul><li>Registration required</li><li>~10 GB compressed, ~11 GB uncompressed</li><li>Two training tiers: Tier 1 (2,799 pairs) + Tier 3 (5,600 pairs) = 8,399 total</li><li>Test set: 933 image pairs</li></ul><p><strong>Disaster Types Included</strong></p><table tabindex="0"><thead><tr><th>Disaster Type</th><th>Count</th><th>Examples</th></tr></thead><tbody><tr><td>Earthquake/Tsunami</td><td>4</td><td>Palu Tsunami, Mexico Earthquake, Lombok Earthquake, Sulawesi Tsunami</td></tr><tr><td>Wildfire</td><td>3</td><td>Santa Rosa Fire, Socal Fire, Pinery Bushfire</td></tr><tr><td>Flooding</td><td>2</td><td>Midwest Flooding, Nepal Flooding, India Monsoon</td></tr><tr><td>Volcanic Eruption</td><td>1</td><td>Guatemala Volcano, Lower Puna Eruption</td></tr><tr><td>Wind/Hurricane</td><td>5</td><td>Hurricane Harvey, Hurricane Florence, Hurricane Michael, Hurricane Matthew, Joplin Tornado</td></tr><tr><td>Landslide/Other</td><td>4</td><td>Additional events</td></tr></tbody></table><hr><h2 id="satellite-image-metadata" tabindex="-1">Satellite Image Metadata <a class="header-anchor" href="#satellite-image-metadata" aria-label="Permalink to &quot;Satellite Image Metadata&quot;">​</a></h2><p><strong>Imagery Source</strong>: Maxar/DigitalGlobe Open Data Program</p><ul><li>URL: <a href="https://www.digitalglobe.com/ecosystem/open-data" target="_blank" rel="noreferrer">https://www.digitalglobe.com/ecosystem/open-data</a></li><li>License: Various (check individual event availability)</li><li>Sensors: Multiple high-resolution optical satellites</li><li>Resolution: ~0.3-0.8 meters GSD</li></ul><p><strong>Image Specifications</strong></p><ul><li>Format: PNG, RGB (3-channel)</li><li>Resolution: 1024×1024 pixels</li><li>Color Depth: 24-bit RGB</li><li>Off-nadir angles: Variable (realistic satellite acquisition)</li><li>Sun elevation angles: Variable</li></ul><p><strong>Annotation Format</strong></p><ul><li>Building polygons in WKT notation</li><li>Damage labels: 0 (no damage), 1 (minor), 2 (major), 3 (destroyed)</li><li>Additional labels: fire, water, smoke, lava (environmental factors)</li><li>Georeferencing metadata included</li></ul><hr><h2 id="damage-classification-scale" tabindex="-1">Damage Classification Scale <a class="header-anchor" href="#damage-classification-scale" aria-label="Permalink to &quot;Damage Classification Scale&quot;">​</a></h2><h3 id="joint-damage-scale-4-level-ordinal-system" tabindex="-1">Joint Damage Scale (4-Level Ordinal System) <a class="header-anchor" href="#joint-damage-scale-4-level-ordinal-system" aria-label="Permalink to &quot;Joint Damage Scale (4-Level Ordinal System)&quot;">​</a></h3><p><strong>Color Scheme</strong> (standard visualization)</p><table tabindex="0"><thead><tr><th>Level</th><th>Value</th><th>Color</th><th>Description</th></tr></thead><tbody><tr><td>No Damage</td><td>0</td><td>Green</td><td>Undisturbed, no structural damage, no burn marks</td></tr><tr><td>Minor Damage</td><td>1</td><td>Blue</td><td>Partial burns, water surrounding, roof elements missing, visible cracks</td></tr><tr><td>Major Damage</td><td>2</td><td>Orange</td><td>Partial wall/roof collapse, significant damage encroaching</td></tr><tr><td>Destroyed</td><td>3</td><td>Red</td><td>Complete collapse, structure uninhabitable</td></tr></tbody></table><p><strong>Label Distribution in xBD</strong></p><ul><li>No Damage: 313,033 polygons (84%)</li><li>Minor Damage: 36,860 polygons (5%)</li><li>Major Damage: 29,904 polygons (4%)</li><li>Destroyed: 31,560 polygons (4%)</li><li>Unclassified: 14,011 polygons (3%)</li><li><strong>Imbalance Challenge</strong>: 8x more &quot;no damage&quot; than other categories</li></ul><hr><h2 id="alternative-access-points-mirrors" tabindex="-1">Alternative Access Points &amp; Mirrors <a class="header-anchor" href="#alternative-access-points-mirrors" aria-label="Permalink to &quot;Alternative Access Points &amp; Mirrors&quot;">​</a></h2><h3 id="_1-hugging-face" tabindex="-1">1. Hugging Face <a class="header-anchor" href="#_1-hugging-face" aria-label="Permalink to &quot;1. Hugging Face&quot;">​</a></h3><p><strong>URL</strong>: <a href="https://huggingface.co/datasets/danielz01/xView2" target="_blank" rel="noreferrer">https://huggingface.co/datasets/danielz01/xView2</a></p><ul><li>Creator: Chenhui Zhang (@danielz01)</li><li>Access: Requires login + accept terms and conditions</li><li>Status: ~104 monthly downloads</li><li>Format: Parquet</li><li>Size: 1K-10K entries</li></ul><h3 id="_2-roboflow-universe" tabindex="-1">2. Roboflow Universe <a class="header-anchor" href="#_2-roboflow-universe" aria-label="Permalink to &quot;2. Roboflow Universe&quot;">​</a></h3><p><strong>URL</strong>: <a href="https://universe.roboflow.com/ozu/xview2" target="_blank" rel="noreferrer">https://universe.roboflow.com/ozu/xview2</a></p><ul><li>Status: Public preview available</li><li>Sample Size: 520 open-source images</li><li>Classes: undamaged, minor-damage, major-damage, destroyed</li><li>License: CC BY 4.0</li><li>Includes: Pre-trained XView2 model + API</li></ul><h3 id="_3-kaggle" tabindex="-1">3. Kaggle <a class="header-anchor" href="#_3-kaggle" aria-label="Permalink to &quot;3. Kaggle&quot;">​</a></h3><ul><li>Searchable via Kaggle dataset exploration</li><li>May contain community kernels and subsets</li><li>Useful for quick experimentation</li></ul><h3 id="_4-torchgeo-library" tabindex="-1">4. TorchGeo Library <a class="header-anchor" href="#_4-torchgeo-library" aria-label="Permalink to &quot;4. TorchGeo Library&quot;">​</a></h3><p><strong>URL</strong>: <a href="https://torchgeo.readthedocs.io/" target="_blank" rel="noreferrer">https://torchgeo.readthedocs.io/</a></p><ul><li>Python library for geospatial deep learning</li><li>Includes xView2 dataset loader</li><li>Challenge training set: ~7.8 GB</li><li>Challenge test set: ~2.6 GB</li></ul><h3 id="_5-earth-observation-database" tabindex="-1">5. Earth Observation Database <a class="header-anchor" href="#_5-earth-observation-database" aria-label="Permalink to &quot;5. Earth Observation Database&quot;">​</a></h3><p><strong>URL</strong>: <a href="https://eod-grss-ieee.com/dataset-detail/MHpyVXNmV0dxaEtWWVBaNzlpckJPUT09" target="_blank" rel="noreferrer">https://eod-grss-ieee.com/dataset-detail/MHpyVXNmV0dxaEtWWVBaNzlpckJPUT09</a></p><ul><li>Metadata and index: xBD (xView2)</li></ul><hr><h2 id="baseline-reference-implementations" tabindex="-1">Baseline &amp; Reference Implementations <a class="header-anchor" href="#baseline-reference-implementations" aria-label="Permalink to &quot;Baseline &amp; Reference Implementations&quot;">​</a></h2><h3 id="official-baseline-repository" tabindex="-1">Official Baseline Repository <a class="header-anchor" href="#official-baseline-repository" aria-label="Permalink to &quot;Official Baseline Repository&quot;">​</a></h3><p><strong>GitHub</strong>: <a href="https://github.com/DIUx-xView/xView2_baseline" target="_blank" rel="noreferrer">https://github.com/DIUx-xView/xView2_baseline</a></p><ul><li>Language: Python 3.6+</li><li>Architecture: U-Net for localization, ResNet50 for classification</li><li>Authors: CMU SEI</li><li>Includes: Data preparation, training, inference scripts</li><li>License: Check repository</li></ul><p><strong>Output Specification</strong></p><ul><li>Grayscale PNG format</li><li>Pixel values: 0-4 (no building, no damage, minor, major, destroyed)</li></ul><h3 id="top-challenge-solutions" tabindex="-1">Top Challenge Solutions <a class="header-anchor" href="#top-challenge-solutions" aria-label="Permalink to &quot;Top Challenge Solutions&quot;">​</a></h3><ul><li><p><strong>1st Place</strong>: <a href="https://github.com/DIUx-xView/xView2_first_place" target="_blank" rel="noreferrer">https://github.com/DIUx-xView/xView2_first_place</a></p><ul><li>Siamese Neural Networks</li><li>Full image (1024×1024) inference with 4 TTA</li></ul></li><li><p><strong>2nd Place</strong>: <a href="https://github.com/ethanweber/xview2" target="_blank" rel="noreferrer">https://github.com/ethanweber/xview2</a></p><ul><li>Detectron2-based (Facebook)</li><li>Multi-temporal fusion</li></ul></li><li><p><strong>Visualization Tool</strong>: ethanweber/xview2 includes notebook for visualizing predictions</p></li></ul><h3 id="toolkit-utilities" tabindex="-1">Toolkit &amp; Utilities <a class="header-anchor" href="#toolkit-utilities" aria-label="Permalink to &quot;Toolkit &amp; Utilities&quot;">​</a></h3><ul><li><strong>xview2-toolkit</strong>: <a href="https://github.com/ashnair1/xview2-toolkit" target="_blank" rel="noreferrer">https://github.com/ashnair1/xview2-toolkit</a><ul><li>Annotation visualization</li><li>MS-COCO format conversion</li><li>Segmentation map generation</li></ul></li></ul><hr><h2 id="related-papers-research" tabindex="-1">Related Papers &amp; Research <a class="header-anchor" href="#related-papers-research" aria-label="Permalink to &quot;Related Papers &amp; Research&quot;">​</a></h2><h3 id="extended-works" tabindex="-1">Extended Works <a class="header-anchor" href="#extended-works" aria-label="Permalink to &quot;Extended Works&quot;">​</a></h3><ol><li><p><strong>[2212.13876] xFBD: Focused Building Damage Dataset and Analysis</strong></p><ul><li>URL: <a href="https://arxiv.org/pdf/2212.13876" target="_blank" rel="noreferrer">https://arxiv.org/pdf/2212.13876</a></li><li>Focused subset with different methodology</li></ul></li><li><p><strong>[2405.04800v1] DeepDamageNet</strong></p><ul><li>URL: <a href="https://arxiv.org/html/2405.04800v1" target="_blank" rel="noreferrer">https://arxiv.org/html/2405.04800v1</a></li><li>Two-step model for multi-disaster assessment</li></ul></li><li><p><strong>Building Damage Assessment Papers</strong></p><ul><li>ScienceDirect: <a href="https://www.sciencedirect.com/science/article/abs/pii/S0034425721003564" target="_blank" rel="noreferrer">https://www.sciencedirect.com/science/article/abs/pii/S0034425721003564</a></li><li>Taylor &amp; Francis: <a href="https://www.tandfonline.com/doi/full/10.1080/17538947.2024.2302577" target="_blank" rel="noreferrer">https://www.tandfonline.com/doi/full/10.1080/17538947.2024.2302577</a></li></ul></li></ol><h3 id="benchmark-collections" tabindex="-1">Benchmark Collections <a class="header-anchor" href="#benchmark-collections" aria-label="Permalink to &quot;Benchmark Collections&quot;">​</a></h3><ul><li><strong>Satellite Image Deep Learning Datasets</strong>: <a href="https://github.com/satellite-image-deep-learning/datasets" target="_blank" rel="noreferrer">https://github.com/satellite-image-deep-learning/datasets</a></li><li><strong>NAD Benchmarks</strong>: <a href="https://roc-hci.github.io/NADBenchmarks/" target="_blank" rel="noreferrer">https://roc-hci.github.io/NADBenchmarks/</a></li></ul><hr><h2 id="dataset-statistics-insights" tabindex="-1">Dataset Statistics &amp; Insights <a class="header-anchor" href="#dataset-statistics-insights" aria-label="Permalink to &quot;Dataset Statistics &amp; Insights&quot;">​</a></h2><h3 id="coverage-by-disaster" tabindex="-1">Coverage by Disaster <a class="header-anchor" href="#coverage-by-disaster" aria-label="Permalink to &quot;Coverage by Disaster&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Event</th><th>Region</th><th>Type</th><th>Coverage</th><th>Building Count</th></tr></thead><tbody><tr><td>Hurricane Harvey</td><td>Houston, TX</td><td>Wind</td><td>~8000 km²</td><td>100,000+</td></tr><tr><td>Palu Tsunami</td><td>Indonesia</td><td>Tsunami</td><td>&lt;1000 km²</td><td>100,000+</td></tr><tr><td>Mexico Earthquake</td><td>Mexico City</td><td>Earthquake</td><td>&lt;1000 km²</td><td>100,000+</td></tr><tr><td>Pinery Bushfire</td><td>Australia</td><td>Fire</td><td>~8000 km²</td><td>Large</td></tr></tbody></table><h3 id="model-performance-baseline" tabindex="-1">Model Performance Baseline <a class="header-anchor" href="#model-performance-baseline" aria-label="Permalink to &quot;Model Performance Baseline&quot;">​</a></h3><ul><li>Localization F1 (U-Net): 0.80</li><li>Localization IoU: 0.66</li><li>Classification F1 (ResNet50): Varies by damage class</li><li>Combined F1 (weighted): ~0.71 (IBM approach)</li></ul><h3 id="challenge-application" tabindex="-1">Challenge Application <a class="header-anchor" href="#challenge-application" aria-label="Permalink to &quot;Challenge Application&quot;">​</a></h3><ul><li>Real-world use: California wildfire damage assessment</li><li>Processing time: 10-20 minutes per large area (vs 1-2 days manual)</li><li>Deployed by: California National Guard</li></ul><hr><h2 id="image-download-instructions" tabindex="-1">Image Download Instructions <a class="header-anchor" href="#image-download-instructions" aria-label="Permalink to &quot;Image Download Instructions&quot;">​</a></h2><h3 id="method-1-official-xview2-portal" tabindex="-1">Method 1: Official xView2 Portal <a class="header-anchor" href="#method-1-official-xview2-portal" aria-label="Permalink to &quot;Method 1: Official xView2 Portal&quot;">​</a></h3><ol><li>Navigate to <a href="https://xview2.org/dataset" target="_blank" rel="noreferrer">https://xview2.org/dataset</a></li><li>Create account and login</li><li>Accept challenge terms</li><li>Download data (10 GB compressed)</li><li>Extract to working directory</li></ol><h3 id="method-2-maxar-open-data-stac" tabindex="-1">Method 2: Maxar Open Data STAC <a class="header-anchor" href="#method-2-maxar-open-data-stac" aria-label="Permalink to &quot;Method 2: Maxar Open Data STAC&quot;">​</a></h3><ol><li>Access STAC catalog: <a href="https://maxar-opendata.s3.amazonaws.com/events/catalog.json" target="_blank" rel="noreferrer">https://maxar-opendata.s3.amazonaws.com/events/catalog.json</a></li><li>Use GIS tools or STAC client to fetch individual events</li><li>Filter by disaster event</li><li>Download GeoTIFF or COG format</li></ol><h3 id="method-3-roboflow-quick-start" tabindex="-1">Method 3: Roboflow (Quick Start) <a class="header-anchor" href="#method-3-roboflow-quick-start" aria-label="Permalink to &quot;Method 3: Roboflow (Quick Start)&quot;">​</a></h3><ol><li>Visit <a href="https://universe.roboflow.com/ozu/xview2" target="_blank" rel="noreferrer">https://universe.roboflow.com/ozu/xview2</a></li><li>Browse 520 sample images</li><li>Download with API key for programmatic access</li></ol><h3 id="method-4-torchgeo-python" tabindex="-1">Method 4: TorchGeo (Python) <a class="header-anchor" href="#method-4-torchgeo-python" aria-label="Permalink to &quot;Method 4: TorchGeo (Python)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torchgeo.datasets </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> XView2</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">dataset </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> XView2(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">root</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;path/to/data&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">download</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h3 id="method-5-hugging-face-python" tabindex="-1">Method 5: Hugging Face (Python) <a class="header-anchor" href="#method-5-hugging-face-python" aria-label="Permalink to &quot;Method 5: Hugging Face (Python)&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> datasets </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> load_dataset</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ds </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> load_dataset(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;danielz01/xView2&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Requires login/acceptance</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h2 id="image-licensing-attribution" tabindex="-1">Image Licensing &amp; Attribution <a class="header-anchor" href="#image-licensing-attribution" aria-label="Permalink to &quot;Image Licensing &amp; Attribution&quot;">​</a></h2><h3 id="satellite-imagery" tabindex="-1">Satellite Imagery <a class="header-anchor" href="#satellite-imagery" aria-label="Permalink to &quot;Satellite Imagery&quot;">​</a></h3><ul><li><strong>Source</strong>: Maxar/DigitalGlobe Open Data Program</li><li><strong>License</strong>: Varies per event (typically public domain for crisis events)</li><li><strong>Attribution</strong>: DigitalGlobe/Maxar required</li><li><strong>Terms</strong>: Check specific event in Open Data Program catalog</li></ul><h3 id="dataset-annotations" tabindex="-1">Dataset Annotations <a class="header-anchor" href="#dataset-annotations" aria-label="Permalink to &quot;Dataset Annotations&quot;">​</a></h3><ul><li><strong>License</strong>: Creative Commons (varies by version)</li><li><strong>Citation</strong>: Gupta, R., et al. (2019). &quot;Creating xBD: A Dataset for Assessing Building Damage from Satellite Imagery&quot;</li><li><strong>Challenge Data</strong>: xView2 Challenge terms apply</li></ul><h3 id="research-use" tabindex="-1">Research Use <a class="header-anchor" href="#research-use" aria-label="Permalink to &quot;Research Use&quot;">​</a></h3><ul><li><strong>Academic</strong>: Permitted with attribution</li><li><strong>Commercial</strong>: Check licensing terms before deployment</li><li><strong>Redistribution</strong>: Allowed under CC BY 4.0 (Roboflow subset)</li></ul><hr><h2 id="format-specifications" tabindex="-1">Format Specifications <a class="header-anchor" href="#format-specifications" aria-label="Permalink to &quot;Format Specifications&quot;">​</a></h2><h3 id="pre-disaster-post-disaster-images" tabindex="-1">Pre-Disaster &amp; Post-Disaster Images <a class="header-anchor" href="#pre-disaster-post-disaster-images" aria-label="Permalink to &quot;Pre-Disaster &amp; Post-Disaster Images&quot;">​</a></h3><ul><li><strong>Container</strong>: PNG or GeoTIFF</li><li><strong>Bands</strong>: RGB (3 channels)</li><li><strong>Resolution</strong>: 1024×1024 pixels</li><li><strong>Bit Depth</strong>: 8-bit per channel</li><li><strong>Color Space</strong>: sRGB</li></ul><h3 id="ground-truth-annotations" tabindex="-1">Ground Truth Annotations <a class="header-anchor" href="#ground-truth-annotations" aria-label="Permalink to &quot;Ground Truth Annotations&quot;">​</a></h3><ul><li><strong>Format</strong>: JSON (polygon vertices) + CSV (damage labels)</li><li><strong>Polygon Notation</strong>: WKT (Well-Known Text)</li><li><strong>Damage Values</strong>: 0-3 (ordinal scale)</li><li><strong>Metadata</strong>: Georeferencing, sensor info, timestamp</li></ul><h3 id="output-format-predictions" tabindex="-1">Output Format (Predictions) <a class="header-anchor" href="#output-format-predictions" aria-label="Permalink to &quot;Output Format (Predictions)&quot;">​</a></h3><ul><li><strong>Container</strong>: PNG (grayscale)</li><li><strong>Values</strong>: 0-4 (pixel-wise damage classification)</li><li><strong>Resolution</strong>: 1024×1024 pixels</li></ul><hr><h2 id="key-statistics-summary" tabindex="-1">Key Statistics Summary <a class="header-anchor" href="#key-statistics-summary" aria-label="Permalink to &quot;Key Statistics Summary&quot;">​</a></h2><table tabindex="0"><thead><tr><th>Metric</th><th>Value</th></tr></thead><tbody><tr><td>Total Images</td><td>22,068</td></tr><tr><td>Image Pairs</td><td>11,034</td></tr><tr><td>Building Polygons</td><td>850,736</td></tr><tr><td>Geographic Coverage</td><td>45,362 km²</td></tr><tr><td>Countries</td><td>15+</td></tr><tr><td>Disaster Events</td><td>19</td></tr><tr><td>Disaster Types</td><td>6</td></tr><tr><td>GSD (Resolution)</td><td>&lt;0.8 meters</td></tr><tr><td>Image Size</td><td>1024×1024 pixels</td></tr><tr><td>Color Format</td><td>RGB (24-bit)</td></tr><tr><td>Damage Classes</td><td>4 levels (0-3)</td></tr><tr><td>Download Size</td><td>~10 GB (compressed), ~11 GB (uncompressed)</td></tr><tr><td>Training Pairs</td><td>8,399 (Tier1 + Tier3)</td></tr><tr><td>Test Pairs</td><td>933</td></tr></tbody></table><hr><h2 id="recommended-citation" tabindex="-1">Recommended Citation <a class="header-anchor" href="#recommended-citation" aria-label="Permalink to &quot;Recommended Citation&quot;">​</a></h2><div class="language-bibtex vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bibtex</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@inproceedings</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">gupta2019xbd</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  title</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Creating xBD: A Dataset for Assessing Building Damage from Satellite Imagery</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  author</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Gupta, Ritwik and Goodman, Bryce and Patel, Nirav and Hosfelt, Ricky and Sajeev, Sandra and Heim, Eric and Doshi, Jigar and Lucas, Keane and Choset, Howie and Gaston, Matthew</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  booktitle</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  pages</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">18--26</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  year</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">2019</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><hr><h2 id="important-notes" tabindex="-1">Important Notes <a class="header-anchor" href="#important-notes" aria-label="Permalink to &quot;Important Notes&quot;">​</a></h2><h3 id="data-imbalance" tabindex="-1">Data Imbalance <a class="header-anchor" href="#data-imbalance" aria-label="Permalink to &quot;Data Imbalance&quot;">​</a></h3><ul><li>&quot;No damage&quot; class heavily overrepresented (84% of labels)</li><li>Imbalance factor: ~8x more &quot;no damage&quot; than other classes</li><li>Poses challenge for class-balanced training</li></ul><h3 id="visual-similarity-challenge" tabindex="-1">Visual Similarity Challenge <a class="header-anchor" href="#visual-similarity-challenge" aria-label="Permalink to &quot;Visual Similarity Challenge&quot;">​</a></h3><ul><li>Minor vs. major damage can have subtle visual differences</li><li>Models often confuse adjacent damage classes</li><li>Joint Damage Scale designed as practical trade-off</li></ul><h3 id="real-world-application" tabindex="-1">Real-World Application <a class="header-anchor" href="#real-world-application" aria-label="Permalink to &quot;Real-World Application&quot;">​</a></h3><ul><li>Successfully used by California National Guard for wildfire assessment</li><li>Demonstrated 10-20 minute assessment vs. 1-2 days manual analysis</li><li>High-impact humanitarian/disaster response tool</li></ul><h3 id="access-limitations" tabindex="-1">Access Limitations <a class="header-anchor" href="#access-limitations" aria-label="Permalink to &quot;Access Limitations&quot;">​</a></h3><ul><li>Full dataset requires registration at xView2.org</li><li>Some mirrors/subsets publicly available (Roboflow, HuggingFace)</li><li>Roboflow subset (520 images) under CC BY 4.0 license</li></ul><hr><h2 id="unresolved-questions" tabindex="-1">Unresolved Questions <a class="header-anchor" href="#unresolved-questions" aria-label="Permalink to &quot;Unresolved Questions&quot;">​</a></h2><ol><li><strong>Exact GSD by Event</strong>: Paper states &quot;&lt;0.8 meters&quot; GSD but specific resolution per disaster varies</li><li><strong>Sensor Specifications</strong>: Which satellites exactly? (Likely DigitalGlobe WorldView series, specifics not documented)</li><li><strong>Temporal Gap</strong>: Exact time between pre- and post-disaster capture not always specified</li><li><strong>Quality Assurance</strong>: Annotation inter-rater agreement / QA metrics not provided in CVPR paper</li><li><strong>Update Frequency</strong>: Whether new disasters have been added to dataset post-2019</li><li><strong>Raw Image URLs</strong>: Specific CDN/S3 paths for individual images not publicly listed (must download via portal)</li></ol><hr><h2 id="last-updated" tabindex="-1">Last Updated <a class="header-anchor" href="#last-updated" aria-label="Permalink to &quot;Last Updated&quot;">​</a></h2><p>Research Date: 2025-12-19</p><p>All URLs verified as active at time of documentation. Note that websites and data availability may change.</p>`,133)])])}const u=e(r,[["render",l]]);export{p as __pageData,u as default};
