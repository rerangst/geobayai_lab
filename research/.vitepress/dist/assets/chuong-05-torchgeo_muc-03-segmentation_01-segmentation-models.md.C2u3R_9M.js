import{_ as c,C as g,c as d,o,a2 as i,b as l,j as e,w as n,a,G as s,a3 as h}from"./chunks/framework.nRfFlDZQ.js";const u="/sen_doc/assets/unet_page3_fig1.yQy7v2QU.png",p="/sen_doc/assets/hrnet_page2.CDuJPukp.png",_=JSON.parse('{"title":"Chương 5.3: Segmentation Models cho Phân Đoạn Ảnh Viễn Thám","description":"","frontmatter":{},"headers":[],"relativePath":"chuong-05-torchgeo/muc-03-segmentation/01-segmentation-models.md","filePath":"chuong-05-torchgeo/muc-03-segmentation/01-segmentation-models.md","lastUpdated":1766163869000}'),m={name:"chuong-05-torchgeo/muc-03-segmentation/01-segmentation-models.md"},T={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},b={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.025ex"},xmlns:"http://www.w3.org/2000/svg",width:"1.02ex",height:"1.025ex",role:"img",focusable:"false",viewBox:"0 -442 451 453","aria-hidden":"true"},A={class:"MathJax",jax:"SVG",style:{direction:"ltr",position:"relative"}},C={style:{overflow:"visible","min-height":"1px","min-width":"1px","vertical-align":"-0.566ex"},xmlns:"http://www.w3.org/2000/svg",width:"18.383ex",height:"2.262ex",role:"img",focusable:"false",viewBox:"0 -750 8125.3 1000","aria-hidden":"true"};function v(k,t,f,P,Q,y){const r=g("Mermaid");return o(),d("div",null,[t[12]||(t[12]=i('<h1 id="chuong-5-3-segmentation-models-cho-phan-đoan-anh-vien-tham" tabindex="-1">Chương 5.3: Segmentation Models cho Phân Đoạn Ảnh Viễn Thám <a class="header-anchor" href="#chuong-5-3-segmentation-models-cho-phan-đoan-anh-vien-tham" aria-label="Permalink to &quot;Chương 5.3: Segmentation Models cho Phân Đoạn Ảnh Viễn Thám&quot;">​</a></h1><h2 id="_5-3-1-gioi-thieu" tabindex="-1">5.3.1. Giới Thiệu <a class="header-anchor" href="#_5-3-1-gioi-thieu" aria-label="Permalink to &quot;5.3.1. Giới Thiệu&quot;">​</a></h2><p>Semantic segmentation - bài toán gán nhãn cho từng pixel trong ảnh - đóng vai trò then chốt trong nhiều ứng dụng viễn thám quan trọng. Từ việc lập bản đồ lớp phủ mặt đất, chiết xuất footprint công trình, đến phân vùng ngập lụt và khoanh vùng đất nông nghiệp, segmentation cung cấp thông tin không gian chi tiết mà classification ở mức scene không thể đạt được.</p><p>Khác với classification chỉ dự đoán một nhãn cho toàn ảnh, segmentation yêu cầu mô hình duy trì thông tin không gian xuyên suốt quá trình xử lý. Điều này đặt ra thách thức về việc cân bằng giữa context (để hiểu &quot;cái gì&quot;) và resolution (để biết &quot;ở đâu&quot;). Các kiến trúc segmentation hiện đại giải quyết trade-off này theo nhiều cách tiếp cận khác nhau: encoder-decoder với skip connections (U-Net), atrous convolution với multi-scale pooling (DeepLabV3+), feature pyramid (FPN), hay duy trì high-resolution xuyên suốt (HRNet).</p><p>TorchGeo tích hợp các kiến trúc segmentation tiên tiến này thông qua <code>segmentation_models_pytorch</code>, cho phép sử dụng các encoder đã được pre-trained trên dữ liệu viễn thám. Chương này phân tích năm kiến trúc segmentation chính và cách áp dụng chúng cho các bài toán geospatial.</p><h2 id="_5-3-2-u-net-encoder-decoder-voi-skip-connections" tabindex="-1">5.3.2. U-Net: Encoder-Decoder với Skip Connections <a class="header-anchor" href="#_5-3-2-u-net-encoder-decoder-voi-skip-connections" aria-label="Permalink to &quot;5.3.2. U-Net: Encoder-Decoder với Skip Connections&quot;">​</a></h2><h3 id="_5-3-2-1-kien-truc-đoi-xung" tabindex="-1">5.3.2.1. Kiến Trúc Đối Xứng <a class="header-anchor" href="#_5-3-2-1-kien-truc-đoi-xung" aria-label="Permalink to &quot;5.3.2.1. Kiến Trúc Đối Xứng&quot;">​</a></h3><p>U-Net [Ronneberger et al., 2015] được thiết kế ban đầu cho segmentation ảnh y sinh với lượng dữ liệu huấn luyện hạn chế. Ý tưởng cốt lõi là kết hợp encoder (contracting path) giảm resolution để nắm bắt context, với decoder (expanding path) tăng resolution để khôi phục chi tiết không gian.</p>',8)),(o(),l(h,null,{default:n(()=>[s(r,{id:"mermaid-24",class:"mermaid",graph:"graph%20TD%0A%20%20%20%20subgraph%20%22Encoder%20(Contracting%20Path)%22%0A%20%20%20%20%20%20%20%20A%5B%22Input%20572%C3%97572%22%5D%20--%3E%20B%5B%22Conv%2064%22%5D%0A%20%20%20%20%20%20%20%20B%20--%3E%20C%5B%22MaxPool%20%E2%86%92%20Conv%20128%22%5D%0A%20%20%20%20%20%20%20%20C%20--%3E%20D%5B%22MaxPool%20%E2%86%92%20Conv%20256%22%5D%0A%20%20%20%20%20%20%20%20D%20--%3E%20E%5B%22MaxPool%20%E2%86%92%20Conv%20512%22%5D%0A%20%20%20%20%20%20%20%20E%20--%3E%20F%5B%22MaxPool%20%E2%86%92%20Conv%201024%22%5D%0A%20%20%20%20end%0A%0A%20%20%20%20subgraph%20%22Decoder%20(Expanding%20Path)%22%0A%20%20%20%20%20%20%20%20F%20--%3E%20G%5B%22UpConv%20512%22%5D%0A%20%20%20%20%20%20%20%20G%20--%3E%20H%5B%22UpConv%20256%22%5D%0A%20%20%20%20%20%20%20%20H%20--%3E%20I%5B%22UpConv%20128%22%5D%0A%20%20%20%20%20%20%20%20I%20--%3E%20J%5B%22UpConv%2064%22%5D%0A%20%20%20%20%20%20%20%20J%20--%3E%20K%5B%22Conv%201%C3%971%20%E2%86%92%20Output%22%5D%0A%20%20%20%20end%0A%0A%20%20%20%20E%20-.-%3E%7Cskip%7C%20G%0A%20%20%20%20D%20-.-%3E%7Cskip%7C%20H%0A%20%20%20%20C%20-.-%3E%7Cskip%7C%20I%0A%20%20%20%20B%20-.-%3E%7Cskip%7C%20J%0A%0A%20%20%20%20style%20A%20fill%3A%23e3f2fd%0A%20%20%20%20style%20K%20fill%3A%23c8e6c9%0A"})]),fallback:n(()=>[...t[0]||(t[0]=[a(" Loading... ",-1)])]),_:1})),t[13]||(t[13]=i('<p><strong>Hình 5.9:</strong> Kiến trúc U-Net với encoder-decoder đối xứng và skip connections</p><p><img src="'+u+'" alt="U-Net Segmentation Example"></p><p><strong>Hình 5.9b:</strong> Ví dụ segmentation với U-Net - input image (trái), ground truth (giữa), prediction (phải) [Ronneberger et al., 2015]</p><p>Điểm đặc trưng của U-Net là các <strong>skip connections</strong> nối trực tiếp từ encoder sang decoder ở cùng mức resolution. Các feature maps từ encoder (có thông tin chi tiết về vị trí) được concatenate với feature maps từ decoder (có thông tin về context), cho phép decoder khôi phục boundary chính xác.</p><h3 id="_5-3-2-2-uu-điem-cho-vien-tham" tabindex="-1">5.3.2.2. Ưu Điểm cho Viễn Thám <a class="header-anchor" href="#_5-3-2-2-uu-điem-cho-vien-tham" aria-label="Permalink to &quot;5.3.2.2. Ưu Điểm cho Viễn Thám&quot;">​</a></h3><p>U-Net đặc biệt phù hợp với các điều kiện thường gặp trong viễn thám:</p><ul><li><strong>Dữ liệu labeled hạn chế:</strong> Thiết kế tận dụng tối đa thông tin từ mỗi sample thông qua data augmentation mạnh (elastic deformations, flips, rotations)</li><li><strong>Boundary chính xác:</strong> Skip connections bảo toàn chi tiết spatial, quan trọng cho extraction building footprints, roads, parcels</li><li><strong>Ảnh kích thước lớn:</strong> Kiến trúc fully convolutional hỗ trợ seamless tiling - xử lý ảnh lớn bằng cách chia thành patches và merge kết quả</li></ul><h3 id="_5-3-2-3-benchmark" tabindex="-1">5.3.2.3. Benchmark <a class="header-anchor" href="#_5-3-2-3-benchmark" aria-label="Permalink to &quot;5.3.2.3. Benchmark&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Task</th><th>Dataset</th><th>Metric</th><th>Score</th></tr></thead><tbody><tr><td>Cell Segmentation</td><td>ISBI EM Stack</td><td>Warping Error</td><td>0.000353 (1st)</td></tr><tr><td>Cell Tracking</td><td>PhC-U373</td><td>IoU</td><td>92%</td></tr><tr><td>Cell Tracking</td><td>DIC-HeLa</td><td>IoU</td><td>77.5%</td></tr></tbody></table><p><strong>Bảng 5.10:</strong> Benchmark U-Net trên các bài toán biomedical</p><h2 id="_5-3-3-deeplabv3-atrous-convolution-va-aspp" tabindex="-1">5.3.3. DeepLabV3+: Atrous Convolution và ASPP <a class="header-anchor" href="#_5-3-3-deeplabv3-atrous-convolution-va-aspp" aria-label="Permalink to &quot;5.3.3. DeepLabV3+: Atrous Convolution và ASPP&quot;">​</a></h2><h3 id="_5-3-3-1-nguyen-ly-atrous-convolution" tabindex="-1">5.3.3.1. Nguyên Lý Atrous Convolution <a class="header-anchor" href="#_5-3-3-1-nguyen-ly-atrous-convolution" aria-label="Permalink to &quot;5.3.3.1. Nguyên Lý Atrous Convolution&quot;">​</a></h3><p>DeepLabV3+ [Chen et al., 2018] giải quyết vấn đề trade-off resolution-context thông qua <strong>atrous (dilated) convolution</strong> - kỹ thuật chèn &quot;lỗ hổng&quot; vào kernel để mở rộng receptive field mà không tăng số parameters hay giảm resolution.</p>',13)),e("p",null,[t[5]||(t[5]=a("Với dilation rate ",-1)),e("mjx-container",T,[(o(),d("svg",b,[...t[1]||(t[1]=[e("g",{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"scale(1,-1)"},[e("g",{"data-mml-node":"math"},[e("g",{"data-mml-node":"mi"},[e("path",{"data-c":"1D45F",d:"M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z",style:{"stroke-width":"3"}})])])],-1)])])),t[2]||(t[2]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mi",null,"r")])],-1))]),t[6]||(t[6]=a(", một kernel 3×3 có effective receptive field là ",-1)),e("mjx-container",A,[(o(),d("svg",C,[...t[3]||(t[3]=[i('<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="scale(1,-1)"><g data-mml-node="math"><g data-mml-node="mo"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(389,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(889,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(1562.2,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(2562.4,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3062.4,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(3673.7,0)"><path data-c="D7" d="M630 29Q630 9 609 9Q604 9 587 25T493 118L389 222L284 117Q178 13 175 11Q171 9 168 9Q160 9 154 15T147 29Q147 36 161 51T255 146L359 250L255 354Q174 435 161 449T147 471Q147 480 153 485T168 490Q173 490 175 489Q178 487 284 383L389 278L493 382Q570 459 587 475T609 491Q630 491 630 471Q630 464 620 453T522 355L418 250L522 145Q606 61 618 48T630 29Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(4673.9,0)"><path data-c="28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(5062.9,0)"><path data-c="32" d="M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z" style="stroke-width:3;"></path></g><g data-mml-node="mi" transform="translate(5562.9,0)"><path data-c="1D45F" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(6236.1,0)"><path data-c="2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z" style="stroke-width:3;"></path></g><g data-mml-node="mn" transform="translate(7236.3,0)"><path data-c="31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z" style="stroke-width:3;"></path></g><g data-mml-node="mo" transform="translate(7736.3,0)"><path data-c="29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z" style="stroke-width:3;"></path></g></g></g>',1)])])),t[4]||(t[4]=e("mjx-assistive-mml",{unselectable:"on",display:"inline",style:{top:"0px",left:"0px",clip:"rect(1px, 1px, 1px, 1px)","-webkit-touch-callout":"none","-webkit-user-select":"none","-khtml-user-select":"none","-moz-user-select":"none","-ms-user-select":"none","user-select":"none",position:"absolute",padding:"1px 0px 0px 0px",border:"0px",display:"block",width:"auto",overflow:"hidden"}},[e("math",{xmlns:"http://www.w3.org/1998/Math/MathML"},[e("mo",{stretchy:"false"},"("),e("mn",null,"2"),e("mi",null,"r"),e("mo",null,"+"),e("mn",null,"1"),e("mo",{stretchy:"false"},")"),e("mo",null,"×"),e("mo",{stretchy:"false"},"("),e("mn",null,"2"),e("mi",null,"r"),e("mo",null,"+"),e("mn",null,"1"),e("mo",{stretchy:"false"},")")])],-1))]),t[7]||(t[7]=a(" trong khi vẫn chỉ sử dụng 9 parameters. Điều này cho phép nắm bắt context ở nhiều scale khác nhau.",-1))]),t[14]||(t[14]=e("h3",{id:"_5-3-3-2-atrous-spatial-pyramid-pooling-aspp",tabindex:"-1"},[a("5.3.3.2. Atrous Spatial Pyramid Pooling (ASPP) "),e("a",{class:"header-anchor",href:"#_5-3-3-2-atrous-spatial-pyramid-pooling-aspp","aria-label":'Permalink to "5.3.3.2. Atrous Spatial Pyramid Pooling (ASPP)"'},"​")],-1)),t[15]||(t[15]=e("p",null,"ASPP module áp dụng atrous convolution với nhiều dilation rates khác nhau (thường 6, 12, 18) song song, sau đó concatenate kết quả để tạo multi-scale features:",-1)),(o(),l(h,null,{default:n(()=>[s(r,{id:"mermaid-146",class:"mermaid",graph:"graph%20LR%0A%20%20%20%20A%5B%22Encoder%20Features%22%5D%20--%3E%20B%5B%221%C3%971%20Conv%22%5D%0A%20%20%20%20A%20--%3E%20C%5B%223%C3%973%20Atrous%20r%3D6%22%5D%0A%20%20%20%20A%20--%3E%20D%5B%223%C3%973%20Atrous%20r%3D12%22%5D%0A%20%20%20%20A%20--%3E%20E%5B%223%C3%973%20Atrous%20r%3D18%22%5D%0A%20%20%20%20A%20--%3E%20F%5B%22Image%20Pooling%22%5D%0A%0A%20%20%20%20B%20--%3E%20G%5B%22Concatenate%22%5D%0A%20%20%20%20C%20--%3E%20G%0A%20%20%20%20D%20--%3E%20G%0A%20%20%20%20E%20--%3E%20G%0A%20%20%20%20F%20--%3E%20G%0A%0A%20%20%20%20G%20--%3E%20H%5B%221%C3%971%20Conv%3Cbr%2F%3E256%20channels%22%5D%0A%20%20%20%20H%20--%3E%20I%5B%22Decoder%22%5D%0A%0A%20%20%20%20style%20A%20fill%3A%23e3f2fd%0A%20%20%20%20style%20I%20fill%3A%23c8e6c9%0A"})]),fallback:n(()=>[...t[8]||(t[8]=[a(" Loading... ",-1)])]),_:1})),t[16]||(t[16]=i('<p><strong>Hình 5.11:</strong> Cấu trúc ASPP module trong DeepLabV3+</p><h3 id="_5-3-3-3-decoder-va-benchmark" tabindex="-1">5.3.3.3. Decoder và Benchmark <a class="header-anchor" href="#_5-3-3-3-decoder-va-benchmark" aria-label="Permalink to &quot;5.3.3.3. Decoder và Benchmark&quot;">​</a></h3><p>Decoder trong DeepLabV3+ đơn giản nhưng hiệu quả: upsample encoder features 4×, fuse với low-level features từ backbone, refine qua 3×3 convolutions, và final upsample về resolution gốc.</p><table tabindex="0"><thead><tr><th>Dataset</th><th>mIoU</th><th>Ghi chú</th></tr></thead><tbody><tr><td>PASCAL VOC 2012</td><td>89.0%</td><td>Test set, no CRF</td></tr><tr><td>Cityscapes</td><td>82.1%</td><td>Test set</td></tr></tbody></table><p><strong>Bảng 5.11:</strong> Benchmark DeepLabV3+ trên natural image datasets</p><p><strong>Ưu điểm cho viễn thám:</strong></p><ul><li>Multi-scale pooling phù hợp với objects có kích thước đa dạng (từ xe cộ đến cánh đồng)</li><li>Decoder cải thiện boundary precision</li><li>Depthwise separable convolutions tăng efficiency</li></ul><h2 id="_5-3-4-feature-pyramid-network-fpn" tabindex="-1">5.3.4. Feature Pyramid Network (FPN) <a class="header-anchor" href="#_5-3-4-feature-pyramid-network-fpn" aria-label="Permalink to &quot;5.3.4. Feature Pyramid Network (FPN)&quot;">​</a></h2><h3 id="_5-3-4-1-top-down-feature-fusion" tabindex="-1">5.3.4.1. Top-Down Feature Fusion <a class="header-anchor" href="#_5-3-4-1-top-down-feature-fusion" aria-label="Permalink to &quot;5.3.4.1. Top-Down Feature Fusion&quot;">​</a></h3><p>Feature Pyramid Network [Lin et al., 2017] giải quyết vấn đề multi-scale object detection bằng cách xây dựng feature pyramid có semantic strength đồng đều ở mọi scale.</p><p>Trong CNN backbone thông thường, features ở các layers sâu có semantic mạnh nhưng resolution thấp, trong khi layers nông có resolution cao nhưng semantic yếu. FPN kết hợp ưu điểm của cả hai thông qua top-down pathway với lateral connections.</p>',11)),(o(),l(h,null,{default:n(()=>[s(r,{id:"mermaid-230",class:"mermaid",graph:"graph%20TD%0A%20%20%20%20subgraph%20%22Bottom-Up%20Pathway%22%0A%20%20%20%20%20%20%20%20C2%5B%22C2%20(1%2F4)%22%5D%20--%3E%20C3%5B%22C3%20(1%2F8)%22%5D%0A%20%20%20%20%20%20%20%20C3%20--%3E%20C4%5B%22C4%20(1%2F16)%22%5D%0A%20%20%20%20%20%20%20%20C4%20--%3E%20C5%5B%22C5%20(1%2F32)%22%5D%0A%20%20%20%20end%0A%0A%20%20%20%20subgraph%20%22Top-Down%20Pathway%22%0A%20%20%20%20%20%20%20%20C5%20--%3E%20P5%5B%22P5%22%5D%0A%20%20%20%20%20%20%20%20P5%20--%3E%7C%222%C3%97%20upsample%22%7C%20add4%5B%22%E2%8A%95%22%5D%0A%20%20%20%20%20%20%20%20C4%20--%3E%7C%221%C3%971%20conv%22%7C%20add4%0A%20%20%20%20%20%20%20%20add4%20--%3E%20P4%5B%22P4%22%5D%0A%0A%20%20%20%20%20%20%20%20P4%20--%3E%7C%222%C3%97%20upsample%22%7C%20add3%5B%22%E2%8A%95%22%5D%0A%20%20%20%20%20%20%20%20C3%20--%3E%7C%221%C3%971%20conv%22%7C%20add3%0A%20%20%20%20%20%20%20%20add3%20--%3E%20P3%5B%22P3%22%5D%0A%0A%20%20%20%20%20%20%20%20P3%20--%3E%7C%222%C3%97%20upsample%22%7C%20add2%5B%22%E2%8A%95%22%5D%0A%20%20%20%20%20%20%20%20C2%20--%3E%7C%221%C3%971%20conv%22%7C%20add2%0A%20%20%20%20%20%20%20%20add2%20--%3E%20P2%5B%22P2%22%5D%0A%20%20%20%20end%0A%0A%20%20%20%20style%20C2%20fill%3A%23e3f2fd%0A%20%20%20%20style%20P2%20fill%3A%23c8e6c9%0A%20%20%20%20style%20P3%20fill%3A%23c8e6c9%0A%20%20%20%20style%20P4%20fill%3A%23c8e6c9%0A%20%20%20%20style%20P5%20fill%3A%23c8e6c9%0A"})]),fallback:n(()=>[...t[9]||(t[9]=[a(" Loading... ",-1)])]),_:1})),t[17]||(t[17]=i('<p><strong>Hình 5.12:</strong> Cấu trúc bottom-up và top-down pathways của FPN</p><h3 id="_5-3-4-2-uu-điem-cho-vien-tham" tabindex="-1">5.3.4.2. Ưu Điểm cho Viễn Thám <a class="header-anchor" href="#_5-3-4-2-uu-điem-cho-vien-tham" aria-label="Permalink to &quot;5.3.4.2. Ưu Điểm cho Viễn Thám&quot;">​</a></h3><p>FPN đặc biệt hiệu quả trong viễn thám nhờ:</p><ul><li><strong>Multi-scale detection:</strong> P2-P5 levels xử lý objects từ rất nhỏ (xe, cây) đến rất lớn (cánh đồng, hồ)</li><li><strong>Small object performance:</strong> +12.9 AR points cho small objects trên COCO</li><li><strong>Computational efficiency:</strong> Single-scale input, tránh expensive image pyramids</li><li><strong>Backbone agnostic:</strong> Áp dụng được với bất kỳ CNN backbone nào</li></ul><table tabindex="0"><thead><tr><th>Metric</th><th>Baseline</th><th>FPN</th><th>Improvement</th></tr></thead><tbody><tr><td>AR@1k</td><td>48.3</td><td>56.3</td><td>+8.0</td></tr><tr><td>ARs (small)</td><td>32.5</td><td>45.4</td><td>+12.9</td></tr><tr><td>Inference</td><td>0.32s</td><td>0.148s</td><td>2.2× faster</td></tr></tbody></table><p><strong>Bảng 5.12:</strong> FPN improvements trên COCO detection</p><h2 id="_5-3-5-pspnet-pyramid-pooling-module" tabindex="-1">5.3.5. PSPNet: Pyramid Pooling Module <a class="header-anchor" href="#_5-3-5-pspnet-pyramid-pooling-module" aria-label="Permalink to &quot;5.3.5. PSPNet: Pyramid Pooling Module&quot;">​</a></h2><h3 id="_5-3-5-1-global-context-aggregation" tabindex="-1">5.3.5.1. Global Context Aggregation <a class="header-anchor" href="#_5-3-5-1-global-context-aggregation" aria-label="Permalink to &quot;5.3.5.1. Global Context Aggregation&quot;">​</a></h3><p>PSPNet [Zhao et al., 2017] nhận diện rằng nhiều lỗi segmentation xuất phát từ thiếu global context - mô hình không &quot;nhìn&quot; đủ rộng để hiểu scene layout. Pyramid Pooling Module (PPM) giải quyết vấn đề này bằng cách pool features ở nhiều scale (1×1, 2×2, 3×3, 6×6) rồi concatenate với original features.</p>',9)),(o(),l(h,null,{default:n(()=>[s(r,{id:"mermaid-336",class:"mermaid",graph:"graph%20LR%0A%20%20%20%20subgraph%20%22Backbone%22%0A%20%20%20%20%20%20%20%20A%5B%22ResNet%20Features%3Cbr%2F%3EH%C3%97W%C3%97C%22%5D%0A%20%20%20%20end%0A%0A%20%20%20%20subgraph%20%22Pyramid%20Pooling%20Module%22%0A%20%20%20%20%20%20%20%20A%20--%3E%20B1%5B%22Pool%201%C3%971%22%5D%0A%20%20%20%20%20%20%20%20A%20--%3E%20B2%5B%22Pool%202%C3%972%22%5D%0A%20%20%20%20%20%20%20%20A%20--%3E%20B3%5B%22Pool%203%C3%973%22%5D%0A%20%20%20%20%20%20%20%20A%20--%3E%20B4%5B%22Pool%206%C3%976%22%5D%0A%0A%20%20%20%20%20%20%20%20B1%20--%3E%20C1%5B%221%C3%971%20Conv%22%5D%0A%20%20%20%20%20%20%20%20B2%20--%3E%20C2%5B%221%C3%971%20Conv%22%5D%0A%20%20%20%20%20%20%20%20B3%20--%3E%20C3%5B%221%C3%971%20Conv%22%5D%0A%20%20%20%20%20%20%20%20B4%20--%3E%20C4%5B%221%C3%971%20Conv%22%5D%0A%0A%20%20%20%20%20%20%20%20C1%20--%3E%20D1%5B%22Upsample%3Cbr%2F%3Eto%20H%C3%97W%22%5D%0A%20%20%20%20%20%20%20%20C2%20--%3E%20D2%5B%22Upsample%3Cbr%2F%3Eto%20H%C3%97W%22%5D%0A%20%20%20%20%20%20%20%20C3%20--%3E%20D3%5B%22Upsample%3Cbr%2F%3Eto%20H%C3%97W%22%5D%0A%20%20%20%20%20%20%20%20C4%20--%3E%20D4%5B%22Upsample%3Cbr%2F%3Eto%20H%C3%97W%22%5D%0A%20%20%20%20end%0A%0A%20%20%20%20A%20--%3E%20E%5B%22Concat%22%5D%0A%20%20%20%20D1%20--%3E%20E%0A%20%20%20%20D2%20--%3E%20E%0A%20%20%20%20D3%20--%3E%20E%0A%20%20%20%20D4%20--%3E%20E%0A%0A%20%20%20%20E%20--%3E%20F%5B%22Conv%20%E2%86%92%20Output%22%5D%0A%0A%20%20%20%20style%20A%20fill%3A%23e3f2fd%0A%20%20%20%20style%20F%20fill%3A%23c8e6c9%0A"})]),fallback:n(()=>[...t[10]||(t[10]=[a(" Loading... ",-1)])]),_:1})),t[18]||(t[18]=i('<p><strong>Hình 5.13:</strong> Pyramid Pooling Module của PSPNet</p><h3 id="_5-3-5-2-benchmark-va-ung-dung-rs" tabindex="-1">5.3.5.2. Benchmark và Ứng Dụng RS <a class="header-anchor" href="#_5-3-5-2-benchmark-va-ung-dung-rs" aria-label="Permalink to &quot;5.3.5.2. Benchmark và Ứng Dụng RS&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Dataset</th><th>mIoU</th><th>Rank</th></tr></thead><tbody><tr><td>ADE20K (ImageNet 2016)</td><td>44.94%</td><td>1st place</td></tr><tr><td>PASCAL VOC 2012</td><td>85.4%</td><td>Top tier</td></tr><tr><td>Cityscapes</td><td>80.2%</td><td>Competitive</td></tr></tbody></table><p><strong>Bảng 5.13:</strong> Benchmark PSPNet trên semantic segmentation</p><p><strong>Ưu điểm cho viễn thám:</strong></p><ul><li>Global context giúp disambiguate similar land covers (phân biệt các loại cây trồng)</li><li>Xử lý complex scenes với nhiều class interactions</li><li>Robust với variation về ánh sáng và điều kiện khí quyển</li></ul><h2 id="_5-3-6-hrnet-high-resolution-networks" tabindex="-1">5.3.6. HRNet: High-Resolution Networks <a class="header-anchor" href="#_5-3-6-hrnet-high-resolution-networks" aria-label="Permalink to &quot;5.3.6. HRNet: High-Resolution Networks&quot;">​</a></h2><h3 id="_5-3-6-1-parallel-high-resolution-streams" tabindex="-1">5.3.6.1. Parallel High-Resolution Streams <a class="header-anchor" href="#_5-3-6-1-parallel-high-resolution-streams" aria-label="Permalink to &quot;5.3.6.1. Parallel High-Resolution Streams&quot;">​</a></h3><p>HRNet [Wang et al., 2019] đề xuất cách tiếp cận hoàn toàn khác: thay vì downsampling rồi upsampling, duy trì high-resolution representation xuyên suốt network. Các streams với resolution khác nhau chạy song song và liên tục exchange thông tin.</p><p><img src="'+p+'" alt="HRNet Architecture"></p><p><strong>Hình 5.14:</strong> Kiến trúc HRNet với các parallel high-resolution streams và multi-resolution fusion [Wang et al., 2019]</p><h3 id="_5-3-6-2-multi-resolution-fusion" tabindex="-1">5.3.6.2. Multi-Resolution Fusion <a class="header-anchor" href="#_5-3-6-2-multi-resolution-fusion" aria-label="Permalink to &quot;5.3.6.2. Multi-Resolution Fusion&quot;">​</a></h3><p>HRNetV2 (cho segmentation) aggregates tất cả parallel streams bằng cách upsample các low-resolution streams về highest resolution, concatenate, và apply final convolution:</p>',13)),(o(),l(h,null,{default:n(()=>[s(r,{id:"mermaid-437",class:"mermaid",graph:"graph%20TD%0A%20%20%20%20subgraph%20%22Parallel%20Streams%22%0A%20%20%20%20%20%20%20%20S1%5B%22Stream%201%20(1%2F4)%22%5D%0A%20%20%20%20%20%20%20%20S2%5B%22Stream%202%20(1%2F8)%22%5D%0A%20%20%20%20%20%20%20%20S3%5B%22Stream%203%20(1%2F16)%22%5D%0A%20%20%20%20%20%20%20%20S4%5B%22Stream%204%20(1%2F32)%22%5D%0A%20%20%20%20end%0A%0A%20%20%20%20subgraph%20%22Fusion%20Blocks%22%0A%20%20%20%20%20%20%20%20S1%20%3C--%3E%7Cexchange%7C%20S2%0A%20%20%20%20%20%20%20%20S2%20%3C--%3E%7Cexchange%7C%20S3%0A%20%20%20%20%20%20%20%20S3%20%3C--%3E%7Cexchange%7C%20S4%0A%20%20%20%20end%0A%0A%20%20%20%20subgraph%20%22HRNetV2%20Output%22%0A%20%20%20%20%20%20%20%20S1%20--%3E%20O%5B%22Concat%22%5D%0A%20%20%20%20%20%20%20%20S2%20--%3E%7C%222%C3%97%20up%22%7C%20O%0A%20%20%20%20%20%20%20%20S3%20--%3E%7C%224%C3%97%20up%22%7C%20O%0A%20%20%20%20%20%20%20%20S4%20--%3E%7C%228%C3%97%20up%22%7C%20O%0A%20%20%20%20%20%20%20%20O%20--%3E%20F%5B%221%C3%971%20Conv%20%E2%86%92%20Output%22%5D%0A%20%20%20%20end%0A%0A%20%20%20%20style%20S1%20fill%3A%23e3f2fd%0A%20%20%20%20style%20F%20fill%3A%23c8e6c9%0A"})]),fallback:n(()=>[...t[11]||(t[11]=[a(" Loading... ",-1)])]),_:1})),t[19]||(t[19]=i('<p><strong>Hình 5.15:</strong> Multi-resolution fusion trong HRNetV2</p><h3 id="_5-3-6-3-benchmark-va-uu-điem-rs" tabindex="-1">5.3.6.3. Benchmark và Ưu Điểm RS <a class="header-anchor" href="#_5-3-6-3-benchmark-va-uu-điem-rs" aria-label="Permalink to &quot;5.3.6.3. Benchmark và Ưu Điểm RS&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Dataset</th><th>Model</th><th>mIoU</th><th>Comparison</th></tr></thead><tbody><tr><td>Cityscapes</td><td>HRNetV2-W48</td><td>81.6%</td><td>&gt; DeepLabV3+</td></tr><tr><td>PASCAL Context</td><td>HRNetV2-W48</td><td>54.0%</td><td>SOTA</td></tr><tr><td>COCO (Faster R-CNN)</td><td>HRNetV2p-W48</td><td>41.8 AP</td><td>&gt; ResNet-101-FPN</td></tr></tbody></table><p><strong>Bảng 5.14:</strong> Benchmark HRNet trên segmentation và detection</p><p><strong>Ưu điểm cho viễn thám:</strong></p><ul><li><strong>Pixel-level precision:</strong> High-resolution xuyên suốt → ideal cho footprint/road/parcel mapping</li><li><strong>Dramatic small-object improvement:</strong> Vehicles, buildings appear tiny trong satellite imagery</li><li><strong>Lower computational cost:</strong> Đạt SOTA với fewer GFLOPs so với alternatives</li><li><strong>Multi-task capable:</strong> HRNetV2p variant hỗ trợ cả detection</li></ul><h2 id="_5-3-7-so-sanh-tong-hop" tabindex="-1">5.3.7. So Sánh Tổng Hợp <a class="header-anchor" href="#_5-3-7-so-sanh-tong-hop" aria-label="Permalink to &quot;5.3.7. So Sánh Tổng Hợp&quot;">​</a></h2><p>Bảng 5.15 so sánh các kiến trúc segmentation theo các tiêu chí quan trọng cho viễn thám:</p><table tabindex="0"><thead><tr><th>Model</th><th>Core Innovation</th><th>Best For</th><th>Small Objects</th><th>Efficiency</th><th>RS Sweet Spot</th></tr></thead><tbody><tr><td>U-Net</td><td>Skip connections</td><td>Boundaries, ít data</td><td>Fair</td><td>Moderate</td><td>Limited data, precise boundaries</td></tr><tr><td>DeepLabV3+</td><td>ASPP + decoder</td><td>Balance context/detail</td><td>Good</td><td>High</td><td>General-purpose RS</td></tr><tr><td>FPN</td><td>Top-down fusion</td><td>Multi-scale</td><td>Excellent</td><td>High</td><td>Scale-diverse detection</td></tr><tr><td>PSPNet</td><td>Pyramid pooling</td><td>Global context</td><td>Fair</td><td>Moderate</td><td>Complex contextual scenes</td></tr><tr><td>HRNet</td><td>Parallel HR streams</td><td>Dense high-res</td><td>Excellent</td><td>High</td><td>High-resolution dense tasks</td></tr></tbody></table><p><strong>Bảng 5.15:</strong> So sánh segmentation architectures cho viễn thám</p><h3 id="khuyen-nghi-thuc-tien" tabindex="-1">Khuyến Nghị Thực Tiễn <a class="header-anchor" href="#khuyen-nghi-thuc-tien" aria-label="Permalink to &quot;Khuyến Nghị Thực Tiễn&quot;">​</a></h3><ol><li><strong>Limited data + boundary critical:</strong> U-Net với strong augmentation</li><li><strong>General-purpose:</strong> DeepLabV3+ với SSL4EO encoder</li><li><strong>Multi-scale objects:</strong> FPN-based architectures</li><li><strong>Complex scenes:</strong> PSPNet cho global context</li><li><strong>High-resolution dense prediction:</strong> HRNet (SOTA)</li></ol><h2 id="_5-3-8-segmentation-datasets-trong-torchgeo" tabindex="-1">5.3.8. Segmentation Datasets trong TorchGeo <a class="header-anchor" href="#_5-3-8-segmentation-datasets-trong-torchgeo" aria-label="Permalink to &quot;5.3.8. Segmentation Datasets trong TorchGeo&quot;">​</a></h2><p>TorchGeo tích hợp nhiều benchmark segmentation datasets:</p><table tabindex="0"><thead><tr><th>Dataset</th><th>Source</th><th>Classes</th><th>Resolution</th><th>Coverage</th></tr></thead><tbody><tr><td>ChesapeakeCVPR</td><td>NAIP + Landsat</td><td>7</td><td>1m</td><td>Chesapeake Bay, USA</td></tr><tr><td>LandCover.ai</td><td>Aerial</td><td>5</td><td>25-50cm</td><td>Poland</td></tr><tr><td>GeoNRW</td><td>Aerial</td><td>10</td><td>1m</td><td>North Rhine-Westphalia</td></tr><tr><td>Potsdam</td><td>Aerial</td><td>6</td><td>5cm</td><td>Germany</td></tr><tr><td>Vaihingen</td><td>Aerial</td><td>6</td><td>9cm</td><td>Germany</td></tr></tbody></table><p><strong>Bảng 5.16:</strong> Benchmark segmentation datasets trong TorchGeo</p><h2 id="_5-3-9-best-practices-cho-training" tabindex="-1">5.3.9. Best Practices cho Training <a class="header-anchor" href="#_5-3-9-best-practices-cho-training" aria-label="Permalink to &quot;5.3.9. Best Practices cho Training&quot;">​</a></h2><h3 id="_5-3-9-1-xu-ly-class-imbalance" tabindex="-1">5.3.9.1. Xử Lý Class Imbalance <a class="header-anchor" href="#_5-3-9-1-xu-ly-class-imbalance" aria-label="Permalink to &quot;5.3.9.1. Xử Lý Class Imbalance&quot;">​</a></h3><p>Dữ liệu viễn thám thường có class imbalance nghiêm trọng (ví dụ: buildings chỉ chiếm &lt;5% diện tích). Các giải pháp:</p><ul><li><strong>Class weighting:</strong> Inverse frequency hoặc median frequency balancing</li><li><strong>Focal Loss:</strong> Giảm weight cho easy examples, focus vào hard cases</li><li><strong>Dice/IoU Loss:</strong> Optimize directly cho overlap metric</li><li><strong>Combination:</strong> CE + Dice thường hiệu quả nhất</li></ul><h3 id="_5-3-9-2-augmentation" tabindex="-1">5.3.9.2. Augmentation <a class="header-anchor" href="#_5-3-9-2-augmentation" aria-label="Permalink to &quot;5.3.9.2. Augmentation&quot;">​</a></h3><p><strong>Geometric augmentations</strong> (apply cùng lúc cho image và mask):</p><ul><li>Random horizontal/vertical flip</li><li>Random rotation (90°, 180°, 270°, hoặc arbitrary)</li><li>Random scale và crop</li></ul><p><strong>Photometric augmentations</strong> (chỉ apply cho image):</p><ul><li>Brightness/contrast adjustment</li><li>Gaussian noise</li><li>Blur</li></ul><p><strong>Quan trọng:</strong> Dùng nearest neighbor interpolation cho mask để bảo toàn class labels.</p><h3 id="_5-3-9-3-inference-strategies" tabindex="-1">5.3.9.3. Inference Strategies <a class="header-anchor" href="#_5-3-9-3-inference-strategies" aria-label="Permalink to &quot;5.3.9.3. Inference Strategies&quot;">​</a></h3><p><strong>Sliding Window:</strong> Chia ảnh lớn thành overlapping patches, average predictions trong overlap regions</p><p><strong>Test Time Augmentation (TTA):</strong> Apply flips và rotations tại inference, average predictions → +1-2% mIoU typical</p><p><strong>Post-processing:</strong> Morphological operations (remove small isolated regions, fill holes), CRF refinement, vectorization cho GIS export</p><h2 id="_5-3-10-ket-luan" tabindex="-1">5.3.10. Kết Luận <a class="header-anchor" href="#_5-3-10-ket-luan" aria-label="Permalink to &quot;5.3.10. Kết Luận&quot;">​</a></h2><p>Các kiến trúc segmentation trong TorchGeo cung cấp toolkit đa dạng cho pixel-level prediction trong viễn thám. U-Net vẫn là lựa chọn robust cho scenarios với ít data và yêu cầu boundary chính xác. DeepLabV3+ cung cấp balance tốt giữa context và detail cho general-purpose applications. HRNet đại diện cho SOTA khi cần highest precision với high-resolution data.</p><p>Điểm chung quan trọng là việc sử dụng pre-trained encoders (đặc biệt từ SSL4EO) giúp cải thiện đáng kể performance so với training from scratch. Chương tiếp theo sẽ chuyển sang Change Detection - ứng dụng đặc thù của viễn thám yêu cầu so sánh ảnh từ nhiều thời điểm.</p><hr><h2 id="tai-lieu-tham-khao" tabindex="-1">Tài Liệu Tham Khảo <a class="header-anchor" href="#tai-lieu-tham-khao" aria-label="Permalink to &quot;Tài Liệu Tham Khảo&quot;">​</a></h2><p>[1] Ronneberger, O., Fischer, P., &amp; Brox, T. (2015). &quot;U-Net: Convolutional Networks for Biomedical Image Segmentation.&quot; MICCAI. arXiv:1505.04597.</p><p>[2] Chen, L. C., Zhu, Y., Papandreou, G., Schroff, F., &amp; Adam, H. (2018). &quot;Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation.&quot; ECCV. arXiv:1802.02611.</p><p>[3] Lin, T. Y., Dollár, P., Girshick, R., He, K., Hariharan, B., &amp; Belongie, S. (2017). &quot;Feature Pyramid Networks for Object Detection.&quot; CVPR. arXiv:1612.03144.</p><p>[4] Zhao, H., Shi, J., Qi, X., Wang, X., &amp; Jia, J. (2017). &quot;Pyramid Scene Parsing Network.&quot; CVPR. arXiv:1612.01105.</p><p>[5] Wang, J., Sun, K., Cheng, T., et al. (2019). &quot;Deep High-Resolution Representation Learning for Visual Recognition.&quot; CVPR. arXiv:1904.04514.</p>',40))])}const D=c(m,[["render",v]]);export{_ as __pageData,D as default};
