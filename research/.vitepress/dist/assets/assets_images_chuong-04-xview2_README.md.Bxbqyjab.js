import{_ as a,c as s,o as i,a2 as t}from"./chunks/framework.nRfFlDZQ.js";const p=JSON.parse('{"title":"xView2/xBD Dataset Documentation Index","description":"","frontmatter":{},"headers":[],"relativePath":"assets/images/chuong-04-xview2/README.md","filePath":"assets/images/chuong-04-xview2/README.md","lastUpdated":1766302189000}'),n={name:"assets/images/chuong-04-xview2/README.md"};function r(l,e,o,d,c,g){return i(),s("div",null,[...e[0]||(e[0]=[t(`<h1 id="xview2-xbd-dataset-documentation-index" tabindex="-1">xView2/xBD Dataset Documentation Index <a class="header-anchor" href="#xview2-xbd-dataset-documentation-index" aria-label="Permalink to &quot;xView2/xBD Dataset Documentation Index&quot;">​</a></h1><p>Comprehensive research and documentation for the xView2/xBD satellite imagery dataset for building damage assessment after natural disasters.</p><hr><h2 id="quick-navigation" tabindex="-1">Quick Navigation <a class="header-anchor" href="#quick-navigation" aria-label="Permalink to &quot;Quick Navigation&quot;">​</a></h2><h3 id="for-first-time-users" tabindex="-1">For First-Time Users <a class="header-anchor" href="#for-first-time-users" aria-label="Permalink to &quot;For First-Time Users&quot;">​</a></h3><ol><li>Start with <strong><a href="./download-guide.html">download-guide.md</a></strong> - Fastest way to access data</li><li>See <strong><a href="./image-reference-catalog.html">image-reference-catalog.md</a></strong> - Understanding damage classification (4 levels, colors, examples)</li><li>Reference <strong><a href="./image-sources.html">image-sources.md</a></strong> - Complete source catalog</li></ol><h3 id="for-researchers" tabindex="-1">For Researchers <a class="header-anchor" href="#for-researchers" aria-label="Permalink to &quot;For Researchers&quot;">​</a></h3><ol><li><strong><a href="./image-sources.html">image-sources.md</a></strong> - Academic sources, paper figures, citations</li><li><strong><a href="./image-reference-catalog.html">image-reference-catalog.md</a></strong> - Visual characteristics, quality metrics, disaster patterns</li><li><strong><a href="./download-guide.html">download-guide.md</a></strong> - Batch download scripts and data access</li></ol><h3 id="for-developers" tabindex="-1">For Developers <a class="header-anchor" href="#for-developers" aria-label="Permalink to &quot;For Developers&quot;">​</a></h3><ol><li><strong><a href="./download-guide.html">download-guide.md</a></strong> - Code examples (Python, bash)</li><li><strong><a href="./image-reference-catalog.html">image-reference-catalog.md</a></strong> - Format specifications (PNG, JSON, CSV)</li><li><strong><a href="./image-sources.html">image-sources.md</a></strong> - Integration points (TorchGeo, Roboflow, HuggingFace)</li></ol><hr><h2 id="document-overview" tabindex="-1">Document Overview <a class="header-anchor" href="#document-overview" aria-label="Permalink to &quot;Document Overview&quot;">​</a></h2><h3 id="_1-image-sources-md-389-lines" tabindex="-1">1. image-sources.md (389 lines) <a class="header-anchor" href="#_1-image-sources-md-389-lines" aria-label="Permalink to &quot;1. image-sources.md (389 lines)&quot;">​</a></h3><p><strong>Complete Image Source Catalog</strong></p><p>Primary purpose: Identify and catalog all image sources, papers, and data access points</p><p>Contains:</p><ul><li>CVPR 2019 paper details with 10 figure descriptions</li><li>Official dataset specifications (850,736 buildings, 45,362 km²)</li><li>Damage classification scale (4 levels with visual descriptions)</li><li>5 alternative access methods (Roboflow, TorchGeo, HuggingFace, Maxar STAC)</li><li>Satellite imagery metadata (GSD, resolution, sensors)</li><li>Download instructions for each access method</li><li>Licensing &amp; attribution requirements</li><li>Related research papers (5+ citations)</li><li>Complete dataset statistics</li><li>Unresolved questions (7 items)</li></ul><p><strong>Best for</strong>:</p><ul><li>Finding authoritative sources</li><li>Understanding dataset scope and specifications</li><li>Academic citations and references</li><li>Comprehensive dataset overview</li></ul><hr><h3 id="_2-download-guide-md-400-lines" tabindex="-1">2. download-guide.md (400 lines) <a class="header-anchor" href="#_2-download-guide-md-400-lines" aria-label="Permalink to &quot;2. download-guide.md (400 lines)&quot;">​</a></h3><p><strong>Image Download &amp; Access Methods</strong></p><p>Primary purpose: Practical guide for accessing and downloading images</p><p>Contains:</p><ul><li>4 public access methods (no/minimal registration)</li><li>Official portal step-by-step download (full dataset)</li><li>JSON annotation format documentation</li><li>CSV metadata format reference</li><li>PNG image specifications (1024×1024, RGB)</li><li>Output format for predictions (grayscale 0-4)</li><li>3 complete bash/Python download scripts</li><li>TorchGeo Python integration code</li><li>Hugging Face dataset loader</li><li>Image loading examples (OpenCV, JSON parsing)</li><li>Troubleshooting guide (8 issues + solutions)</li><li>Storage requirements by tier (4GB-11GB)</li><li>Legal and attribution guidance</li></ul><p><strong>Best for</strong>:</p><ul><li>Getting started quickly</li><li>Downloading images (multiple methods)</li><li>Data processing and loading</li><li>Integration with ML pipelines</li><li>Troubleshooting download/access issues</li></ul><hr><h3 id="_3-image-reference-catalog-md-483-lines" tabindex="-1">3. image-reference-catalog.md (483 lines) <a class="header-anchor" href="#_3-image-reference-catalog-md-483-lines" aria-label="Permalink to &quot;3. image-reference-catalog.md (483 lines)&quot;">​</a></h3><p><strong>Image Types, Formats, and Visual Reference</strong></p><p>Primary purpose: Detailed visual and technical documentation of images</p><p>Contains:</p><ul><li>Pre-disaster satellite image specifications</li><li>Post-disaster satellite image characteristics</li><li>Building polygon annotation format (GeoJSON)</li><li>4-level damage scale with visual examples: <ul><li>Level 0 (Green): No Damage - 313,033 polygons (84%)</li><li>Level 1 (Blue): Minor Damage - 36,860 polygons (5%)</li><li>Level 2 (Orange): Major Damage - 29,904 polygons (4%)</li><li>Level 3 (Red): Destroyed - 31,560 polygons (4%)</li></ul></li><li>Color visualization codes (RGB hex values)</li><li>Disaster-specific damage patterns (earthquake, fire, flood, wind, volcano)</li><li>Geographic distribution across events</li><li>Image quality metrics (GSD, off-nadir, sun elevation)</li><li>Annotation quality statistics</li><li>Class imbalance challenges and solutions</li><li>Performance baselines by class</li><li>PNG/JSON/CSV format specifications</li><li>Visual artifacts and limitations</li><li>When/why to use xBD dataset</li></ul><p><strong>Best for</strong>:</p><ul><li>Understanding damage classification visually</li><li>Learning disaster-specific patterns</li><li>Format/specification reference</li><li>Quality and limitation assessment</li><li>Model performance expectations</li></ul><hr><h2 id="dataset-at-a-glance" tabindex="-1">Dataset at a Glance <a class="header-anchor" href="#dataset-at-a-glance" aria-label="Permalink to &quot;Dataset at a Glance&quot;">​</a></h2><table tabindex="0"><thead><tr><th>Metric</th><th>Value</th></tr></thead><tbody><tr><td><strong>Total Images</strong></td><td>22,068 (1024×1024 RGB)</td></tr><tr><td><strong>Image Pairs</strong></td><td>11,034 (pre/post-disaster)</td></tr><tr><td><strong>Building Polygons</strong></td><td>850,736</td></tr><tr><td><strong>Geographic Coverage</strong></td><td>45,362 km² across 15+ countries</td></tr><tr><td><strong>Disaster Events</strong></td><td>19 (across 6 types)</td></tr><tr><td><strong>Damage Classes</strong></td><td>4 levels (0-3 scale)</td></tr><tr><td><strong>Ground Sampling Distance</strong></td><td>&lt;0.8 meters</td></tr><tr><td><strong>Download Size</strong></td><td>10 GB compressed, 11 GB uncompressed</td></tr><tr><td><strong>Training Pairs</strong></td><td>8,399 (Tier 1 + Tier 3)</td></tr><tr><td><strong>Test Pairs</strong></td><td>933</td></tr></tbody></table><hr><h2 id="damage-classification-quick-reference" tabindex="-1">Damage Classification Quick Reference <a class="header-anchor" href="#damage-classification-quick-reference" aria-label="Permalink to &quot;Damage Classification Quick Reference&quot;">​</a></h2><h3 id="color-coded-scale" tabindex="-1">Color-Coded Scale <a class="header-anchor" href="#color-coded-scale" aria-label="Permalink to &quot;Color-Coded Scale&quot;">​</a></h3><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Level 0 (No Damage)  → GREEN   (#00AA00) - 84% of labels</span></span>
<span class="line"><span>Level 1 (Minor)      → BLUE    (#0000FF) - 5% of labels</span></span>
<span class="line"><span>Level 2 (Major)      → ORANGE  (#FF8800) - 4% of labels</span></span>
<span class="line"><span>Level 3 (Destroyed)  → RED     (#FF0000) - 4% of labels</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><h3 id="visual-characteristics" tabindex="-1">Visual Characteristics <a class="header-anchor" href="#visual-characteristics" aria-label="Permalink to &quot;Visual Characteristics&quot;">​</a></h3><p><strong>Green (No Damage)</strong>: Building stands, roof intact, colors match pre-disaster</p><p><strong>Blue (Minor Damage)</strong>: Some roof elements missing, visible cracks, building recognizable</p><p><strong>Orange (Major Damage)</strong>: Partial collapse, debris visible, significant structural damage</p><p><strong>Red (Destroyed)</strong>: Complete collapse, only rubble visible, foundation-level destruction</p><hr><h2 id="file-organization" tabindex="-1">File Organization <a class="header-anchor" href="#file-organization" aria-label="Permalink to &quot;File Organization&quot;">​</a></h2><div class="language- vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>/home/tchatb/sen_doc/docs/assets/images/chuong-04-xview2/</span></span>
<span class="line"><span>│</span></span>
<span class="line"><span>├── README.md                        ← You are here</span></span>
<span class="line"><span>├── image-sources.md                 ← Source catalog &amp; references</span></span>
<span class="line"><span>├── download-guide.md                ← Download methods &amp; code</span></span>
<span class="line"><span>├── image-reference-catalog.md       ← Visual reference &amp; formats</span></span>
<span class="line"><span>│</span></span>
<span class="line"><span>├── dataset/                         ← For downloaded images</span></span>
<span class="line"><span>│   ├── [pre-disaster .png files]</span></span>
<span class="line"><span>│   ├── [post-disaster .png files]</span></span>
<span class="line"><span>│   └── [annotation .json files]</span></span>
<span class="line"><span>│</span></span>
<span class="line"><span>└── solutions/                       ← Reference implementations</span></span>
<span class="line"><span>    └── [example code/notebooks]</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br></div></div><hr><h2 id="access-methods-summary" tabindex="-1">Access Methods Summary <a class="header-anchor" href="#access-methods-summary" aria-label="Permalink to &quot;Access Methods Summary&quot;">​</a></h2><h3 id="method-1-roboflow-easiest-no-login" tabindex="-1">Method 1: Roboflow (Easiest - No Login) <a class="header-anchor" href="#method-1-roboflow-easiest-no-login" aria-label="Permalink to &quot;Method 1: Roboflow (Easiest - No Login)&quot;">​</a></h3><ul><li><strong>Images</strong>: 520 samples</li><li><strong>License</strong>: CC BY 4.0</li><li><strong>Access</strong>: <a href="https://universe.roboflow.com/ozu/xview2" target="_blank" rel="noreferrer">https://universe.roboflow.com/ozu/xview2</a></li><li><strong>Time</strong>: &lt;5 minutes</li><li><strong>Best for</strong>: Quick experimentation</li></ul><h3 id="method-2-torchgeo-python-library" tabindex="-1">Method 2: TorchGeo (Python Library) <a class="header-anchor" href="#method-2-torchgeo-python-library" aria-label="Permalink to &quot;Method 2: TorchGeo (Python Library)&quot;">​</a></h3><ul><li><strong>Images</strong>: Full dataset auto-download</li><li><strong>Installation</strong>: <code>pip install torchgeo</code></li><li><strong>Access</strong>: Automatic on first use</li><li><strong>Time</strong>: 15-30 minutes</li><li><strong>Best for</strong>: ML/PyTorch integration</li></ul><h3 id="method-3-xview2-official-portal-complete-dataset" tabindex="-1">Method 3: xView2 Official Portal (Complete Dataset) <a class="header-anchor" href="#method-3-xview2-official-portal-complete-dataset" aria-label="Permalink to &quot;Method 3: xView2 Official Portal (Complete Dataset)&quot;">​</a></h3><ul><li><strong>Images</strong>: 11,034 pairs (full dataset)</li><li><strong>Access</strong>: <a href="https://xview2.org/dataset" target="_blank" rel="noreferrer">https://xview2.org/dataset</a></li><li><strong>Registration</strong>: Required + license acceptance</li><li><strong>Time</strong>: 1-2 hours download</li><li><strong>Best for</strong>: Production/research use</li></ul><h3 id="method-4-hugging-face" tabindex="-1">Method 4: Hugging Face <a class="header-anchor" href="#method-4-hugging-face" aria-label="Permalink to &quot;Method 4: Hugging Face&quot;">​</a></h3><ul><li><strong>Images</strong>: Full dataset (parquet format)</li><li><strong>Access</strong>: <a href="https://huggingface.co/datasets/danielz01/xView2" target="_blank" rel="noreferrer">https://huggingface.co/datasets/danielz01/xView2</a></li><li><strong>Registration</strong>: HF account + license acceptance</li><li><strong>Time</strong>: Variable</li><li><strong>Best for</strong>: ML pipelines with HF integration</li></ul><h3 id="method-5-maxar-stac-catalog-advanced" tabindex="-1">Method 5: Maxar STAC Catalog (Advanced) <a class="header-anchor" href="#method-5-maxar-stac-catalog-advanced" aria-label="Permalink to &quot;Method 5: Maxar STAC Catalog (Advanced)&quot;">​</a></h3><ul><li><strong>Images</strong>: Original satellite data</li><li><strong>Format</strong>: GeoTIFF/COG</li><li><strong>Access</strong>: API-based, direct S3</li><li><strong>Registration</strong>: Optional</li><li><strong>Time</strong>: Custom (per image)</li><li><strong>Best for</strong>: GIS professionals, custom analysis</li></ul><hr><h2 id="documentation-statistics" tabindex="-1">Documentation Statistics <a class="header-anchor" href="#documentation-statistics" aria-label="Permalink to &quot;Documentation Statistics&quot;">​</a></h2><table tabindex="0"><thead><tr><th>Document</th><th>Lines</th><th>Focus</th><th>Audience</th></tr></thead><tbody><tr><td>image-sources.md</td><td>389</td><td>Academic sources, dataset specs, citations</td><td>Researchers, academicians</td></tr><tr><td>download-guide.md</td><td>400</td><td>Practical access methods, code examples</td><td>Developers, practitioners</td></tr><tr><td>image-reference-catalog.md</td><td>483</td><td>Visual reference, formats, specifications</td><td>Data scientists, ML engineers</td></tr><tr><td><strong>Total</strong></td><td><strong>1272</strong></td><td>Comprehensive xBD documentation</td><td>All users</td></tr></tbody></table><hr><h2 id="key-statistics" tabindex="-1">Key Statistics <a class="header-anchor" href="#key-statistics" aria-label="Permalink to &quot;Key Statistics&quot;">​</a></h2><h3 id="dataset-composition" tabindex="-1">Dataset Composition <a class="header-anchor" href="#dataset-composition" aria-label="Permalink to &quot;Dataset Composition&quot;">​</a></h3><ul><li><strong>No Damage</strong>: 313,033 polygons (84%) - Severely imbalanced</li><li><strong>Minor Damage</strong>: 36,860 polygons (5%)</li><li><strong>Major Damage</strong>: 29,904 polygons (4%)</li><li><strong>Destroyed</strong>: 31,560 polygons (4%)</li><li><strong>Unclassified</strong>: 14,011 polygons (3%)</li></ul><h3 id="disaster-types" tabindex="-1">Disaster Types <a class="header-anchor" href="#disaster-types" aria-label="Permalink to &quot;Disaster Types&quot;">​</a></h3><ul><li>Earthquake/Tsunami (4 events)</li><li>Wildfire (3 events)</li><li>Flooding (2+ events)</li><li>Volcanic Eruption (2 events)</li><li>Wind/Hurricane (5 events)</li><li>Landslide/Other (4 events)</li></ul><h3 id="model-performance-baselines" tabindex="-1">Model Performance Baselines <a class="header-anchor" href="#model-performance-baselines" aria-label="Permalink to &quot;Model Performance Baselines&quot;">​</a></h3><ul><li><strong>Best Localization F1</strong>: 0.80 (U-Net baseline)</li><li><strong>Best Classification F1</strong>: 0.66 (ResNet50 baseline)</li><li><strong>Combined Weighted F1</strong>: ~0.71 (best challenge submissions)</li><li><strong>No Damage F1</strong>: 0.81 (easiest class)</li><li><strong>Minor Damage F1</strong>: 0.42 (hardest class - high confusion)</li></ul><hr><h2 id="important-limitations" tabindex="-1">Important Limitations <a class="header-anchor" href="#important-limitations" aria-label="Permalink to &quot;Important Limitations&quot;">​</a></h2><ol><li><strong>Class Imbalance</strong>: 84% of labels are &quot;no damage&quot; (9.9:1 destroyed ratio)</li><li><strong>Visual Ambiguity</strong>: Minor vs. major damage have subtle differences</li><li><strong>RGB-Only</strong>: No multispectral or thermal bands</li><li><strong>Realistic Angles</strong>: Off-nadir and varied sun elevation (not nadir/ideal)</li><li><strong>Subjective Boundaries</strong>: Annotation guidelines have inherent subjectivity</li></ol><hr><h2 id="real-world-impact" tabindex="-1">Real-World Impact <a class="header-anchor" href="#real-world-impact" aria-label="Permalink to &quot;Real-World Impact&quot;">​</a></h2><p><strong>California Wildfire Response</strong>:</p><ul><li><strong>Time Saved</strong>: ~98% reduction (10-20 minutes vs 1-2 days per area)</li><li><strong>Deployment</strong>: California National Guard actively using xView2 models</li><li><strong>Coverage</strong>: Successfully assessing large-scale wildfire damage</li><li><strong>Accuracy</strong>: Models provide actionable intelligence for emergency response</li></ul><hr><h2 id="getting-started-checklist" tabindex="-1">Getting Started Checklist <a class="header-anchor" href="#getting-started-checklist" aria-label="Permalink to &quot;Getting Started Checklist&quot;">​</a></h2><ul><li>[ ] Read this README</li><li>[ ] Choose access method (Roboflow for quick start, xView2 portal for complete)</li><li>[ ] Follow <strong>download-guide.md</strong> for your chosen method</li><li>[ ] Review <strong>image-reference-catalog.md</strong> to understand damage levels</li><li>[ ] Reference <strong>image-sources.md</strong> for papers/citations</li><li>[ ] Load sample image pair and inspect with provided code</li><li>[ ] Review damage classification examples</li><li>[ ] Consider class imbalance for your use case</li></ul><hr><h2 id="citation" tabindex="-1">Citation <a class="header-anchor" href="#citation" aria-label="Permalink to &quot;Citation&quot;">​</a></h2><p><strong>Paper</strong>: Gupta, R., et al. (2019). &quot;Creating xBD: A Dataset for Assessing Building Damage from Satellite Imagery.&quot; In CVPR Workshops, pp. 18-26.</p><p><strong>BibTeX</strong>:</p><div class="language-bibtex vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">bibtex</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@inproceedings</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">gupta2019xbd</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  title</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Creating xBD: A Dataset for Assessing Building Damage from Satellite Imagery</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  author</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Gupta, Ritwik and Goodman, Bryce and Patel, Nirav and Hosfelt, Ricky and Sajeev, Sandra and Heim, Eric and Doshi, Jigar and Lucas, Keane and Choset, Howie and Gaston, Matthew</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  booktitle</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  pages</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">18--26</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">  year</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">{</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">2019</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">}</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p><strong>Attribution</strong>: Satellite imagery sourced from Maxar/DigitalGlobe Open Data Program</p><hr><h2 id="quick-links" tabindex="-1">Quick Links <a class="header-anchor" href="#quick-links" aria-label="Permalink to &quot;Quick Links&quot;">​</a></h2><h3 id="official-resources" tabindex="-1">Official Resources <a class="header-anchor" href="#official-resources" aria-label="Permalink to &quot;Official Resources&quot;">​</a></h3><ul><li><a href="https://xview2.org/" target="_blank" rel="noreferrer">xView2 Challenge Portal</a></li><li><a href="https://xview2.org/dataset" target="_blank" rel="noreferrer">xView2 Dataset Page</a></li><li><a href="https://arxiv.org/abs/1911.09296" target="_blank" rel="noreferrer">CVPR 2019 Paper</a></li><li><a href="https://www.digitalglobe.com/ecosystem/open-data" target="_blank" rel="noreferrer">Maxar Open Data</a></li></ul><h3 id="alternative-access" tabindex="-1">Alternative Access <a class="header-anchor" href="#alternative-access" aria-label="Permalink to &quot;Alternative Access&quot;">​</a></h3><ul><li><a href="https://universe.roboflow.com/ozu/xview2" target="_blank" rel="noreferrer">Roboflow Universe</a></li><li><a href="https://huggingface.co/datasets/danielz01/xView2" target="_blank" rel="noreferrer">Hugging Face Dataset</a></li><li><a href="https://torchgeo.readthedocs.io/" target="_blank" rel="noreferrer">TorchGeo Documentation</a></li></ul><h3 id="reference-implementations" tabindex="-1">Reference Implementations <a class="header-anchor" href="#reference-implementations" aria-label="Permalink to &quot;Reference Implementations&quot;">​</a></h3><ul><li><a href="https://github.com/DIUx-xView/xView2_baseline" target="_blank" rel="noreferrer">Official Baseline</a></li><li><a href="https://github.com/DIUx-xView/xView2_first_place" target="_blank" rel="noreferrer">1st Place Solution</a></li><li><a href="https://github.com/ethanweber/xview2" target="_blank" rel="noreferrer">2nd Place Solution</a></li><li><a href="https://github.com/ashnair1/xview2-toolkit" target="_blank" rel="noreferrer">Toolkit &amp; Utilities</a></li></ul><h3 id="research-papers" tabindex="-1">Research Papers <a class="header-anchor" href="#research-papers" aria-label="Permalink to &quot;Research Papers&quot;">​</a></h3><ul><li><a href="https://arxiv.org/abs/1911.09296" target="_blank" rel="noreferrer">xBD Paper (arXiv)</a></li><li><a href="https://arxiv.org/pdf/2212.13876" target="_blank" rel="noreferrer">xFBD Related Work</a></li><li><a href="https://arxiv.org/html/2405.04800v1" target="_blank" rel="noreferrer">DeepDamageNet</a></li></ul><hr><h2 id="document-maintenance" tabindex="-1">Document Maintenance <a class="header-anchor" href="#document-maintenance" aria-label="Permalink to &quot;Document Maintenance&quot;">​</a></h2><p><strong>Last Updated</strong>: 2025-12-19 <strong>Research Scope</strong>: Complete xBD/xView2 dataset documentation <strong>All URLs</strong>: Verified active as of research date <strong>Sources</strong>: 13+ primary sources catalogued</p><p><strong>Notes</strong>: No actual image downloads performed (protected by registration). All URLs, access methods, specifications, and metadata documented for reference.</p><hr><h2 id="navigation-tips" tabindex="-1">Navigation Tips <a class="header-anchor" href="#navigation-tips" aria-label="Permalink to &quot;Navigation Tips&quot;">​</a></h2><ul><li><strong>Need a specific image type?</strong> → See image-reference-catalog.md</li><li><strong>Want to download data?</strong> → See download-guide.md</li><li><strong>Looking for papers/citations?</strong> → See image-sources.md</li><li><strong>Understanding damage levels?</strong> → See image-reference-catalog.md (damage scale section)</li><li><strong>Python code examples?</strong> → See download-guide.md (scripts section)</li><li><strong>Complete dataset specs?</strong> → See image-sources.md (dataset statistics table)</li></ul>`,106)])])}const m=a(n,[["render",r]]);export{p as __pageData,m as default};
