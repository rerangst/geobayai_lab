import{_ as t,c as o,o as e,a2 as i}from"./chunks/framework.nRfFlDZQ.js";const u=JSON.parse('{"title":"Chương 2: Phân đoạn Ngữ nghĩa (Semantic Segmentation) trong Viễn thám","description":"","frontmatter":{},"headers":[],"relativePath":"chuong-02-co-so-ly-thuyet/muc-02-phuong-phap-xu-ly-anh/03-phan-doan-ngu-nghia.md","filePath":"chuong-02-co-so-ly-thuyet/muc-02-phuong-phap-xu-ly-anh/03-phan-doan-ngu-nghia.md","lastUpdated":null}'),a={name:"chuong-02-co-so-ly-thuyet/muc-02-phuong-phap-xu-ly-anh/03-phan-doan-ngu-nghia.md"};function c(h,n,r,g,l,s){return e(),o("div",null,[...n[0]||(n[0]=[i('<h1 id="chuong-2-phan-đoan-ngu-nghia-semantic-segmentation-trong-vien-tham" tabindex="-1">Chương 2: Phân đoạn Ngữ nghĩa (Semantic Segmentation) trong Viễn thám <a class="header-anchor" href="#chuong-2-phan-đoan-ngu-nghia-semantic-segmentation-trong-vien-tham" aria-label="Permalink to &quot;Chương 2: Phân đoạn Ngữ nghĩa (Semantic Segmentation) trong Viễn thám&quot;">​</a></h1><h2 id="_3-12-đinh-nghia-bai-toan-semantic-segmentation" tabindex="-1">3.12. Định nghĩa Bài toán Semantic Segmentation <a class="header-anchor" href="#_3-12-đinh-nghia-bai-toan-semantic-segmentation" aria-label="Permalink to &quot;3.12. Định nghĩa Bài toán Semantic Segmentation&quot;">​</a></h2><p>Semantic Segmentation là bài toán gán nhãn lớp cho từng pixel trong ảnh, tạo ra một dense prediction map có cùng kích thước không gian với ảnh đầu vào. Khác với image classification gán một nhãn cho toàn bộ ảnh, và object detection định vị đối tượng bằng bounding box, semantic segmentation cung cấp thông tin chi tiết nhất về ranh giới và hình dạng của các vùng trong ảnh. Mỗi pixel được gán vào một trong K lớp được định nghĩa trước, tạo thành một segmentation mask.</p><p>Về mặt hình thức, cho ảnh đầu vào X có kích thước H×W×C, output của semantic segmentation là một label map Y có kích thước H×W, trong đó Y[i,j] ∈ {1, 2, ..., K} là nhãn lớp của pixel tại vị trí (i,j). Trong quá trình inference, mạng thường output một tensor có kích thước H×W×K chứa xác suất của mỗi pixel thuộc mỗi lớp, và nhãn cuối cùng được xác định bằng argmax theo chiều lớp.</p><p>Một đặc điểm quan trọng của semantic segmentation là không phân biệt các instance riêng lẻ của cùng một lớp. Nếu có hai con tàu trong ảnh, cả hai đều được gán nhãn &quot;tàu&quot; và thuộc cùng một vùng connected component (nếu chúng chạm nhau). Việc phân biệt từng instance là nhiệm vụ của instance segmentation, sẽ được trình bày ở phần sau.</p><h2 id="_3-13-ung-dung-segmentation-trong-vien-tham" tabindex="-1">3.13. Ứng dụng Segmentation trong Viễn thám <a class="header-anchor" href="#_3-13-ung-dung-segmentation-trong-vien-tham" aria-label="Permalink to &quot;3.13. Ứng dụng Segmentation trong Viễn thám&quot;">​</a></h2><h3 id="_3-13-1-phat-hien-va-phan-vung-dau-loang-oil-spill-segmentation" tabindex="-1">3.13.1. Phát hiện và Phân vùng Dầu loang (Oil Spill Segmentation) <a class="header-anchor" href="#_3-13-1-phat-hien-va-phan-vung-dau-loang-oil-spill-segmentation" aria-label="Permalink to &quot;3.13.1. Phát hiện và Phân vùng Dầu loang (Oil Spill Segmentation)&quot;">​</a></h3><p>Oil spill segmentation là một trong những ứng dụng quan trọng nhất của semantic segmentation trong giám sát biển. Bài toán yêu cầu phân loại từng pixel của ảnh SAR thành các lớp: oil spill, look-alike, water, land, và đôi khi ship. Output là mask chính xác về vị trí, hình dạng, và diện tích của vết dầu loang, thông tin quan trọng cho việc ước tính thiệt hại và lập kế hoạch ứng phó.</p><p>Ưu điểm của segmentation so với detection (bounding box) cho oil spill là khả năng capture hình dạng bất định của vết dầu. Dầu loang thường có hình dạng phức tạp, kéo dài theo hướng gió và dòng chảy, không phù hợp với bounding box hình chữ nhật. Segmentation mask cho phép tính toán chính xác diện tích và volume ước tính của dầu tràn.</p><p>Các mô hình như U-Net, DeepLabV3+, và các biến thể với attention mechanism đã đạt được IoU trên 90% cho oil spill segmentation trên các benchmark datasets.</p><h3 id="_3-13-2-lap-ban-đo-ngap-lut-flood-mapping" tabindex="-1">3.13.2. Lập bản đồ Ngập lụt (Flood Mapping) <a class="header-anchor" href="#_3-13-2-lap-ban-đo-ngap-lut-flood-mapping" aria-label="Permalink to &quot;3.13.2. Lập bản đồ Ngập lụt (Flood Mapping)&quot;">​</a></h3><p>Flood mapping là ứng dụng quan trọng trong quản lý thiên tai, sử dụng ảnh SAR để phát hiện và phân vùng các khu vực bị ngập. SAR được ưu tiên do khả năng chụp xuyên mây, đặc biệt quan trọng trong các sự kiện thời tiết cực đoan khi ảnh quang học không khả dụng.</p><p>Bài toán thường được formulate như binary segmentation (flooded vs non-flooded) hoặc multi-class segmentation (permanent water, flood water, land, urban). Change detection giữa ảnh pre-event và post-event cũng được sử dụng để xác định vùng mới bị ngập.</p><h3 id="_3-13-3-trich-xuat-building-footprint" tabindex="-1">3.13.3. Trích xuất Building Footprint <a class="header-anchor" href="#_3-13-3-trich-xuat-building-footprint" aria-label="Permalink to &quot;3.13.3. Trích xuất Building Footprint&quot;">​</a></h3><p>Building footprint extraction là bài toán phân vùng các tòa nhà từ ảnh vệ tinh hoặc ảnh máy bay độ phân giải cao. Output là binary mask (building vs background) hoặc multi-class mask phân biệt các loại công trình. Ứng dụng bao gồm: cập nhật bản đồ đô thị, ước tính dân số, đánh giá thiệt hại sau thiên tai, và quy hoạch đô thị.</p><p>Thách thức đặc thù của building segmentation bao gồm: sự đa dạng về kích thước và hình dạng công trình, shadows của các tòa nhà cao tầng, occlusion bởi cây xanh, và sự nhầm lẫn với các bề mặt tương tự như bãi đỗ xe và sân thể thao.</p><h3 id="_3-13-4-phan-loai-lop-phu-mat-đat-land-cover-mapping" tabindex="-1">3.13.4. Phân loại Lớp phủ Mặt đất (Land Cover Mapping) <a class="header-anchor" href="#_3-13-4-phan-loai-lop-phu-mat-đat-land-cover-mapping" aria-label="Permalink to &quot;3.13.4. Phân loại Lớp phủ Mặt đất (Land Cover Mapping)&quot;">​</a></h3><p>Land cover segmentation là phiên bản pixel-level của land cover classification, gán mỗi pixel vào một lớp lớp phủ mặt đất. Các lớp phổ biến bao gồm: rừng, đất nông nghiệp, đô thị, mặt nước, đất trống, và các phân lớp chi tiết hơn. Kết quả là bản đồ land cover chi tiết phục vụ cho nhiều ứng dụng về môi trường và quy hoạch.</p><p>Ảnh đa phổ từ Sentinel-2 với 13 kênh cung cấp thông tin phong phú cho land cover segmentation. Các chỉ số phổ như NDVI (Normalized Difference Vegetation Index) có thể được tính và sử dụng như input bổ sung hoặc feature engineering.</p><h2 id="_3-14-kien-truc-encoder-decoder" tabindex="-1">3.14. Kiến trúc Encoder-Decoder <a class="header-anchor" href="#_3-14-kien-truc-encoder-decoder" aria-label="Permalink to &quot;3.14. Kiến trúc Encoder-Decoder&quot;">​</a></h2><h3 id="_3-14-1-nguyen-ly-chung" tabindex="-1">3.14.1. Nguyên lý Chung <a class="header-anchor" href="#_3-14-1-nguyen-ly-chung" aria-label="Permalink to &quot;3.14.1. Nguyên lý Chung&quot;">​</a></h3><p>Hầu hết các kiến trúc semantic segmentation hiện đại tuân theo paradigm encoder-decoder. Encoder (thường là một backbone như ResNet) nhận ảnh đầu vào và tạo ra feature map có độ phân giải thấp nhưng giàu thông tin semantic. Decoder nhận feature map này và dần dần upscale về kích thước gốc, tạo ra dense prediction.</p><p>Quá trình encoding giảm độ phân giải không gian qua các lớp pooling hoặc strided convolution (thường giảm 16× hoặc 32× so với ảnh gốc) nhưng tăng số kênh để capture thông tin semantic phong phú. Quá trình decoding sử dụng upsampling (bilinear interpolation, transposed convolution, hoặc pixel shuffle) để khôi phục độ phân giải không gian.</p><p>Một thách thức quan trọng là thông tin spatial chi tiết (như biên đối tượng) bị mất trong quá trình encoding. Các kiến trúc khác nhau có các cơ chế khác nhau để bảo toàn và khôi phục thông tin này, điển hình là skip connections.</p><h3 id="_3-14-2-u-net" tabindex="-1">3.14.2. U-Net <a class="header-anchor" href="#_3-14-2-u-net" aria-label="Permalink to &quot;3.14.2. U-Net&quot;">​</a></h3><p>U-Net là kiến trúc encoder-decoder kinh điển, ban đầu được thiết kế cho biomedical image segmentation nhưng đã trở thành baseline phổ biến cho nhiều bài toán segmentation, bao gồm viễn thám.</p><p><strong>Kiến trúc:</strong> U-Net có cấu trúc đối xứng hình chữ U với encoder bên trái và decoder bên phải. Encoder gồm các block convolution theo sau bởi max pooling 2×2, giảm resolution 2× tại mỗi level (thường 4-5 levels). Decoder gồm các block upsampling (transposed convolution 2×2) theo sau bởi convolution, tăng resolution 2× tại mỗi level.</p><p><strong>Skip Connections:</strong> Đặc trưng quan trọng nhất của U-Net là skip connections nối trực tiếp từ encoder sang decoder tại mỗi level có cùng resolution. Feature map từ encoder được concatenate với feature map từ decoder trước khi đưa vào các lớp convolution tiếp theo. Skip connections cho phép decoder truy cập trực tiếp vào các low-level features (như cạnh và texture) từ encoder, cải thiện đáng kể độ chính xác của biên đối tượng trong output.</p><p><strong>Ưu điểm cho viễn thám:</strong> U-Net đặc biệt phù hợp với các bài toán viễn thám có ít dữ liệu training do kiến trúc hiệu quả và khả năng học tốt với limited data. Data augmentation extensive được áp dụng trong paper gốc, một practice vẫn quan trọng cho viễn thám.</p><h3 id="_3-14-3-deeplabv3" tabindex="-1">3.14.3. DeepLabV3+ <a class="header-anchor" href="#_3-14-3-deeplabv3" aria-label="Permalink to &quot;3.14.3. DeepLabV3+&quot;">​</a></h3><p>DeepLabV3+ là state-of-the-art semantic segmentation model được phát triển bởi Google, kết hợp Atrous Spatial Pyramid Pooling (ASPP) với encoder-decoder structure.</p><p><strong>Atrous Convolution (Dilated Convolution):</strong> Thay vì convolution thông thường, atrous convolution chèn &quot;lỗ hổng&quot; (zeros) giữa các weights của kernel, hiệu quả tăng receptive field mà không tăng số lượng parameters hoặc giảm resolution. Atrous convolution với rate r tương đương với việc upscale kernel từ k×k lên (k + (k-1)×(r-1))×(...).</p><p><strong>ASPP Module:</strong> ASPP áp dụng song song nhiều atrous convolution với các rate khác nhau (ví dụ 1, 6, 12, 18) trên cùng feature map, sau đó concatenate kết quả. Điều này cho phép capture multi-scale context, quan trọng cho việc segmentation các đối tượng có kích thước đa dạng. Một branch pooling toàn cục cũng được thêm vào để capture image-level context.</p><p><strong>Encoder-Decoder với ASPP:</strong> DeepLabV3+ kết hợp ASPP với decoder module. Low-level features từ encoder (thường từ output của stage sớm trong backbone) được concatenate với upsampled output của ASPP, sau đó đi qua các lớp convolution và upsampling cuối cùng để tạo ra full-resolution prediction.</p><p><strong>Backbone:</strong> DeepLabV3+ thường sử dụng modified ResNet hoặc Xception làm backbone. Các lớp pooling cuối được thay bằng atrous convolution để duy trì resolution cao hơn trong encoder (output stride 8 hoặc 16 thay vì 32).</p><h3 id="_3-14-4-fpn-cho-segmentation" tabindex="-1">3.14.4. FPN cho Segmentation <a class="header-anchor" href="#_3-14-4-fpn-cho-segmentation" aria-label="Permalink to &quot;3.14.4. FPN cho Segmentation&quot;">​</a></h3><p>Feature Pyramid Network (FPN), ban đầu được thiết kế cho object detection, cũng được áp dụng hiệu quả cho semantic segmentation. Kiến trúc xây dựng multi-scale feature maps với cả low-level detail và high-level semantics tại mọi scale.</p><p><strong>Panoptic FPN:</strong> Là extension của FPN cho segmentation, thêm một lightweight semantic segmentation branch lên trên FPN features. Branch này upsample và merge predictions từ tất cả các levels của pyramid.</p><p><strong>FPN trong TorchGeo:</strong> TorchGeo cung cấp FPN-based segmentation models với various backbones, phù hợp cho các bài toán viễn thám đòi hỏi xử lý đối tượng đa tỷ lệ.</p><h3 id="_3-14-5-cac-kien-truc-khac" tabindex="-1">3.14.5. Các Kiến trúc Khác <a class="header-anchor" href="#_3-14-5-cac-kien-truc-khac" aria-label="Permalink to &quot;3.14.5. Các Kiến trúc Khác&quot;">​</a></h3><p><strong>PSPNet (Pyramid Scene Parsing Network):</strong> Sử dụng Pyramid Pooling Module với 4 scale khác nhau (1×1, 2×2, 3×3, 6×6 pooling) để aggregate global context trước khi upsampling.</p><p><strong>HRNet (High-Resolution Network):</strong> Duy trì high-resolution representations throughout network thay vì giảm rồi khôi phục resolution như encoder-decoder truyền thống. Các parallel branches với resolutions khác nhau liên tục trao đổi thông tin.</p><p><strong>SegFormer:</strong> Kết hợp hierarchical Transformer encoder với lightweight MLP decoder, đạt state-of-the-art trên nhiều benchmarks.</p><h2 id="_3-15-loss-function-cho-segmentation" tabindex="-1">3.15. Loss Function cho Segmentation <a class="header-anchor" href="#_3-15-loss-function-cho-segmentation" aria-label="Permalink to &quot;3.15. Loss Function cho Segmentation&quot;">​</a></h2><h3 id="_3-15-1-pixel-wise-cross-entropy" tabindex="-1">3.15.1. Pixel-wise Cross-Entropy <a class="header-anchor" href="#_3-15-1-pixel-wise-cross-entropy" aria-label="Permalink to &quot;3.15.1. Pixel-wise Cross-Entropy&quot;">​</a></h3><p>Áp dụng Cross-Entropy loss cho từng pixel độc lập:</p><p>L = -(1/N) × Σᵢ Σⱼ Σₖ yᵢⱼₖ × log(pᵢⱼₖ)</p><p>trong đó N là tổng số pixels, yᵢⱼₖ là ground truth (1 nếu pixel (i,j) thuộc lớp k, 0 nếu không), và pᵢⱼₖ là predicted probability.</p><p><strong>Class-weighted Cross-Entropy:</strong> Để xử lý class imbalance (ví dụ background nhiều hơn nhiều so với oil spill), weights được gán cho mỗi lớp, thường inversely proportional với class frequency:</p><p>L = -(1/N) × Σᵢ Σⱼ Σₖ wₖ × yᵢⱼₖ × log(pᵢⱼₖ)</p><h3 id="_3-15-2-dice-loss" tabindex="-1">3.15.2. Dice Loss <a class="header-anchor" href="#_3-15-2-dice-loss" aria-label="Permalink to &quot;3.15.2. Dice Loss&quot;">​</a></h3><p>Dice Loss dựa trên Dice coefficient (hay F1-Score), đo overlap giữa prediction và ground truth:</p><p>Dice = 2 × |P ∩ G| / (|P| + |G|) Dice Loss = 1 - Dice</p><p>Đối với soft prediction (probabilities thay vì hard labels):</p><p>Dice = 2 × Σ pᵢ × gᵢ / (Σ pᵢ + Σ gᵢ)</p><p>Dice Loss ít bị ảnh hưởng bởi class imbalance hơn Cross-Entropy do nó đo ratio thay vì absolute numbers. Dice Loss thường được sử dụng cho binary segmentation hoặc tính riêng cho mỗi lớp trong multi-class setting.</p><h3 id="_3-15-3-focal-loss-cho-segmentation" tabindex="-1">3.15.3. Focal Loss cho Segmentation <a class="header-anchor" href="#_3-15-3-focal-loss-cho-segmentation" aria-label="Permalink to &quot;3.15.3. Focal Loss cho Segmentation&quot;">​</a></h3><p>Áp dụng Focal Loss cho từng pixel để down-weight easy pixels và focus vào hard pixels:</p><p>FL = -(1/N) × Σᵢ Σⱼ αc × (1 - pᵢⱼc)^γ × log(pᵢⱼc)</p><p>trong đó c là ground truth class của pixel (i,j). Focal Loss đặc biệt hữu ích khi có nhiều easy background pixels.</p><h3 id="_3-15-4-combined-loss" tabindex="-1">3.15.4. Combined Loss <a class="header-anchor" href="#_3-15-4-combined-loss" aria-label="Permalink to &quot;3.15.4. Combined Loss&quot;">​</a></h3><p>Nhiều paper sử dụng combination của multiple losses:</p><p>L = λ₁ × LCE + λ₂ × LDice + λ₃ × LFocal</p><p>với các weights λ được tune như hyperparameters. Combination thường cho kết quả tốt hơn single loss do mỗi loss có điểm mạnh riêng.</p><h3 id="_3-15-5-boundary-loss" tabindex="-1">3.15.5. Boundary Loss <a class="header-anchor" href="#_3-15-5-boundary-loss" aria-label="Permalink to &quot;3.15.5. Boundary Loss&quot;">​</a></h3><p>Để cải thiện segmentation của object boundaries, boundary-aware losses được sử dụng:</p><p><strong>Binary Cross-Entropy on Boundaries:</strong> Tăng weight cho pixels gần boundary của ground truth.</p><p><strong>Active Contour Loss:</strong> Dựa trên active contour model, encourage smooth boundaries.</p><h2 id="_3-16-metrics-đanh-gia-segmentation" tabindex="-1">3.16. Metrics Đánh giá Segmentation <a class="header-anchor" href="#_3-16-metrics-đanh-gia-segmentation" aria-label="Permalink to &quot;3.16. Metrics Đánh giá Segmentation&quot;">​</a></h2><h3 id="_3-16-1-pixel-accuracy" tabindex="-1">3.16.1. Pixel Accuracy <a class="header-anchor" href="#_3-16-1-pixel-accuracy" aria-label="Permalink to &quot;3.16.1. Pixel Accuracy&quot;">​</a></h3><p>Pixel Accuracy là tỷ lệ pixels được phân loại đúng:</p><p>Pixel Accuracy = Σᵢ nᵢᵢ / Σᵢ Σⱼ nᵢⱼ</p><p>trong đó nᵢⱼ là số pixels thuộc lớp i được dự đoán là lớp j. Đây là metric đơn giản nhưng bị dominate bởi các lớp lớn (như background).</p><h3 id="_3-16-2-intersection-over-union-iou-va-mean-iou" tabindex="-1">3.16.2. Intersection over Union (IoU) và Mean IoU <a class="header-anchor" href="#_3-16-2-intersection-over-union-iou-va-mean-iou" aria-label="Permalink to &quot;3.16.2. Intersection over Union (IoU) và Mean IoU&quot;">​</a></h3><p>IoU cho mỗi lớp k:</p><p>IoU_k = nₖₖ / (Σⱼ nₖⱼ + Σᵢ nᵢₖ - nₖₖ)</p><p>= TP / (TP + FP + FN)</p><p>Mean IoU (mIoU) là trung bình IoU trên tất cả các lớp:</p><p>mIoU = (1/K) × Σₖ IoU_k</p><p>mIoU là metric tiêu chuẩn nhất cho semantic segmentation, coi tất cả các lớp có tầm quan trọng như nhau bất kể kích thước.</p><h3 id="_3-16-3-dice-coefficient-f1-score" tabindex="-1">3.16.3. Dice Coefficient (F1-Score) <a class="header-anchor" href="#_3-16-3-dice-coefficient-f1-score" aria-label="Permalink to &quot;3.16.3. Dice Coefficient (F1-Score)&quot;">​</a></h3><p>Dice coefficient tương đương F1-Score cho binary segmentation:</p><p>Dice = 2TP / (2TP + FP + FN) = 2 × |P ∩ G| / (|P| + |G|)</p><p>Dice và IoU có mối quan hệ: Dice = 2×IoU / (1 + IoU), nên ranking bởi Dice và IoU thường tương đồng.</p><h3 id="_3-16-4-boundary-metrics" tabindex="-1">3.16.4. Boundary Metrics <a class="header-anchor" href="#_3-16-4-boundary-metrics" aria-label="Permalink to &quot;3.16.4. Boundary Metrics&quot;">​</a></h3><p>Để đánh giá chất lượng segmentation tại boundaries:</p><p><strong>Boundary F1 (BF1):</strong> Precision và Recall được tính chỉ cho pixels gần boundary (trong một khoảng cách threshold).</p><p><strong>Hausdorff Distance:</strong> Đo khoảng cách lớn nhất từ điểm trên boundary prediction đến boundary ground truth.</p>',88)])])}const d=t(a,[["render",c]]);export{u as __pageData,d as default};
