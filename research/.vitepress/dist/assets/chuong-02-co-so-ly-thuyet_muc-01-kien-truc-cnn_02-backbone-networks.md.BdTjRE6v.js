import{_ as t,c as i,o as e,a2 as a}from"./chunks/framework.nRfFlDZQ.js";const u=JSON.parse('{"title":"Chương 2: Backbone Networks: VGG, ResNet, EfficientNet và Swin Transformer","description":"","frontmatter":{},"headers":[],"relativePath":"chuong-02-co-so-ly-thuyet/muc-01-kien-truc-cnn/02-backbone-networks.md","filePath":"chuong-02-co-so-ly-thuyet/muc-01-kien-truc-cnn/02-backbone-networks.md","lastUpdated":1766302189000}'),c={name:"chuong-02-co-so-ly-thuyet/muc-01-kien-truc-cnn/02-backbone-networks.md"};function h(o,n,r,s,g,l){return e(),i("div",null,[...n[0]||(n[0]=[a('<h1 id="chuong-2-backbone-networks-vgg-resnet-efficientnet-va-swin-transformer" tabindex="-1">Chương 2: Backbone Networks: VGG, ResNet, EfficientNet và Swin Transformer <a class="header-anchor" href="#chuong-2-backbone-networks-vgg-resnet-efficientnet-va-swin-transformer" aria-label="Permalink to &quot;Chương 2: Backbone Networks: VGG, ResNet, EfficientNet và Swin Transformer&quot;">​</a></h1><h2 id="_2-10-khai-niem-backbone-network" tabindex="-1">2.10. Khái niệm Backbone Network <a class="header-anchor" href="#_2-10-khai-niem-backbone-network" aria-label="Permalink to &quot;2.10. Khái niệm Backbone Network&quot;">​</a></h2><p>Trong các bài toán thị giác máy tính hiện đại như phát hiện đối tượng (object detection), phân đoạn ngữ nghĩa (semantic segmentation), và instance segmentation, thuật ngữ &quot;backbone network&quot; được sử dụng để chỉ phần mạng nơ-ron chịu trách nhiệm trích xuất đặc trưng từ ảnh đầu vào. Backbone đóng vai trò như bộ encoder, biến đổi ảnh thô thành các feature map đa tầng chứa thông tin semantic phong phú, làm nền tảng cho các task-specific head thực hiện nhiệm vụ cuối cùng như phân loại, regression bounding box, hay tạo segmentation mask.</p><p>Việc lựa chọn backbone có ảnh hưởng quyết định đến hiệu suất của toàn bộ hệ thống. Một backbone mạnh có khả năng trích xuất các đặc trưng discriminative và robust, giúp các downstream task đạt độ chính xác cao hơn. Đồng thời, kích thước và độ phức tạp của backbone cũng ảnh hưởng trực tiếp đến tốc độ inference và yêu cầu tài nguyên phần cứng. Do đó, việc cân bằng giữa accuracy và efficiency là một trong những cân nhắc quan trọng khi thiết kế hệ thống.</p><p>Trong lĩnh vực viễn thám, các backbone được pre-train trên ImageNet thường được fine-tune cho các bài toán cụ thể. Gần đây, các thư viện như TorchGeo còn cung cấp backbone được pre-train trực tiếp trên ảnh vệ tinh từ các cảm biến như Sentinel-1, Sentinel-2, và Landsat, mang lại hiệu suất tốt hơn cho các bài toán viễn thám nhờ domain-specific knowledge.</p><h2 id="_2-11-vggnet-kien-truc-sau-voi-convolution-3×3" tabindex="-1">2.11. VGGNet: Kiến trúc Sâu với Convolution 3×3 <a class="header-anchor" href="#_2-11-vggnet-kien-truc-sau-voi-convolution-3×3" aria-label="Permalink to &quot;2.11. VGGNet: Kiến trúc Sâu với Convolution 3×3&quot;">​</a></h2><h3 id="_2-11-1-nguon-goc-va-đong-gop" tabindex="-1">2.11.1. Nguồn gốc và Đóng góp <a class="header-anchor" href="#_2-11-1-nguon-goc-va-đong-gop" aria-label="Permalink to &quot;2.11.1. Nguồn gốc và Đóng góp&quot;">​</a></h3><p>VGGNet được phát triển bởi nhóm Visual Geometry Group tại Đại học Oxford và đạt vị trí á quân trong cuộc thi ILSVRC 2014 (sau GoogLeNet). Mặc dù không giành chiến thắng, VGGNet có ảnh hưởng sâu rộng đến thiết kế kiến trúc CNN nhờ tính đơn giản và hiệu quả của nó. Đóng góp chính của VGGNet là chứng minh rằng độ sâu của mạng là yếu tố quan trọng quyết định hiệu suất, và việc sử dụng các bộ lọc nhỏ 3×3 xếp chồng có thể đạt được receptive field tương đương các bộ lọc lớn hơn với ít tham số hơn.</p><h3 id="_2-11-2-thiet-ke-kien-truc" tabindex="-1">2.11.2. Thiết kế Kiến trúc <a class="header-anchor" href="#_2-11-2-thiet-ke-kien-truc" aria-label="Permalink to &quot;2.11.2. Thiết kế Kiến trúc&quot;">​</a></h3><p>VGGNet tuân theo một nguyên tắc thiết kế nhất quán: tất cả các lớp convolution đều sử dụng bộ lọc 3×3 với stride 1 và padding &quot;same&quot;, và tất cả các lớp pooling đều là max pooling 2×2 với stride 2. Mạng được chia thành 5 khối convolution, mỗi khối kết thúc bằng một lớp max pooling. Số bộ lọc bắt đầu từ 64 ở khối đầu tiên và tăng gấp đôi sau mỗi lần pooling: 64 → 128 → 256 → 512 → 512.</p><p>Hai biến thể phổ biến nhất là VGG-16 và VGG-19, với 16 và 19 lớp có trọng số tương ứng. VGG-16 gồm 13 lớp convolution và 3 lớp fully connected, trong khi VGG-19 có thêm 3 lớp convolution. Cả hai đều kết thúc bằng 3 lớp fully connected với 4096, 4096, và 1000 nơ-ron (tương ứng 1000 lớp của ImageNet).</p><h3 id="_2-11-3-y-nghia-cua-convolution-3×3" tabindex="-1">2.11.3. Ý nghĩa của Convolution 3×3 <a class="header-anchor" href="#_2-11-3-y-nghia-cua-convolution-3×3" aria-label="Permalink to &quot;2.11.3. Ý nghĩa của Convolution 3×3&quot;">​</a></h3><p>Một insight quan trọng từ VGGNet là hai lớp convolution 3×3 xếp chồng có receptive field 5×5, và ba lớp 3×3 có receptive field 7×7. Tuy nhiên, ba lớp 3×3 chỉ cần 3 × (3² × C²) = 27C² tham số, trong khi một lớp 7×7 cần 7² × C² = 49C² tham số, giảm 45%. Đồng thời, việc xen kẽ các hàm kích hoạt ReLU giữa các lớp 3×3 tăng khả năng biểu diễn phi tuyến của mạng.</p><h3 id="_2-11-4-han-che" tabindex="-1">2.11.4. Hạn chế <a class="header-anchor" href="#_2-11-4-han-che" aria-label="Permalink to &quot;2.11.4. Hạn chế&quot;">​</a></h3><p>Mặc dù có kiến trúc đơn giản và hiệu quả, VGGNet có một số hạn chế đáng kể. Thứ nhất, ba lớp fully connected cuối cùng chứa khoảng 120 triệu tham số (chiếm 90% tổng số tham số của mạng), làm tăng nguy cơ overfitting và yêu cầu bộ nhớ lớn. Thứ hai, mạng có xu hướng bão hòa về độ chính xác khi tăng độ sâu, do vấn đề vanishing gradient khiến việc huấn luyện các mạng rất sâu trở nên khó khăn. Thứ ba, tốc độ inference chậm do số lượng tham số và phép tính lớn.</p><h2 id="_2-12-resnet-mang-residual-va-skip-connection" tabindex="-1">2.12. ResNet: Mạng Residual và Skip Connection <a class="header-anchor" href="#_2-12-resnet-mang-residual-va-skip-connection" aria-label="Permalink to &quot;2.12. ResNet: Mạng Residual và Skip Connection&quot;">​</a></h2><h3 id="_2-12-1-van-đe-degradation" tabindex="-1">2.12.1. Vấn đề Degradation <a class="header-anchor" href="#_2-12-1-van-đe-degradation" aria-label="Permalink to &quot;2.12.1. Vấn đề Degradation&quot;">​</a></h3><p>Trước ResNet, một nghịch lý được quan sát: khi tăng độ sâu của mạng beyond một ngưỡng nhất định, accuracy không tăng mà còn giảm, ngay cả trên training set. Điều này không phải do overfitting (vì training error cũng tăng) mà do vấn đề optimization - các mạng rất sâu khó được huấn luyện để đạt được minimum tốt. Kaiming He và cộng sự gọi hiện tượng này là &quot;degradation problem&quot;.</p><h3 id="_2-12-2-residual-learning" tabindex="-1">2.12.2. Residual Learning <a class="header-anchor" href="#_2-12-2-residual-learning" aria-label="Permalink to &quot;2.12.2. Residual Learning&quot;">​</a></h3><p>Ý tưởng cốt lõi của ResNet là thay vì học ánh xạ trực tiếp H(x) từ input x sang output, mạng học residual function F(x) = H(x) - x. Output thực tế được tính là H(x) = F(x) + x, với x được truyền trực tiếp qua một kết nối tắt (skip connection hay shortcut). Nếu identity mapping là tối ưu, mạng chỉ cần học F(x) = 0, điều này dễ dàng hơn nhiều so với việc học H(x) = x từ đầu.</p><p>Skip connection cho phép gradient chảy trực tiếp qua nhiều lớp trong quá trình backpropagation, giải quyết vấn đề vanishing gradient và cho phép huấn luyện thành công các mạng với hàng trăm hoặc thậm chí hàng nghìn lớp. Điều này mở ra khả năng xây dựng các mạng sâu hơn đáng kể so với trước đây.</p><h3 id="_2-12-3-kien-truc-resnet" tabindex="-1">2.12.3. Kiến trúc ResNet <a class="header-anchor" href="#_2-12-3-kien-truc-resnet" aria-label="Permalink to &quot;2.12.3. Kiến trúc ResNet&quot;">​</a></h3><p>ResNet sử dụng các residual block làm đơn vị xây dựng cơ bản. Có hai loại block chính:</p><p><strong>Basic Block:</strong> Sử dụng trong ResNet-18 và ResNet-34, gồm hai lớp convolution 3×3 với skip connection. Cấu trúc: Conv3×3 → BN → ReLU → Conv3×3 → BN → (+x) → ReLU.</p><p><strong>Bottleneck Block:</strong> Sử dụng trong ResNet-50, ResNet-101, và ResNet-152, gồm ba lớp convolution theo cấu trúc 1×1 → 3×3 → 1×1. Lớp 1×1 đầu tiên giảm số kênh (bottleneck), lớp 3×3 thực hiện convolution chính, và lớp 1×1 cuối khôi phục số kênh. Thiết kế này giảm số lượng tham số và chi phí tính toán trong khi duy trì hoặc tăng khả năng biểu diễn.</p><p>Khi kích thước không gian thay đổi (do strided convolution), skip connection sử dụng convolution 1×1 với stride tương ứng để match dimension. Tương tự, khi số kênh thay đổi, convolution 1×1 được sử dụng để projection.</p><h3 id="_2-12-4-cac-bien-the-resnet" tabindex="-1">2.12.4. Các Biến thể ResNet <a class="header-anchor" href="#_2-12-4-cac-bien-the-resnet" aria-label="Permalink to &quot;2.12.4. Các Biến thể ResNet&quot;">​</a></h3><p>Từ thiết kế gốc, nhiều biến thể và cải tiến của ResNet đã được phát triển:</p><p><strong>ResNet-V2:</strong> Thay đổi thứ tự các thành phần trong residual block thành BN → ReLU → Conv (pre-activation), cải thiện khả năng huấn luyện và kết quả cuối cùng.</p><p><strong>ResNeXt:</strong> Giới thiệu &quot;cardinality&quot; như một chiều mới bên cạnh depth và width. Mỗi residual block được chia thành nhiều nhánh song song với cấu trúc giống nhau, sau đó aggregate. ResNeXt-101 (32×4d) đạt kết quả tốt hơn ResNet-101 với số tham số tương đương.</p><p><strong>SE-ResNet:</strong> Tích hợp Squeeze-and-Excitation (SE) module, học cách recalibrate channel-wise feature responses bằng cách mô hình hóa interdependencies giữa các kênh.</p><p><strong>ResNeSt:</strong> Kết hợp ý tưởng từ ResNeXt và SE-Net với split-attention mechanism, đạt state-of-the-art trên nhiều benchmark.</p><h2 id="_2-13-efficientnet-compound-scaling" tabindex="-1">2.13. EfficientNet: Compound Scaling <a class="header-anchor" href="#_2-13-efficientnet-compound-scaling" aria-label="Permalink to &quot;2.13. EfficientNet: Compound Scaling&quot;">​</a></h2><h3 id="_2-13-1-van-đe-scaling" tabindex="-1">2.13.1. Vấn đề Scaling <a class="header-anchor" href="#_2-13-1-van-đe-scaling" aria-label="Permalink to &quot;2.13.1. Vấn đề Scaling&quot;">​</a></h3><p>Trước EfficientNet, việc tăng hiệu suất CNN thường được thực hiện bằng cách tăng một trong ba chiều: độ sâu (thêm lớp), độ rộng (thêm kênh), hoặc độ phân giải (tăng kích thước ảnh đầu vào). Tuy nhiên, việc scale theo từng chiều riêng lẻ thường đạt điểm bão hòa nhanh chóng. Ví dụ, ResNet-1000 không tốt hơn đáng kể so với ResNet-152 mặc dù sâu hơn nhiều.</p><h3 id="_2-13-2-compound-scaling" tabindex="-1">2.13.2. Compound Scaling <a class="header-anchor" href="#_2-13-2-compound-scaling" aria-label="Permalink to &quot;2.13.2. Compound Scaling&quot;">​</a></h3><p>Mingxing Tan và Quoc V. Le đề xuất phương pháp compound scaling, cân bằng đồng thời cả ba chiều depth, width, và resolution theo một tỷ lệ cố định. Họ sử dụng neural architecture search (NAS) để tìm baseline network (EfficientNet-B0) tối ưu, sau đó scale up theo công thức:</p><ul><li>Depth: d = α^φ</li><li>Width: w = β^φ</li><li>Resolution: r = γ^φ</li></ul><p>với ràng buộc α × β² × γ² ≈ 2 để đảm bảo FLOPS tăng xấp xỉ 2^φ. Các hệ số α, β, γ được tìm bằng grid search trên baseline. Compound coefficient φ được điều chỉnh để tạo ra các biến thể từ B0 đến B7 với quy mô tăng dần.</p><h3 id="_2-13-3-kien-truc-mbconv" tabindex="-1">2.13.3. Kiến trúc MBConv <a class="header-anchor" href="#_2-13-3-kien-truc-mbconv" aria-label="Permalink to &quot;2.13.3. Kiến trúc MBConv&quot;">​</a></h3><p>EfficientNet sử dụng Mobile Inverted Bottleneck Convolution (MBConv) làm building block chính, kế thừa từ MobileNetV2. MBConv có cấu trúc ngược với bottleneck truyền thống: mở rộng số kênh ở đầu, thực hiện depthwise separable convolution ở giữa, rồi thu hẹp ở cuối. Cấu trúc: Conv1×1 (expand) → Depthwise Conv3×3/5×5 → SE → Conv1×1 (project).</p><p>Depthwise separable convolution chia convolution thông thường thành hai bước: depthwise convolution (một filter riêng cho mỗi kênh) và pointwise convolution (1×1 để mix kênh). Điều này giảm đáng kể số lượng tham số và FLOPs. Squeeze-and-Excitation (SE) module được tích hợp để tăng khả năng biểu diễn.</p><h3 id="_2-13-4-hieu-qua-cua-efficientnet" tabindex="-1">2.13.4. Hiệu quả của EfficientNet <a class="header-anchor" href="#_2-13-4-hieu-qua-cua-efficientnet" aria-label="Permalink to &quot;2.13.4. Hiệu quả của EfficientNet&quot;">​</a></h3><p>EfficientNet đạt được sự cân bằng ấn tượng giữa accuracy và efficiency. EfficientNet-B0 đạt 77.3% top-1 accuracy trên ImageNet với chỉ 5.3M tham số, so với ResNet-50 đạt 76% với 26M tham số. EfficientNet-B7 đạt 84.3% accuracy - state-of-the-art vào thời điểm công bố - với 66M tham số, ít hơn nhiều so với các phương pháp khác đạt accuracy tương đương.</p><p>Đối với các bài toán viễn thám, EfficientNet là lựa chọn hấp dẫn do khả năng xử lý ảnh có độ phân giải cao (compound scaling bao gồm resolution) và efficiency cho phép triển khai trên các thiết bị edge hoặc xử lý khối lượng ảnh lớn.</p><h2 id="_2-14-vision-transformer-vit-transformer-cho-computer-vision" tabindex="-1">2.14. Vision Transformer (ViT): Transformer cho Computer Vision <a class="header-anchor" href="#_2-14-vision-transformer-vit-transformer-cho-computer-vision" aria-label="Permalink to &quot;2.14. Vision Transformer (ViT): Transformer cho Computer Vision&quot;">​</a></h2><h3 id="_2-14-1-tu-nlp-sang-vision" tabindex="-1">2.14.1. Từ NLP sang Vision <a class="header-anchor" href="#_2-14-1-tu-nlp-sang-vision" aria-label="Permalink to &quot;2.14.1. Từ NLP sang Vision&quot;">​</a></h3><p>Transformer architecture, ban đầu được phát triển cho xử lý ngôn ngữ tự nhiên (NLP), đạt thành công vượt bậc với các mô hình như BERT và GPT. Vision Transformer (ViT), được đề xuất bởi Dosovitskiy và cộng sự năm 2020, là nỗ lực đầu tiên áp dụng kiến trúc Transformer thuần túy (không dùng convolution) cho image classification và đạt kết quả cạnh tranh với các CNN state-of-the-art.</p><h3 id="_2-14-2-kien-truc-vit" tabindex="-1">2.14.2. Kiến trúc ViT <a class="header-anchor" href="#_2-14-2-kien-truc-vit" aria-label="Permalink to &quot;2.14.2. Kiến trúc ViT&quot;">​</a></h3><p><strong>Patch Embedding:</strong> Thay vì xử lý từng pixel, ViT chia ảnh đầu vào H×W×C thành các patch không chồng lấp, mỗi patch có kích thước P×P (thường P=16 hoặc 32). Mỗi patch được flatten thành vector và ánh xạ qua linear projection thành embedding vector có chiều D. Với ảnh 224×224 và patch size 16, ta có (224/16)² = 196 patches.</p><p><strong>Position Embedding:</strong> Không giống CNN có inductive bias về cấu trúc không gian, Transformer cần được cung cấp thông tin vị trí tường minh. ViT thêm learnable position embedding vào mỗi patch embedding để mã hóa vị trí 2D của patch trong ảnh gốc.</p><p><strong>Transformer Encoder:</strong> Chuỗi patch embeddings (cộng với một [CLS] token đặc biệt ở đầu) được đưa qua L layers của Transformer encoder. Mỗi layer gồm: Multi-Head Self-Attention (MSA) cho phép mỗi patch &quot;attend&quot; tới tất cả các patches khác, và Feed-Forward Network (FFN) áp dụng MLP riêng biệt cho từng patch. Layer Normalization và residual connections được sử dụng.</p><p><strong>Classification Head:</strong> Output của [CLS] token từ layer cuối cùng được đưa qua một MLP head để tạo ra class probabilities.</p><h3 id="_2-14-3-uu-va-nhuoc-điem-cua-vit" tabindex="-1">2.14.3. Ưu và Nhược điểm của ViT <a class="header-anchor" href="#_2-14-3-uu-va-nhuoc-điem-cua-vit" aria-label="Permalink to &quot;2.14.3. Ưu và Nhược điểm của ViT&quot;">​</a></h3><p><strong>Ưu điểm:</strong></p><ul><li><strong>Long-range dependencies:</strong> Self-attention cho phép mô hình hóa mối quan hệ giữa các vùng xa nhau trong ảnh ngay từ layer đầu tiên, khác với CNN cần xếp chồng nhiều layers để mở rộng receptive field.</li><li><strong>Scaling tốt với dữ liệu:</strong> Khi được pre-train trên datasets cực lớn (hàng trăm triệu ảnh như JFT-300M), ViT vượt trội hơn CNN. Transformer architecture có khả năng scaling tốt hơn với cả model size và data size.</li><li><strong>Transfer learning hiệu quả:</strong> Pre-trained ViT weights chuyển giao tốt sang các downstream tasks.</li></ul><p><strong>Nhược điểm:</strong></p><ul><li><strong>Cần dữ liệu lớn:</strong> Không có inductive bias như CNN (locality, translation invariance), ViT cần lượng lớn dữ liệu training. Khi train từ đầu trên ImageNet-1K, ViT không tốt bằng ResNet. Chỉ khi pre-train trên datasets lớn hơn nhiều, ViT mới phát huy ưu thế.</li><li><strong>Độ phức tạp O(n²):</strong> Self-attention có độ phức tạp bậc hai theo số patches, không hiệu quả cho ảnh độ phân giải cao hoặc dense prediction tasks.</li><li><strong>Thiếu multi-scale features:</strong> ViT output single-scale feature map, không phù hợp cho object detection và segmentation yêu cầu multi-scale representations.</li></ul><h2 id="_2-15-swin-transformer-hierarchical-vision-transformer" tabindex="-1">2.15. Swin Transformer: Hierarchical Vision Transformer <a class="header-anchor" href="#_2-15-swin-transformer-hierarchical-vision-transformer" aria-label="Permalink to &quot;2.15. Swin Transformer: Hierarchical Vision Transformer&quot;">​</a></h2><h3 id="_2-15-1-khac-phuc-han-che-cua-vit" tabindex="-1">2.15.1. Khắc phục Hạn chế của ViT <a class="header-anchor" href="#_2-15-1-khac-phuc-han-che-cua-vit" aria-label="Permalink to &quot;2.15.1. Khắc phục Hạn chế của ViT&quot;">​</a></h3><p>Swin Transformer (Shifted Window Transformer), được đề xuất bởi Liu và cộng sự năm 2021, giải quyết các hạn chế của ViT để trở thành backbone general-purpose cho nhiều vision tasks. Hai cải tiến chính là hierarchical architecture và window-based self-attention.</p><h3 id="_2-15-2-hierarchical-feature-maps" tabindex="-1">2.15.2. Hierarchical Feature Maps <a class="header-anchor" href="#_2-15-2-hierarchical-feature-maps" aria-label="Permalink to &quot;2.15.2. Hierarchical Feature Maps&quot;">​</a></h3><p>Giống như CNN, Swin Transformer tạo ra feature map phân cấp với độ phân giải giảm dần. Ảnh đầu vào được chia thành các patch không chồng lấp (thường 4×4 pixel mỗi patch), sau đó qua các stage với patch merging giảm resolution 2× và tăng số kênh 2× tại mỗi stage. Kết quả là feature maps với tỷ lệ 1/4, 1/8, 1/16, 1/32 so với ảnh gốc - tương tự như output của các stage trong ResNet.</p><h3 id="_2-15-3-window-based-self-attention" tabindex="-1">2.15.3. Window-based Self-Attention <a class="header-anchor" href="#_2-15-3-window-based-self-attention" aria-label="Permalink to &quot;2.15.3. Window-based Self-Attention&quot;">​</a></h3><p>Thay vì tính global self-attention trên toàn bộ ảnh (như ViT), Swin Transformer chia feature map thành các window không chồng lấp (thường 7×7 patch mỗi window) và tính self-attention trong từng window. Điều này giảm độ phức tạp từ O(n²) xuống O(n × M²) với M là kích thước window, cho phép xử lý ảnh độ phân giải cao.</p><p>Để tạo kết nối giữa các window, Swin Transformer sử dụng shifted window partitioning: trong các lớp xen kẽ, các window được dịch chuyển (window_size/2, window_size/2) pixel. Điều này cho phép thông tin được trao đổi giữa các window liền kề qua các lớp liên tiếp, mở rộng receptive field mà không tăng chi phí tính toán.</p><h3 id="_2-15-4-ung-dung-trong-vien-tham" tabindex="-1">2.15.4. Ứng dụng trong Viễn thám <a class="header-anchor" href="#_2-15-4-ung-dung-trong-vien-tham" aria-label="Permalink to &quot;2.15.4. Ứng dụng trong Viễn thám&quot;">​</a></h3><p>Swin Transformer và các biến thể như Swin-V2 đã chứng minh hiệu quả vượt trội trong nhiều bài toán viễn thám. Khả năng mô hình hóa long-range dependencies thông qua self-attention đặc biệt hữu ích cho các bài toán yêu cầu ngữ cảnh rộng như phân loại scene, change detection, và segmentation các đối tượng lớn. TorchGeo cung cấp Swin Transformer pre-trained trên ảnh NAIP, cho phép transfer learning hiệu quả cho các bài toán viễn thám.</p><h2 id="_2-16-self-supervised-pre-training" tabindex="-1">2.16. Self-Supervised Pre-training <a class="header-anchor" href="#_2-16-self-supervised-pre-training" aria-label="Permalink to &quot;2.16. Self-Supervised Pre-training&quot;">​</a></h2><h3 id="_2-16-1-contrastive-learning-moco" tabindex="-1">2.16.1. Contrastive Learning: MoCo <a class="header-anchor" href="#_2-16-1-contrastive-learning-moco" aria-label="Permalink to &quot;2.16.1. Contrastive Learning: MoCo&quot;">​</a></h3><p>Momentum Contrast (MoCo) là phương pháp self-supervised learning sử dụng contrastive loss. Ý tưởng cốt lõi là học representations bằng cách maximize agreement giữa các augmented views khác nhau của cùng một ảnh (positive pairs) trong khi minimize agreement giữa các ảnh khác nhau (negative pairs). MoCo v2 và v3 đã được áp dụng thành công cho pre-training backbone trên ảnh vệ tinh.</p><h3 id="_2-16-2-masked-image-modeling-mae" tabindex="-1">2.16.2. Masked Image Modeling: MAE <a class="header-anchor" href="#_2-16-2-masked-image-modeling-mae" aria-label="Permalink to &quot;2.16.2. Masked Image Modeling: MAE&quot;">​</a></h3><p>Masked Autoencoder (MAE) áp dụng ý tưởng masked language modeling (từ BERT) cho vision. Một tỷ lệ lớn patches (75%) được mask ngẫu nhiên, và mô hình học cách reconstruct các patches bị mask từ các patches visible. MAE đặc biệt hiệu quả cho ViT và đã được TorchGeo sử dụng để pre-train trên Sentinel-2 và Landsat.</p><p><strong>SatMAE</strong> là biến thể của MAE được thiết kế đặc biệt cho ảnh vệ tinh đa phổ, xử lý hiệu quả việc pre-train trên 13 kênh của Sentinel-2.</p><h3 id="_2-16-3-ssl4eo-self-supervised-learning-for-earth-observation" tabindex="-1">2.16.3. SSL4EO: Self-Supervised Learning for Earth Observation <a class="header-anchor" href="#_2-16-3-ssl4eo-self-supervised-learning-for-earth-observation" aria-label="Permalink to &quot;2.16.3. SSL4EO: Self-Supervised Learning for Earth Observation&quot;">​</a></h3><p>SSL4EO là framework pre-training tổng hợp cho Earth Observation, kết hợp các kỹ thuật self-supervised learning hiện đại với đặc thù của ảnh vệ tinh. Framework cung cấp pre-trained weights cho nhiều backbones trên datasets lớn như Million-AID, giúp cải thiện đáng kể performance cho các downstream tasks viễn thám với limited labeled data.</p><h2 id="_2-17-so-sanh-backbone-cnn-vs-transformer" tabindex="-1">2.17. So sánh Backbone: CNN vs Transformer <a class="header-anchor" href="#_2-17-so-sanh-backbone-cnn-vs-transformer" aria-label="Permalink to &quot;2.17. So sánh Backbone: CNN vs Transformer&quot;">​</a></h2><h3 id="_2-17-1-bang-so-sanh-performance" tabindex="-1">2.17.1. Bảng So sánh Performance <a class="header-anchor" href="#_2-17-1-bang-so-sanh-performance" aria-label="Permalink to &quot;2.17.1. Bảng So sánh Performance&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Backbone</th><th>Params</th><th>Top-1 Acc</th><th>FLOPs</th><th>Đặc điểm chính</th></tr></thead><tbody><tr><td>VGG-16</td><td>138M</td><td>71.5%</td><td>15.5G</td><td>Đơn giản, dễ hiểu, nhiều params</td></tr><tr><td>ResNet-50</td><td>26M</td><td>76.1%</td><td>4.1G</td><td>Skip connection, robust, phổ biến</td></tr><tr><td>ResNet-101</td><td>45M</td><td>77.4%</td><td>7.9G</td><td>Deeper ResNet, accuracy cao hơn</td></tr><tr><td>EfficientNet-B0</td><td>5.3M</td><td>77.3%</td><td>0.4G</td><td>Nhỏ gọn, hiệu quả</td></tr><tr><td>EfficientNet-B4</td><td>19M</td><td>82.9%</td><td>4.2G</td><td>Cân bằng accuracy/efficiency</td></tr><tr><td>ViT-B/16</td><td>86M</td><td>77.9%*</td><td>17.6G</td><td>Transformer, cần dữ liệu lớn (*khi pre-train trên ImageNet-21K)</td></tr><tr><td>Swin-T</td><td>29M</td><td>81.3%</td><td>4.5G</td><td>Hierarchical Transformer, efficient</td></tr><tr><td>Swin-B</td><td>88M</td><td>83.5%</td><td>15.4G</td><td>Larger Swin, SOTA accuracy</td></tr></tbody></table><h3 id="_2-17-2-so-sanh-cnn-va-transformer-cho-vien-tham" tabindex="-1">2.17.2. So sánh CNN và Transformer cho Viễn thám <a class="header-anchor" href="#_2-17-2-so-sanh-cnn-va-transformer-cho-vien-tham" aria-label="Permalink to &quot;2.17.2. So sánh CNN và Transformer cho Viễn thám&quot;">​</a></h3><table tabindex="0"><thead><tr><th>Khía cạnh</th><th>CNN (ResNet, EfficientNet)</th><th>Transformer (ViT, Swin)</th></tr></thead><tbody><tr><td><strong>Inductive bias</strong></td><td>Locality, translation invariance</td><td>Minimal (học từ data)</td></tr><tr><td><strong>Yêu cầu dữ liệu</strong></td><td>Thấp - tốt với limited data</td><td>Cao - cần pre-training hoặc large dataset</td></tr><tr><td><strong>Receptive field</strong></td><td>Tăng dần qua layers</td><td>Global từ đầu (ViT) hoặc hierarchical (Swin)</td></tr><tr><td><strong>Multi-scale</strong></td><td>Tự nhiên với pooling/stride</td><td>Cần thiết kế đặc biệt (Swin)</td></tr><tr><td><strong>Long-range dependencies</strong></td><td>Khó (cần deep network)</td><td>Dễ (self-attention)</td></tr><tr><td><strong>Hiệu quả tính toán</strong></td><td>Cao, phù hợp edge devices</td><td>Trung bình đến thấp (ViT O(n²))</td></tr><tr><td><strong>Transfer learning</strong></td><td>ImageNet → RS: tốt</td><td>ImageNet → RS: tốt, SSL (MAE) → RS: rất tốt</td></tr><tr><td><strong>Ảnh SAR</strong></td><td>ResNet pre-trained on Sentinel-1</td><td>ViT/Swin với SatMAE</td></tr><tr><td><strong>Ảnh đa phổ</strong></td><td>Conv đầu mở rộng cho C kênh</td><td>Patch embedding tự nhiên hỗ trợ C kênh</td></tr></tbody></table><h3 id="_2-17-3-khuyen-nghi-cho-vien-tham" tabindex="-1">2.17.3. Khuyến nghị cho Viễn thám <a class="header-anchor" href="#_2-17-3-khuyen-nghi-cho-vien-tham" aria-label="Permalink to &quot;2.17.3. Khuyến nghị cho Viễn thám&quot;">​</a></h3><p><strong>Dùng CNN (ResNet, EfficientNet) khi:</strong></p><ul><li>Dữ liệu training hạn chế (&lt; 10K ảnh)</li><li>Cần tốc độ inference nhanh hoặc deploy trên edge devices</li><li>Task yêu cầu multi-scale features rõ ràng (object detection, segmentation)</li><li>Có pre-trained weights chất lượng trên domain tương tự (ResNet-50 on Sentinel-1)</li></ul><p><strong>Dùng Transformer (ViT, Swin) khi:</strong></p><ul><li>Có lượng lớn dữ liệu hoặc sử dụng self-supervised pre-training (MAE, MoCo)</li><li>Task cần model long-range spatial dependencies (scene classification, change detection trên vùng rộng)</li><li>Có tài nguyên tính toán đủ mạnh</li><li>Muốn sử dụng state-of-the-art pre-trained weights từ SatMAE hoặc SSL4EO</li></ul><p><strong>Khuyến nghị cụ thể theo bài toán:</strong></p><p><em>Phát hiện tàu biển (Ship Detection):</em></p><ul><li>Thời gian thực: EfficientNet-B0/B1 + YOLO head</li><li>Accuracy cao: Swin-T + Faster R-CNN hoặc ResNet-101 + Cascade R-CNN</li><li>Cân bằng: ResNet-50 + RetinaNet</li></ul><p><em>Phát hiện dầu loang (Oil Spill Segmentation):</em></p><ul><li>Binary segmentation: ResNet-50 + U-Net hoặc DeepLabV3+</li><li>Phân biệt oil/look-alike phức tạp: Swin-T + DeepLabV3+ (benefit từ long-range context)</li><li>Limited data: ResNet-50 pre-trained on Sentinel-1 + U-Net với extensive data augmentation</li></ul><p><em>Scene classification viễn thám:</em></p><ul><li>ViT hoặc Swin pre-trained với SSL (MAE, SSL4EO) thường cho kết quả tốt nhất</li><li>ResNet-50/101 vẫn competitive và nhanh hơn</li></ul><p><strong>TorchGeo Pre-trained Weights:</strong></p><ul><li>ResNet-50: Sentinel-1 SAR, Sentinel-2 multispectral</li><li>ViT: Sentinel-2 với SatMAE</li><li>Swin-T: NAIP aerial imagery</li><li>Prithvi: Foundation model từ IBM/NASA cho Sentinel-2</li></ul>',95)])])}const p=t(c,[["render",h]]);export{u as __pageData,p as default};
