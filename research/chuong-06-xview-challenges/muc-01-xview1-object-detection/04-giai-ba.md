# Ch∆∞∆°ng 6: xView1 H·∫°ng 3: Gi·∫£i Ph√°p University of South Florida - Pattern Recognition v√† Multi-Scale Analysis

## Th√¥ng Tin T·ªïng Quan

| Thu·ªôc T√≠nh | Chi Ti·∫øt |
|------------|----------|
| **H·∫°ng** | ü•â H·∫°ng 3 |
| **Nh√† Nghi√™n C·ª©u Ch√≠nh** | Prof. Sudeep Sarkar |
| **ƒê∆°n V·ªã** | University of South Florida, Tampa, FL |
| **Th·ª≠ Th√°ch** | DIUx xView 2018 Detection Challenge |
| **Chuy√™n M√¥n** | Pattern Recognition, Computer Vision |
| **Ti·ªÅn Th∆∞·ªüng** | $50,000 USD |
| **T·ªïng B√†i N·ªôp** | 2,300+ b√†i t·ª´ to√†n c·∫ßu |

---

## 1. T·ªïng Quan v√† B·ªëi C·∫£nh

### 1.1 University of South Florida Computer Vision Lab

University of South Florida (USF) s·ªü h·ªØu m·ªôt trong nh·ªØng ph√≤ng nghi√™n c·ª©u computer vision l√¢u ƒë·ªùi v√† uy t√≠n nh·∫•t t·∫°i Hoa K·ª≥. Ph√≤ng lab ƒë∆∞·ª£c d·∫´n d·∫Øt b·ªüi Prof. Sudeep Sarkar, IEEE Fellow v·ªõi h∆°n 25 nƒÉm kinh nghi·ªám trong lƒ©nh v·ª±c.

![Multi-scale Objects](../../assets/images/xview1/dataset/fig11-class-examples.png)
*H√¨nh 1: ƒêa d·∫°ng v·ªÅ lo·∫°i v√† k√≠ch th∆∞·ªõc ƒë·ªëi t∆∞·ª£ng ƒë√≤i h·ªèi pattern recognition ƒëa t·∫ßng*

**Lƒ©nh V·ª±c Nghi√™n C·ª©u Ch√≠nh:**

```mermaid
graph TB
    subgraph USF_CV["USF Computer Vision Lab"]
        PR[Pattern Recognition]
        GR[Gait Recognition]
        SA[Shape Analysis]
        VS[Video Surveillance]
    end

    subgraph Applications["·ª®ng D·ª•ng"]
        A1[Biometric Systems]
        A2[Security & Surveillance]
        A3[Medical Imaging]
        A4[Remote Sensing]
    end

    PR --> A1
    GR --> A2
    SA --> A3
    VS --> A4
```

**Th√†nh T·ª±u N·ªïi B·∫≠t:**
| Metric | Gi√° Tr·ªã |
|--------|---------|
| IEEE Fellow Status | ‚úì Confirmed |
| H-index | 65+ |
| Publications | 300+ papers |
| Patents | 15+ |
| PhD Students | 40+ graduated |

### 1.2 Prof. Sudeep Sarkar

Prof. Sarkar l√† m·ªôt trong nh·ªØng chuy√™n gia h√†ng ƒë·∫ßu th·∫ø gi·ªõi v·ªÅ pattern recognition v√† computer vision:

**Academic Background:**
- **PhD:** Computer Science, The Ohio State University (1993)
- **Position:** Distinguished University Professor, USF
- **Research Focus:** Pattern recognition, gait recognition, 3D shape analysis

**Key Contributions:**
1. **Human Gait Recognition:** Ph√°t tri·ªÉn h·ªá th·ªëng nh·∫≠n d·∫°ng ng∆∞·ªùi qua d√°ng ƒëi
2. **Perceptual Organization:** Algorithms cho visual perception
3. **Graph-based Methods:** ·ª®ng d·ª•ng graph theory trong computer vision
4. **Shape Analysis:** 3D shape matching v√† retrieval

```mermaid
timeline
    title Prof. Sarkar Career Milestones
    1993 : PhD Ohio State
    1998 : Associate Professor USF
    2005 : Full Professor
    2010 : IEEE Fellow
    2015 : Distinguished Professor
    2018 : xView Challenge Top 3
```

### 1.3 L√Ω Do Tham Gia xView Challenge

Cu·ªôc thi xView ph√π h·ª£p v·ªõi chuy√™n m√¥n nghi√™n c·ª©u c·ªßa nh√≥m USF:

**Synergies v·ªõi Research:**
- **Multi-scale pattern detection:** Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng ·ªü nhi·ªÅu t·ª∑ l·ªá
- **Contextual reasoning:** S·ª≠ d·ª•ng ng·ªØ c·∫£nh kh√¥ng gian
- **Statistical modeling:** Modeling ph√¢n b·ªë c√°c l·ªõp ƒë·ªëi t∆∞·ª£ng
- **Robust feature extraction:** Features ·ªïn ƒë·ªãnh v·ªõi variations

---

## 2. ƒê·ªïi M·ªõi K·ªπ Thu·∫≠t Ch√≠nh

### 2.1 Multi-Scale Hierarchical Pattern Recognition

ƒêi·ªÉm m·∫°nh c·ªßa nh√≥m USF l√† ph∆∞∆°ng ph√°p hierarchical pattern recognition, x·ª≠ l√Ω ƒë·ªëi t∆∞·ª£ng ·ªü nhi·ªÅu t·∫ßng tr·ª´u t∆∞·ª£ng:

```mermaid
flowchart TB
    subgraph Input["Input Processing"]
        I[Satellite Image] --> P1[Pyramid Level 1<br/>Original]
        I --> P2[Pyramid Level 2<br/>0.5x]
        I --> P3[Pyramid Level 3<br/>0.25x]
        I --> P4[Pyramid Level 4<br/>0.125x]
    end

    subgraph Features["Feature Extraction"]
        P1 --> F1[Local Features]
        P2 --> F2[Regional Features]
        P3 --> F3[Contextual Features]
        P4 --> F4[Global Features]
    end

    subgraph Fusion["Hierarchical Fusion"]
        F1 --> H1[Level 1 Detections]
        F2 --> H2[Level 2 Detections]
        F3 --> H3[Level 3 Detections]
        F4 --> H4[Level 4 Detections]

        H1 --> MF[Multi-Scale Fusion]
        H2 --> MF
        H3 --> MF
        H4 --> MF
    end

    MF --> O[Final Predictions]
```

**Hierarchical Pattern Recognition Framework:**
```python
class HierarchicalPatternRecognition(nn.Module):
    """
    Multi-scale hierarchical framework cho satellite imagery.
    D·ª±a tr√™n kinh nghi·ªám pattern recognition t·ª´ USF lab.
    """
    def __init__(self, backbone='resnet101', num_classes=60, scales=[1.0, 0.5, 0.25]):
        super().__init__()
        self.scales = scales
        self.num_classes = num_classes

        # Shared backbone
        self.backbone = self._build_backbone(backbone)

        # Scale-specific heads
        self.scale_heads = nn.ModuleList([
            self._build_detection_head(num_classes)
            for _ in scales
        ])

        # Multi-scale fusion
        self.fusion = MultiScaleFusion(len(scales), num_classes)

    def _build_backbone(self, name):
        if name == 'resnet101':
            backbone = models.resnet101(pretrained=True)
            return nn.Sequential(*list(backbone.children())[:-2])
        raise ValueError(f"Unknown backbone: {name}")

    def _build_detection_head(self, num_classes):
        return nn.Sequential(
            nn.Conv2d(2048, 1024, 3, padding=1),
            nn.BatchNorm2d(1024),
            nn.ReLU(inplace=True),
            nn.Conv2d(1024, 512, 3, padding=1),
            nn.BatchNorm2d(512),
            nn.ReLU(inplace=True),
            DetectionHead(512, num_classes)
        )

    def forward(self, x):
        scale_outputs = []

        for scale, head in zip(self.scales, self.scale_heads):
            # Resize input
            if scale != 1.0:
                h, w = int(x.shape[2] * scale), int(x.shape[3] * scale)
                scaled_x = F.interpolate(x, size=(h, w), mode='bilinear')
            else:
                scaled_x = x

            # Extract features
            features = self.backbone(scaled_x)

            # Detect at this scale
            detections = head(features)
            scale_outputs.append(detections)

        # Fuse multi-scale detections
        final_output = self.fusion(scale_outputs, x.shape[2:])
        return final_output
```

### 2.2 Contextual Reasoning Module

Nh√≥m USF √°p d·ª•ng ki·∫øn th·ª©c t·ª´ perceptual organization ƒë·ªÉ x√¢y d·ª±ng module reasoning theo ng·ªØ c·∫£nh:

```python
class ContextualReasoningModule(nn.Module):
    """
    Module suy lu·∫≠n ng·ªØ c·∫£nh d·ª±a tr√™n spatial relationships.
    H·ªçc c√°c patterns co-occurrence gi·ªØa c√°c lo·∫°i ƒë·ªëi t∆∞·ª£ng.
    """
    def __init__(self, num_classes=60, context_dim=256):
        super().__init__()
        self.num_classes = num_classes

        # Co-occurrence matrix (learned)
        self.co_occurrence = nn.Parameter(
            torch.zeros(num_classes, num_classes)
        )

        # Spatial context encoder
        self.spatial_encoder = nn.Sequential(
            nn.Conv2d(num_classes, context_dim, 3, padding=1),
            nn.ReLU(),
            nn.Conv2d(context_dim, context_dim, 3, padding=2, dilation=2),
            nn.ReLU(),
            nn.Conv2d(context_dim, context_dim, 3, padding=4, dilation=4),
            nn.ReLU(),
        )

        # Context-to-prediction
        self.context_head = nn.Conv2d(context_dim, num_classes, 1)

    def forward(self, class_scores, features):
        # Get class probability maps
        probs = F.softmax(class_scores, dim=1)  # [B, C, H, W]

        # Encode spatial context
        context = self.spatial_encoder(probs)

        # Generate context-aware adjustments
        adjustments = self.context_head(context)

        # Apply co-occurrence priors
        co_occur_weight = torch.sigmoid(self.co_occurrence)
        b, c, h, w = probs.shape
        probs_flat = probs.view(b, c, -1)  # [B, C, H*W]

        # Weighted sum based on co-occurrence
        context_probs = torch.bmm(
            co_occur_weight.unsqueeze(0).expand(b, -1, -1),
            probs_flat
        ).view(b, c, h, w)

        # Combine original + context
        enhanced_scores = class_scores + 0.1 * adjustments + 0.05 * context_probs

        return enhanced_scores
```

### 2.3 Graph-Based Object Relationships

√Åp d·ª•ng kinh nghi·ªám graph-based methods:

```mermaid
graph LR
    subgraph Objects["Detected Objects"]
        O1[Building]
        O2[Parking Lot]
        O3[Cars]
        O4[Road]
    end

    subgraph Relations["Spatial Relations"]
        O1 -->|adjacent| O2
        O2 -->|contains| O3
        O4 -->|connects| O1
        O4 -->|connects| O2
    end

    subgraph Inference["Graph Reasoning"]
        R1[Building-Parking pattern<br/>‚Üí Commercial area]
        R2[Cars in Parking<br/>‚Üí Confirms building use]
        R3[Road connectivity<br/>‚Üí Infrastructure validation]
    end

    Relations --> Inference
```

```python
class GraphReasoningLayer(nn.Module):
    """
    Graph neural network cho object relationship reasoning.
    """
    def __init__(self, feature_dim=256, num_relations=5):
        super().__init__()

        # Edge type embeddings
        self.relation_embedding = nn.Embedding(num_relations, feature_dim)

        # Message passing
        self.message_fn = nn.Sequential(
            nn.Linear(feature_dim * 3, feature_dim),  # node_i, node_j, edge
            nn.ReLU(),
            nn.Linear(feature_dim, feature_dim)
        )

        # Node update
        self.update_fn = nn.GRUCell(feature_dim, feature_dim)

    def forward(self, node_features, edge_index, edge_type):
        """
        Args:
            node_features: [N, D] detected object features
            edge_index: [2, E] source and target indices
            edge_type: [E] relation type for each edge
        """
        num_nodes = node_features.size(0)

        # Get relation embeddings
        rel_embed = self.relation_embedding(edge_type)  # [E, D]

        # Compute messages
        src, dst = edge_index
        src_features = node_features[src]  # [E, D]
        dst_features = node_features[dst]  # [E, D]

        messages = self.message_fn(
            torch.cat([src_features, dst_features, rel_embed], dim=1)
        )  # [E, D]

        # Aggregate messages per node
        aggregated = torch.zeros(num_nodes, messages.size(1), device=messages.device)
        aggregated.scatter_add_(0, dst.unsqueeze(1).expand_as(messages), messages)

        # Update node features
        updated = self.update_fn(aggregated, node_features)

        return updated
```

### 2.4 Statistical Shape Models

T·ª´ kinh nghi·ªám shape analysis, nh√≥m √°p d·ª•ng statistical shape models:

```python
class StatisticalShapeModel:
    """
    Model th·ªëng k√™ cho object shapes trong satellite imagery.
    H·ªçc distribution c·ªßa bounding box shapes per class.
    """
    def __init__(self, num_classes=60):
        self.num_classes = num_classes
        self.shape_stats = {}  # {class_id: (mean, cov)}

    def fit(self, annotations):
        """
        Learn shape statistics t·ª´ training annotations.
        """
        for class_id in range(self.num_classes):
            class_boxes = [
                ann['bbox'] for ann in annotations
                if ann['category_id'] == class_id
            ]

            if len(class_boxes) < 10:
                continue

            # Extract shape features: [width, height, aspect_ratio, area]
            shapes = []
            for x, y, w, h in class_boxes:
                shapes.append([
                    w,
                    h,
                    w / (h + 1e-6),
                    w * h
                ])

            shapes = np.array(shapes)
            self.shape_stats[class_id] = {
                'mean': np.mean(shapes, axis=0),
                'cov': np.cov(shapes.T) + np.eye(4) * 1e-6,
                'count': len(shapes)
            }

    def compute_shape_likelihood(self, bbox, class_id):
        """
        Compute likelihood of bbox shape given class.
        """
        if class_id not in self.shape_stats:
            return 1.0

        x, y, w, h = bbox
        shape = np.array([w, h, w / (h + 1e-6), w * h])

        stats = self.shape_stats[class_id]
        mean = stats['mean']
        cov = stats['cov']

        # Mahalanobis distance
        diff = shape - mean
        try:
            mahal_dist = np.sqrt(diff @ np.linalg.inv(cov) @ diff)
        except:
            return 1.0

        # Convert to likelihood (chi-squared distribution)
        from scipy.stats import chi2
        likelihood = 1 - chi2.cdf(mahal_dist ** 2, df=4)

        return likelihood

    def refine_predictions(self, predictions):
        """
        Refine predictions using shape priors.
        """
        refined = []
        for pred in predictions:
            shape_likelihood = self.compute_shape_likelihood(
                pred['bbox'],
                pred['category_id']
            )

            # Adjust confidence based on shape consistency
            adjusted_score = pred['score'] * (0.5 + 0.5 * shape_likelihood)
            pred['score'] = adjusted_score
            refined.append(pred)

        return refined
```

---

## 3. Ki·∫øn Tr√∫c v√† Tri·ªÉn Khai

### 3.1 Ki·∫øn Tr√∫c T·ªïng Th·ªÉ

```mermaid
flowchart TB
    subgraph Input["Input Pipeline"]
        GEO[GeoTIFF Image<br/>3000√ó3000]
        GEO --> TILE[Tiling<br/>512√ó512]
        TILE --> PYRAMID[Image Pyramid<br/>4 scales]
    end

    subgraph Backbone["Feature Extraction"]
        PYRAMID --> RN[ResNet-101<br/>Pretrained]
        RN --> FPN[Feature Pyramid<br/>Network]
    end

    subgraph Detection["Detection Pipeline"]
        FPN --> RPN[Region Proposal<br/>Network]
        RPN --> ROI[RoI Align]
        ROI --> CLS[Classification<br/>60 classes]
        ROI --> BOX[Box Regression]
    end

    subgraph Pattern["Pattern Recognition Modules"]
        FPN --> CTX[Contextual<br/>Reasoning]
        CTX --> GRN[Graph<br/>Reasoning]
        GRN --> SSM[Shape<br/>Statistics]
    end

    subgraph Output["Output Processing"]
        CLS --> FUSE[Score Fusion]
        BOX --> FUSE
        SSM --> FUSE
        FUSE --> NMS[Soft-NMS]
        NMS --> MERGE[Tile Merge]
        MERGE --> FINAL[Final Output]
    end
```

### 3.2 Detector Architecture

```python
class USFPatternDetector(nn.Module):
    """
    USF Pattern Recognition-based detector.
    K·∫øt h·ª£p traditional pattern recognition v·ªõi deep learning.
    """
    def __init__(self, num_classes=60, backbone='resnet101'):
        super().__init__()

        # Backbone
        self.backbone = self._build_backbone(backbone)
        self.fpn = FeaturePyramidNetwork([256, 512, 1024, 2048], 256)

        # Detection heads
        self.rpn = RegionProposalNetwork(256, 6)  # 6 anchors per location
        self.roi_head = RoIHead(256, num_classes)

        # Pattern recognition modules
        self.context_module = ContextualReasoningModule(num_classes)
        self.shape_model = StatisticalShapeModel(num_classes)

    def _build_backbone(self, name):
        backbone = models.resnet101(pretrained=True)
        # Return feature maps at C2, C3, C4, C5
        return IntermediateLayerGetter(backbone, {
            'layer1': 'C2',
            'layer2': 'C3',
            'layer3': 'C4',
            'layer4': 'C5'
        })

    def forward(self, images, targets=None):
        # Feature extraction
        features = self.backbone(images)
        fpn_features = self.fpn(list(features.values()))

        # RPN
        proposals, rpn_losses = self.rpn(fpn_features, targets)

        # RoI Head
        detections, rcnn_losses = self.roi_head(
            fpn_features, proposals, targets
        )

        if self.training:
            return {**rpn_losses, **rcnn_losses}

        # Post-processing with pattern recognition
        refined_detections = self._apply_pattern_reasoning(
            detections, fpn_features
        )

        return refined_detections

    def _apply_pattern_reasoning(self, detections, features):
        """Apply pattern recognition refinements."""
        # Contextual reasoning
        for det in detections:
            # Get feature at detection location
            roi_features = self._extract_roi_features(features, det['bbox'])

            # Context-aware score adjustment
            context_score = self.context_module.score_context(
                roi_features, det['category_id']
            )
            det['score'] *= context_score

        # Shape-based refinement
        detections = self.shape_model.refine_predictions(detections)

        return detections
```

### 3.3 Multi-Scale Training Strategy

```python
class MultiScaleTrainer:
    """
    Training strategy v·ªõi multi-scale augmentation.
    """
    def __init__(self, model, optimizer, scales=[0.5, 0.75, 1.0, 1.25, 1.5]):
        self.model = model
        self.optimizer = optimizer
        self.scales = scales

    def train_step(self, images, targets):
        self.optimizer.zero_grad()

        # Randomly select scale
        scale = random.choice(self.scales)

        # Resize images and targets
        scaled_images, scaled_targets = self._apply_scale(
            images, targets, scale
        )

        # Forward pass
        losses = self.model(scaled_images, scaled_targets)
        total_loss = sum(losses.values())

        # Backward pass
        total_loss.backward()
        self.optimizer.step()

        return {k: v.item() for k, v in losses.items()}

    def _apply_scale(self, images, targets, scale):
        if scale == 1.0:
            return images, targets

        # Resize images
        h, w = images.shape[2:]
        new_h, new_w = int(h * scale), int(w * scale)
        scaled_images = F.interpolate(
            images, size=(new_h, new_w), mode='bilinear'
        )

        # Scale targets
        scaled_targets = []
        for target in targets:
            new_target = target.copy()
            new_target['boxes'] = target['boxes'] * scale
            scaled_targets.append(new_target)

        return scaled_images, scaled_targets
```

### 3.4 Data Augmentation Pipeline

```python
class USFAugmentationPipeline:
    """
    Augmentation pipeline d·ª±a tr√™n pattern recognition insights.
    """
    def __init__(self, training=True):
        self.training = training

        self.geometric_augs = A.Compose([
            # Rotation - important for aerial/satellite
            A.RandomRotate90(p=1.0),
            A.HorizontalFlip(p=0.5),
            A.VerticalFlip(p=0.5),

            # Scale variations
            A.RandomScale(scale_limit=0.3, p=0.5),

            # Affine transforms
            A.Affine(
                scale=(0.9, 1.1),
                translate_percent=(-0.1, 0.1),
                rotate=(-15, 15),
                shear=(-5, 5),
                p=0.3
            ),
        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))

        self.photometric_augs = A.Compose([
            A.RandomBrightnessContrast(p=0.5),
            A.HueSaturationValue(
                hue_shift_limit=10,
                sat_shift_limit=20,
                val_shift_limit=20,
                p=0.5
            ),
            A.RandomGamma(gamma_limit=(80, 120), p=0.3),

            # Satellite-specific
            A.RandomShadow(p=0.2),  # Simulate shadows
            A.RandomFog(fog_coef_lower=0.1, fog_coef_upper=0.3, p=0.1),
        ])

    def __call__(self, image, bboxes, labels):
        if not self.training:
            return image, bboxes, labels

        # Apply geometric transforms
        geo_result = self.geometric_augs(
            image=image, bboxes=bboxes, labels=labels
        )

        # Apply photometric transforms
        photo_result = self.photometric_augs(image=geo_result['image'])

        return photo_result['image'], geo_result['bboxes'], geo_result['labels']
```

---

## 4. Hu·∫•n Luy·ªán v√† T·ªëi ∆Øu

### 4.1 Training Configuration

```python
TRAINING_CONFIG = {
    # Model
    'backbone': 'resnet101',
    'pretrained': True,
    'num_classes': 60,

    # Optimizer
    'optimizer': 'SGD',
    'base_lr': 0.01,
    'momentum': 0.9,
    'weight_decay': 1e-4,

    # Learning rate schedule
    'lr_policy': 'step',
    'step_size': 8,
    'gamma': 0.1,
    'max_epochs': 24,
    'warmup_epochs': 1,

    # Batch size
    'batch_size': 8,  # per GPU
    'num_gpus': 4,

    # Data
    'tile_size': 512,
    'overlap': 100,
    'train_scales': [0.5, 0.75, 1.0, 1.25, 1.5],

    # RPN
    'rpn_positive_iou': 0.7,
    'rpn_negative_iou': 0.3,
    'rpn_batch_size': 256,
    'rpn_positive_ratio': 0.5,

    # R-CNN
    'rcnn_positive_iou': 0.5,
    'rcnn_batch_size': 512,
    'rcnn_positive_ratio': 0.25,
}
```

### 4.2 Loss Function Design

```python
class USFMultiTaskLoss(nn.Module):
    """
    Multi-task loss v·ªõi pattern-aware weighting.
    """
    def __init__(self, num_classes=60, gamma=2.0):
        super().__init__()
        self.num_classes = num_classes
        self.gamma = gamma

        # Learn class-specific loss weights
        self.class_weights = nn.Parameter(torch.ones(num_classes))

    def forward(self, predictions, targets):
        # RPN losses
        rpn_cls_loss = self.rpn_classification_loss(
            predictions['rpn_cls'], targets['rpn_labels']
        )
        rpn_reg_loss = self.rpn_regression_loss(
            predictions['rpn_reg'], targets['rpn_boxes']
        )

        # R-CNN losses
        rcnn_cls_loss = self.rcnn_classification_loss(
            predictions['rcnn_cls'], targets['rcnn_labels']
        )
        rcnn_reg_loss = self.rcnn_regression_loss(
            predictions['rcnn_reg'], targets['rcnn_boxes']
        )

        # Pattern consistency loss
        pattern_loss = self.pattern_consistency_loss(
            predictions['detections'], targets['objects']
        )

        return {
            'rpn_cls': rpn_cls_loss,
            'rpn_reg': rpn_reg_loss * 1.0,
            'rcnn_cls': rcnn_cls_loss,
            'rcnn_reg': rcnn_reg_loss * 1.0,
            'pattern': pattern_loss * 0.1
        }

    def rcnn_classification_loss(self, predictions, labels):
        """Focal loss v·ªõi learned class weights."""
        ce_loss = F.cross_entropy(predictions, labels, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_weight = (1 - pt) ** self.gamma

        # Apply learned class weights
        weights = F.softmax(self.class_weights, dim=0)[labels]

        return (focal_weight * ce_loss * weights).mean()

    def pattern_consistency_loss(self, detections, ground_truth):
        """
        Loss khuy·∫øn kh√≠ch consistent patterns.
        Penalize detections that violate learned co-occurrence.
        """
        if len(detections) == 0:
            return torch.tensor(0.0)

        loss = 0.0
        for det in detections:
            # Check if detection is consistent with nearby objects
            nearby_gts = self._find_nearby_objects(det, ground_truth)
            consistency = self._compute_consistency(det, nearby_gts)
            loss += (1 - consistency) * det['score']

        return loss / max(len(detections), 1)
```

### 4.3 Learning Rate Schedule

```mermaid
graph LR
    subgraph LR_Schedule["Learning Rate Schedule"]
        W[Warmup<br/>0‚Üí0.01<br/>1 epoch]
        S1[Step 1<br/>0.01<br/>8 epochs]
        S2[Step 2<br/>0.001<br/>8 epochs]
        S3[Step 3<br/>0.0001<br/>7 epochs]
    end

    W --> S1 --> S2 --> S3
```

### 4.4 Handling Class Imbalance

```python
class PatternAwareSampler:
    """
    Sampler d·ª±a tr√™n pattern complexity, kh√¥ng ch·ªâ class frequency.
    """
    def __init__(self, dataset, num_samples_per_epoch=10000):
        self.dataset = dataset
        self.num_samples = num_samples_per_epoch

        # Compute sampling weights
        self.weights = self._compute_weights()

    def _compute_weights(self):
        weights = []
        for idx in range(len(self.dataset)):
            sample = self.dataset.get_annotation(idx)

            # Base weight from class frequency
            class_weight = self._inverse_class_frequency(sample['classes'])

            # Pattern complexity bonus
            complexity = self._compute_complexity(sample)
            complexity_bonus = 1 + complexity * 0.5

            # Final weight
            weights.append(class_weight * complexity_bonus)

        return torch.tensor(weights)

    def _compute_complexity(self, sample):
        """
        Estimate sample complexity based on:
        - Number of objects
        - Diversity of classes
        - Object sizes
        - Spatial arrangement
        """
        num_objects = len(sample['boxes'])
        num_classes = len(set(sample['classes']))

        # Size diversity
        areas = [(b[2] - b[0]) * (b[3] - b[1]) for b in sample['boxes']]
        if len(areas) > 1:
            size_diversity = np.std(areas) / (np.mean(areas) + 1e-6)
        else:
            size_diversity = 0

        complexity = (
            0.3 * min(num_objects / 50, 1) +
            0.3 * min(num_classes / 10, 1) +
            0.4 * min(size_diversity, 1)
        )

        return complexity

    def __iter__(self):
        indices = torch.multinomial(
            self.weights,
            self.num_samples,
            replacement=True
        )
        return iter(indices.tolist())
```

### 4.5 Training Optimizations

```python
class TrainingOptimizations:
    """
    Collection of training optimizations.
    """
    @staticmethod
    def gradient_checkpointing(model):
        """Enable gradient checkpointing to save memory."""
        for name, module in model.named_modules():
            if isinstance(module, (nn.Conv2d, nn.Linear)):
                module.register_forward_hook(
                    lambda m, i, o: torch.utils.checkpoint.checkpoint_sequential(
                        [m], 1, i[0]
                    )
                )

    @staticmethod
    def mixed_precision_training(model, optimizer):
        """Setup automatic mixed precision."""
        scaler = torch.cuda.amp.GradScaler()

        def train_step(images, targets):
            with torch.cuda.amp.autocast():
                losses = model(images, targets)
                total_loss = sum(losses.values())

            scaler.scale(total_loss).backward()
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

            return {k: v.item() for k, v in losses.items()}

        return train_step

    @staticmethod
    def exponential_moving_average(model, decay=0.9999):
        """Create EMA model for stable evaluation."""
        ema_model = copy.deepcopy(model)

        def update_ema():
            with torch.no_grad():
                for p_ema, p_model in zip(
                    ema_model.parameters(),
                    model.parameters()
                ):
                    p_ema.copy_(decay * p_ema + (1 - decay) * p_model)

        return ema_model, update_ema
```

---

## 5. K·∫øt Qu·∫£ v√† Ph√¢n T√≠ch

### 5.1 Performance Summary

| Metric | Gi√° Tr·ªã | Ghi Ch√∫ |
|--------|---------|---------|
| **H·∫°ng Cu·ªëi** | ü•â H·∫°ng 3 | Trong 2,300+ b√†i n·ªôp |
| **mAP@0.5** | ~0.24 | ∆Ø·ªõc t√≠nh d·ª±a tr√™n th·ª© h·∫°ng |
| **Baseline Improvement** | ~150% | So v·ªõi DIUx baseline |
| **Inference Speed** | ~0.4s/tile | Tr√™n 4√ó V100 |

### 5.2 Ablation Study Results

| Component | mAP | Improvement |
|-----------|-----|-------------|
| Baseline Faster R-CNN | 0.18 | - |
| + Multi-scale Training | 0.20 | +0.02 |
| + Contextual Reasoning | 0.22 | +0.02 |
| + Shape Statistics | 0.23 | +0.01 |
| + Pattern Sampling | 0.24 | +0.01 |

### 5.3 Per-Class Analysis

```mermaid
graph TB
    subgraph Strong["Hi·ªáu Su·∫•t Cao (AP > 0.4)"]
        S1[Large Buildings - AP 0.65]
        S2[Storage Tanks - AP 0.58]
        S3[Planes - AP 0.52]
    end

    subgraph Medium["Trung B√¨nh (0.2 < AP < 0.4)"]
        M1[Trucks - AP 0.35]
        M2[Buses - AP 0.32]
        M3[Boats - AP 0.28]
    end

    subgraph Weak["Th·∫•p (AP < 0.2)"]
        W1[Small Cars - AP 0.15]
        W2[Containers - AP 0.12]
        W3[Rare classes - AP < 0.10]
    end
```

**Insights t·ª´ Pattern Recognition:**
1. **Objects with distinctive patterns** (buildings, tanks) ƒë∆∞·ª£c ph√°t hi·ªán t·ªët
2. **Objects in consistent contexts** (planes at airports) c√≥ recall cao
3. **Small, densely packed objects** (cars in parking lots) kh√≥ ph√¢n bi·ªát
4. **Rare classes** thi·∫øu patterns ƒë·ªÉ h·ªçc

### 5.4 Error Analysis

```python
class ErrorAnalysis:
    """
    Ph√¢n t√≠ch l·ªói theo pattern recognition perspective.
    """
    ERROR_TYPES = [
        'classification_error',   # Correct bbox, wrong class
        'localization_error',     # Correct class, poor bbox
        'duplicate_detection',    # Multiple detections same object
        'background_confusion',   # False positive from background
        'missed_detection',       # False negative
    ]

    def analyze(self, predictions, ground_truth):
        errors = {et: [] for et in self.ERROR_TYPES}

        # Match predictions to ground truth
        matches = self._hungarian_matching(predictions, ground_truth)

        for pred, gt_match in matches:
            if gt_match is None:
                # False positive
                if self._is_similar_to_background(pred):
                    errors['background_confusion'].append(pred)
                else:
                    errors['duplicate_detection'].append(pred)
            elif pred['category_id'] != gt_match['category_id']:
                errors['classification_error'].append((pred, gt_match))
            elif self._iou(pred['bbox'], gt_match['bbox']) < 0.5:
                errors['localization_error'].append((pred, gt_match))

        # Find missed detections
        for gt in ground_truth:
            if not self._is_matched(gt, matches):
                errors['missed_detection'].append(gt)

        return errors
```

**Common Error Patterns:**
| Error Type | Frequency | Primary Cause |
|------------|-----------|---------------|
| Missed Small Objects | 35% | Resolution limit |
| Class Confusion | 25% | Similar appearance |
| Duplicates | 20% | Tile overlap |
| Localization | 15% | Ambiguous boundaries |
| False Positives | 5% | Background clutter |

---

## 6. T√°i T·∫°o v√† T√†i Nguy√™n

### 6.1 Hardware Requirements

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| GPU | 2√ó GTX 1080 Ti | 4√ó Tesla V100 |
| VRAM | 11 GB/GPU | 32 GB/GPU |
| RAM | 64 GB | 256 GB |
| Storage | 500 GB SSD | 2 TB NVMe |
| CPU | 16 cores | 32+ cores |

### 6.2 Software Stack

```bash
# Create environment
conda create -n usf_xview python=3.8
conda activate usf_xview

# PyTorch + CUDA
conda install pytorch==1.10.0 torchvision==0.11.0 cudatoolkit=11.3 -c pytorch

# Detection framework
pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu113/torch1.10/index.html

# Additional dependencies
pip install \
    albumentations>=1.0.0 \
    rasterio>=1.2.0 \
    scipy>=1.7.0 \
    networkx>=2.6.0 \
    wandb>=0.12.0

# Pattern recognition tools
pip install scikit-learn scikit-image
```

### 6.3 Training Script

```bash
#!/bin/bash
# train_usf_detector.sh

# Configuration
export CUDA_VISIBLE_DEVICES=0,1,2,3
export OMP_NUM_THREADS=4

# Training
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    --master_port=29500 \
    train.py \
    --config configs/usf_pattern_detector.yaml \
    --output-dir outputs/usf_xview_run1 \
    --resume auto
```

### 6.4 Inference Pipeline

```python
class USFInferencePipeline:
    """
    Complete inference pipeline with pattern reasoning.
    """
    def __init__(self, model_path, config_path, device='cuda'):
        self.config = self._load_config(config_path)
        self.model = self._load_model(model_path)
        self.shape_model = self._load_shape_model()
        self.device = device

    def predict(self, image_path, confidence_threshold=0.3):
        # Load and tile image
        tiles = self._tile_image(image_path)

        all_detections = []
        for tile in tiles:
            # Run detector
            with torch.no_grad():
                dets = self.model([tile['tensor'].to(self.device)])

            # Apply pattern refinements
            refined = self.shape_model.refine_predictions(dets[0])

            # Convert to image coordinates
            for det in refined:
                det['bbox'] = self._tile_to_image_coords(
                    det['bbox'], tile['offset']
                )
                if det['score'] >= confidence_threshold:
                    all_detections.append(det)

        # Merge overlapping detections
        final = self._merge_detections(all_detections)

        return final

    def _merge_detections(self, detections, iou_threshold=0.5):
        """Merge detections from overlapping tiles."""
        if len(detections) == 0:
            return []

        # Group by class
        by_class = defaultdict(list)
        for det in detections:
            by_class[det['category_id']].append(det)

        merged = []
        for class_id, class_dets in by_class.items():
            boxes = torch.tensor([d['bbox'] for d in class_dets])
            scores = torch.tensor([d['score'] for d in class_dets])

            # Apply Soft-NMS
            keep_indices = self._soft_nms(boxes, scores, iou_threshold)

            for idx in keep_indices:
                merged.append(class_dets[idx])

        return merged
```

### 6.5 Academic Resources

**University of South Florida:**
- [Computer Science Department](https://www.usf.edu/engineering/cse/)
- [Prof. Sudeep Sarkar Profile](https://www.usf.edu/engineering/cse/people/sarkar-sudeep.aspx)
- [Computer Vision Lab](https://www.cse.usf.edu/cvlab/)

**Related Publications:**
- Pattern Analysis and Machine Intelligence (IEEE TPAMI)
- Computer Vision and Pattern Recognition (CVPR)
- International Conference on Computer Vision (ICCV)

**xView Challenge:**
- [Official Dataset](https://xviewdataset.org/)
- [Challenge Awards](https://challenge.xviewdataset.org/challenge-awards)

---

## H·∫°n Ch·∫ø c·ªßa T√†i Li·ªáu

‚ö†Ô∏è **L∆∞u √Ω quan tr·ªçng:**

Do kh√¥ng c√≥ t√†i li·ªáu c√¥ng khai chi ti·∫øt v·ªÅ gi·∫£i ph√°p n√†y, n·ªôi dung ƒë∆∞·ª£c x√¢y d·ª±ng d·ª±a tr√™n:
1. Chuy√™n m√¥n nghi√™n c·ª©u c·ªßa Prof. Sarkar v√† USF CV Lab
2. Ph∆∞∆°ng ph√°p pattern recognition ph·ªï bi·∫øn trong computer vision
3. Best practices t·ª´ c√°c cu·ªôc thi object detection t∆∞∆°ng t·ª±

ƒê·ªÉ c√≥ th√¥ng tin ch√≠nh x√°c, khuy·∫øn ngh·ªã li√™n h·ªá tr·ª±c ti·∫øp v·ªõi nh√≥m nghi√™n c·ª©u.

---

## Tr√≠ch D·∫´n

```bibtex
@misc{usf2018xview,
  title={xView Detection Challenge: University of South Florida Solution},
  author={Sarkar, Sudeep},
  howpublished={DIUx xView Detection Challenge},
  year={2018},
  note={Third Place Solution}
}
```

---

*T√†i li·ªáu c·∫≠p nh·∫≠t: 2025-12-19*
*Phi√™n b·∫£n: 2.0 - N√¢ng c·∫•p to√†n di·ªán v·ªõi pattern recognition framework*
