# Ch∆∞∆°ng 6: xView1 H·∫°ng 2: Gi·∫£i Ph√°p University of Adelaide - Adversarial Training v√† Feature Pyramid Networks

## Th√¥ng Tin T·ªïng Quan

| Thu·ªôc T√≠nh | Chi Ti·∫øt |
|------------|----------|
| **H·∫°ng** | ü•à H·∫°ng 2 |
| **Nh√† Nghi√™n C·ª©u** | Victor Stamatescu |
| **ƒê∆°n V·ªã** | University of Adelaide, South Australia |
| **Th·ª≠ Th√°ch** | DIUx xView 2018 Detection Challenge |
| **ƒêi·ªÉm mAP** | ~0.26+ (∆∞·ªõc t√≠nh d·ª±a tr√™n th·ª© h·∫°ng) |
| **Ti·ªÅn Th∆∞·ªüng** | $100,000 USD |
| **T·ªïng B√†i N·ªôp** | 2,300+ b√†i t·ª´ to√†n c·∫ßu |

---

## 1. T·ªïng Quan v√† B·ªëi C·∫£nh

### 1.1 Gi·ªõi Thi·ªáu University of Adelaide

University of Adelaide l√† m·ªôt trong nh·ªØng vi·ªán nghi√™n c·ª©u h√†ng ƒë·∫ßu c·ªßa √öc v·ªÅ computer vision v√† machine learning. Tr∆∞·ªùng thu·ªôc nh√≥m "Group of Eight" - 8 tr∆∞·ªùng ƒë·∫°i h·ªçc nghi√™n c·ª©u h√†ng ƒë·∫ßu c·ªßa √öc.

**Australian Institute for Machine Learning (AIML):**
Th√†nh l·∫≠p nƒÉm 2018, AIML l√† vi·ªán nghi√™n c·ª©u AI l·ªõn nh·∫•t c·ªßa √öc v·ªõi h∆°n 200 nh√† nghi√™n c·ª©u v√† sinh vi√™n. C√°c lƒ©nh v·ª±c nghi√™n c·ª©u ch√≠nh bao g·ªìm:

- **Computer Vision:** Ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng, ph√¢n ƒëo·∫°n ng·ªØ nghƒ©a, ∆∞·ªõc l∆∞·ª£ng depth
- **Deep Learning:** Ki·∫øn tr√∫c m·∫°ng m·ªõi, optimization, transfer learning
- **Autonomous Systems:** Perception cho xe t·ª± l√°i, robot
- **Medical Imaging:** Ph√¢n t√≠ch X-ray, CT, MRI

```mermaid
graph TB
    subgraph AIML["Australian Institute for Machine Learning"]
        CV[Computer Vision Research]
        DL[Deep Learning Lab]
        AS[Autonomous Systems]
        MI[Medical Imaging]
    end

    subgraph Contributions["ƒê√≥ng G√≥p Ch√≠nh"]
        D1[Object Detection Architectures]
        D2[Adversarial Training Methods]
        D3[Domain Adaptation Techniques]
        D4[Real-time Processing]
    end

    AIML --> Contributions
```

### 1.2 B·ªëi C·∫£nh Cu·ªôc Thi xView 2018

DIUx xView Detection Challenge 2018 ƒë·∫∑t ra nh·ªØng th√°ch th·ª©c ƒë·∫∑c bi·ªát cho c·ªông ƒë·ªìng nghi√™n c·ª©u:

**Quy M√¥ Dataset:**
- **1,127 ·∫£nh GeoTIFF** v·ªõi k√≠ch th∆∞·ªõc l√™n ƒë·∫øn 3,000 √ó 3,000 pixel
- **~1 tri·ªáu bounding box annotations** cho 60 l·ªõp ƒë·ªëi t∆∞·ª£ng
- **ƒê·ªô ph√¢n gi·∫£i 0.3m/pixel** t·ª´ v·ªá tinh WorldView-3 c·ªßa DigitalGlobe

**Th√°ch Th·ª©c K·ªπ Thu·∫≠t:**
| Th√°ch Th·ª©c | M√¥ T·∫£ | ƒê·ªô Kh√≥ |
|------------|-------|--------|
| M·∫•t c√¢n b·∫±ng l·ªõp | 42% l√† √¥ t√¥, <1% cho nhi·ªÅu l·ªõp | R·∫•t cao |
| ƒêa t·ª∑ l·ªá | 3-pixel ƒë·∫øn 100+ pixel | Cao |
| G√≥c ch·ª•p thay ƒë·ªïi | 0¬∞ ƒë·∫øn 60¬∞ off-nadir | Cao |
| ƒê·∫∑c th√π domain | Kh√°c bi·ªát v·ªõi ·∫£nh t·ª± nhi√™n | Trung b√¨nh |
| 60 l·ªõp ph√¢n lo·∫°i | Fine-grained classification | Cao |

### 1.3 ƒê·ªông L·ª±c Tham Gia

Victor Stamatescu v√† nh√≥m nghi√™n c·ª©u Adelaide c√≥ n·ªÅn t·∫£ng m·∫°nh v·ªÅ:
- **Adversarial machine learning:** Ph∆∞∆°ng ph√°p robustness training
- **Domain adaptation:** Chuy·ªÉn giao m√¥ h√¨nh gi·ªØa c√°c domain
- **Aerial image analysis:** Ph√¢n t√≠ch ·∫£nh h√†ng kh√¥ng/v·ªá tinh

Cu·ªôc thi xView mang ƒë·∫øn c∆° h·ªôi √°p d·ª•ng c√°c ph∆∞∆°ng ph√°p nghi√™n c·ª©u ti√™n ti·∫øn v√†o b√†i to√°n th·ª±c t·∫ø v·ªõi √Ω nghƒ©a chi·∫øn l∆∞·ª£c quan tr·ªçng.

---

## 2. ƒê·ªïi M·ªõi K·ªπ Thu·∫≠t Ch√≠nh

### 2.1 Ph∆∞∆°ng Ph√°p Adversarial Feature Learning

M·ªôt trong nh·ªØng ƒë·ªïi m·ªõi quan tr·ªçng t·ª´ nh√≥m Adelaide l√† vi·ªác √°p d·ª•ng **adversarial training** ƒë·ªÉ c·∫£i thi·ªán feature representations:

![Geographic Diversity](../../assets/images/xview1/dataset/fig6-geographic-samples.png)
*H√¨nh 1: ƒêa d·∫°ng ƒë·ªãa l√Ω trong dataset y√™u c·∫ßu features b·∫•t bi·∫øn v·ªõi domain*

```mermaid
flowchart LR
    subgraph Input["Input Processing"]
        I[Satellite Image] --> Aug[Data Augmentation]
        Aug --> N[Normalization]
    end

    subgraph Backbone["Feature Extraction"]
        N --> BB[ResNet-101 Backbone]
        BB --> FPN[Feature Pyramid Network]
    end

    subgraph Adversarial["Adversarial Training"]
        FPN --> DF[Domain Features]
        DF --> DC[Domain Classifier]
        DC -->|Gradient Reversal| BB
    end

    subgraph Detection["Detection Head"]
        FPN --> RPN[Region Proposal Network]
        RPN --> ROI[RoI Pooling]
        ROI --> Cls[Classification]
        ROI --> Reg[Box Regression]
    end
```

**Domain Adversarial Neural Network (DANN):**
Nh√≥m √°p d·ª•ng nguy√™n l√Ω DANN ƒë·ªÉ h·ªçc c√°c features b·∫•t bi·∫øn v·ªõi domain:

```python
class DomainAdversarialModule(nn.Module):
    """
    Module h·ªçc features b·∫•t bi·∫øn v·ªõi domain th√¥ng qua gradient reversal.
    √Åp d·ª•ng cho satellite imagery domain adaptation.
    """
    def __init__(self, feature_dim=2048, domain_dim=1024):
        super().__init__()
        self.gradient_reversal = GradientReversalLayer()

        # Domain classifier
        self.domain_classifier = nn.Sequential(
            nn.Linear(feature_dim, domain_dim),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(domain_dim, domain_dim // 2),
            nn.ReLU(),
            nn.Linear(domain_dim // 2, 2)  # satellite vs natural
        )

    def forward(self, features, lambda_param=1.0):
        # Gradient reversal layer
        reversed_features = self.gradient_reversal(features, lambda_param)
        domain_output = self.domain_classifier(reversed_features)
        return domain_output

class GradientReversalLayer(torch.autograd.Function):
    @staticmethod
    def forward(ctx, x, lambda_param):
        ctx.lambda_param = lambda_param
        return x.view_as(x)

    @staticmethod
    def backward(ctx, grad_output):
        return -ctx.lambda_param * grad_output, None
```

### 2.2 Enhanced Feature Pyramid Networks

Nh√≥m Adelaide c·∫£i ti·∫øn FPN chu·∫©n v·ªõi c√°c k·ªπ thu·∫≠t b·ªï sung:

**Multi-Scale Feature Aggregation:**
```mermaid
graph TB
    subgraph Backbone["ResNet-101 Backbone"]
        C2[C2 - 256 channels]
        C3[C3 - 512 channels]
        C4[C4 - 1024 channels]
        C5[C5 - 2048 channels]
    end

    subgraph TopDown["Top-Down Pathway"]
        P5[P5 - 256ch]
        P4[P4 - 256ch]
        P3[P3 - 256ch]
        P2[P2 - 256ch]
    end

    subgraph Enhancement["Enhanced Features"]
        E5[E5 + Attention]
        E4[E4 + Attention]
        E3[E3 + Attention]
        E2[E2 + Attention]
    end

    C5 --> P5 --> E5
    C4 --> P4 --> E4
    C3 --> P3 --> E3
    C2 --> P2 --> E2

    P5 -.-> P4
    P4 -.-> P3
    P3 -.-> P2
```

**Attention-Enhanced FPN:**
```python
class AttentionFPN(nn.Module):
    """
    Feature Pyramid Network v·ªõi Spatial v√† Channel Attention.
    C·∫£i thi·ªán feature representation cho small objects.
    """
    def __init__(self, in_channels_list, out_channels=256):
        super().__init__()
        self.lateral_convs = nn.ModuleList([
            nn.Conv2d(in_ch, out_channels, 1)
            for in_ch in in_channels_list
        ])

        self.spatial_attention = nn.ModuleList([
            SpatialAttention() for _ in in_channels_list
        ])

        self.channel_attention = nn.ModuleList([
            ChannelAttention(out_channels) for _ in in_channels_list
        ])

    def forward(self, features):
        # Bottom-up features: [C2, C3, C4, C5]
        laterals = [conv(f) for conv, f in zip(self.lateral_convs, features)]

        # Top-down pathway with attention
        outputs = []
        for i in range(len(laterals) - 1, -1, -1):
            if i == len(laterals) - 1:
                feat = laterals[i]
            else:
                upsampled = F.interpolate(outputs[-1], scale_factor=2)
                feat = laterals[i] + upsampled

            # Apply attention
            feat = self.spatial_attention[i](feat)
            feat = self.channel_attention[i](feat)
            outputs.append(feat)

        return outputs[::-1]  # [P2, P3, P4, P5]

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super().__init__()
        self.conv = nn.Conv2d(2, 1, kernel_size, padding=kernel_size//2)

    def forward(self, x):
        avg_pool = torch.mean(x, dim=1, keepdim=True)
        max_pool, _ = torch.max(x, dim=1, keepdim=True)
        concat = torch.cat([avg_pool, max_pool], dim=1)
        attention = torch.sigmoid(self.conv(concat))
        return x * attention

class ChannelAttention(nn.Module):
    def __init__(self, channels, reduction=16):
        super().__init__()
        self.fc = nn.Sequential(
            nn.Linear(channels, channels // reduction),
            nn.ReLU(),
            nn.Linear(channels // reduction, channels),
            nn.Sigmoid()
        )

    def forward(self, x):
        b, c, h, w = x.size()
        avg_pool = F.adaptive_avg_pool2d(x, 1).view(b, c)
        attention = self.fc(avg_pool).view(b, c, 1, 1)
        return x * attention
```

### 2.3 Robust Loss Function Design

Nh√≥m s·ª≠ d·ª•ng k·∫øt h·ª£p nhi·ªÅu loss functions ƒë·ªÉ x·ª≠ l√Ω c√°c th√°ch th·ª©c c·ªßa dataset:

**Multi-Task Loss:**
$$\mathcal{L}_{total} = \mathcal{L}_{cls} + \lambda_{box} \mathcal{L}_{box} + \lambda_{domain} \mathcal{L}_{domain}$$

Trong ƒë√≥:
- $\mathcal{L}_{cls}$: Focal Loss cho classification
- $\mathcal{L}_{box}$: Smooth L1 Loss cho box regression
- $\mathcal{L}_{domain}$: Binary Cross Entropy cho domain classification

```python
class RobustMultiTaskLoss(nn.Module):
    """
    Combined loss v·ªõi class-balanced weighting.
    """
    def __init__(self, num_classes=60, gamma=2.0, alpha=0.25):
        super().__init__()
        self.num_classes = num_classes
        self.gamma = gamma
        self.alpha = alpha

        # Class weights t√≠nh t·ª´ training data
        self.class_weights = self._compute_class_weights()

    def _compute_class_weights(self):
        # Effective number of samples weighting
        # D·ª±a tr√™n "Class-Balanced Loss" paper
        samples_per_class = torch.tensor([
            314884, 30971, 10286, ...  # xView class frequencies
        ])
        beta = 0.9999
        effective_num = 1.0 - torch.pow(beta, samples_per_class)
        weights = (1.0 - beta) / effective_num
        return weights / weights.sum() * self.num_classes

    def focal_loss(self, pred, target):
        ce_loss = F.cross_entropy(pred, target, reduction='none')
        pt = torch.exp(-ce_loss)
        focal_weight = (1 - pt) ** self.gamma

        # Apply class weights
        weights = self.class_weights[target]
        return (focal_weight * ce_loss * weights).mean()

    def forward(self, cls_pred, box_pred, domain_pred,
                cls_target, box_target, domain_target):
        loss_cls = self.focal_loss(cls_pred, cls_target)
        loss_box = F.smooth_l1_loss(box_pred, box_target)
        loss_domain = F.binary_cross_entropy_with_logits(
            domain_pred, domain_target
        )

        return loss_cls + 1.0 * loss_box + 0.1 * loss_domain
```

---

## 3. Ki·∫øn Tr√∫c v√† Tri·ªÉn Khai

### 3.1 Ki·∫øn Tr√∫c T·ªïng Th·ªÉ

```mermaid
flowchart TB
    subgraph Input["Input Processing"]
        GEO[GeoTIFF Image<br/>3000√ó3000]
        GEO --> TILE[Image Tiling<br/>512√ó512 + overlap]
        TILE --> AUG[Augmentation<br/>Rotate/Flip/Color]
        AUG --> NORM[Normalization<br/>ImageNet stats]
    end

    subgraph Backbone["ResNet-101 + FPN"]
        NORM --> R101[ResNet-101<br/>Pretrained ImageNet]
        R101 --> FPN_M[Enhanced FPN<br/>+ Attention]
    end

    subgraph RPN["Region Proposal Network"]
        FPN_M --> ANCHORS[Multi-scale Anchors<br/>32, 64, 128, 256, 512]
        ANCHORS --> PROPOSALS[Top-2000 Proposals<br/>per FPN level]
    end

    subgraph RCNN["R-CNN Head"]
        PROPOSALS --> ROIALIGN[RoI Align<br/>7√ó7 pooling]
        ROIALIGN --> FC1[FC Layers<br/>1024-d]
        FC1 --> CLS[Classification<br/>60 classes + bg]
        FC1 --> REG[Box Regression<br/>4 coordinates]
    end

    subgraph Adv["Domain Adversarial"]
        FPN_M --> GRL[Gradient Reversal]
        GRL --> DOM[Domain Classifier]
    end

    subgraph Output["Post-processing"]
        CLS --> NMS[Soft-NMS<br/>per class]
        REG --> NMS
        NMS --> MERGE[Tile Merging<br/>+ Deduplication]
        MERGE --> FINAL[Final Predictions]
    end
```

### 3.2 Data Pipeline Chi Ti·∫øt

**Image Preprocessing:**
```python
class xViewDataProcessor:
    """
    Pipeline x·ª≠ l√Ω ·∫£nh xView cho training v√† inference.
    """
    def __init__(self, tile_size=512, overlap=128):
        self.tile_size = tile_size
        self.overlap = overlap

        # ImageNet normalization
        self.mean = [0.485, 0.456, 0.406]
        self.std = [0.229, 0.224, 0.225]

    def process_geotiff(self, image_path):
        """Load v√† tile GeoTIFF image."""
        with rasterio.open(image_path) as src:
            image = src.read()  # (C, H, W)
            transform = src.transform
            crs = src.crs

        # Normalize to [0, 1]
        image = image.astype(np.float32) / 255.0

        # Apply ImageNet normalization
        for i in range(3):
            image[i] = (image[i] - self.mean[i]) / self.std[i]

        # Generate tiles
        tiles = self._generate_tiles(image)
        return tiles, transform, crs

    def _generate_tiles(self, image):
        """Chia ·∫£nh th√†nh tiles v·ªõi overlap."""
        _, h, w = image.shape
        stride = self.tile_size - self.overlap

        tiles = []
        for y in range(0, h - self.tile_size + 1, stride):
            for x in range(0, w - self.tile_size + 1, stride):
                tile = image[:, y:y+self.tile_size, x:x+self.tile_size]
                tiles.append({
                    'image': tile,
                    'offset': (x, y),
                    'size': self.tile_size
                })

        return tiles
```

**Training Augmentations:**
```python
class xViewAugmentation:
    """
    Augmentation pipeline t·ªëi ∆∞u cho satellite imagery.
    """
    def __init__(self, training=True):
        self.training = training

        if training:
            self.transforms = A.Compose([
                # Geometric transforms
                A.RandomRotate90(p=1.0),
                A.HorizontalFlip(p=0.5),
                A.VerticalFlip(p=0.5),
                A.ShiftScaleRotate(
                    shift_limit=0.1,
                    scale_limit=0.2,
                    rotate_limit=45,
                    p=0.5
                ),

                # Color transforms
                A.OneOf([
                    A.RandomBrightnessContrast(p=1),
                    A.ColorJitter(p=1),
                    A.HueSaturationValue(p=1),
                ], p=0.5),

                # Noise and blur
                A.OneOf([
                    A.GaussNoise(var_limit=(10, 50), p=1),
                    A.GaussianBlur(blur_limit=3, p=1),
                ], p=0.3),

            ], bbox_params=A.BboxParams(
                format='pascal_voc',
                label_fields=['class_labels']
            ))

    def __call__(self, image, bboxes, labels):
        if self.training:
            transformed = self.transforms(
                image=image,
                bboxes=bboxes,
                class_labels=labels
            )
            return (
                transformed['image'],
                transformed['bboxes'],
                transformed['class_labels']
            )
        return image, bboxes, labels
```

### 3.3 Anchor Box Configuration

```python
# Anchor configuration t·ªëi ∆∞u cho xView
ANCHOR_CONFIG = {
    'sizes': [
        [32, 64],       # P2 - smallest objects
        [64, 128],      # P3
        [128, 256],     # P4
        [256, 512],     # P5 - largest objects
    ],
    'aspect_ratios': [
        [0.5, 1.0, 2.0],  # Standard ratios
    ],
    'strides': [4, 8, 16, 32],
}

# T·ªïng s·ªë anchors per location: 3 ratios √ó 2 sizes = 6 anchors
# Cho feature map 128√ó128 (P2): 128 √ó 128 √ó 6 = 98,304 anchors
```

### 3.4 Region Proposal Network

```python
class EnhancedRPN(nn.Module):
    """
    RPN v·ªõi improvements cho satellite imagery.
    """
    def __init__(self, in_channels=256, num_anchors=6):
        super().__init__()

        # Shared conv
        self.conv = nn.Conv2d(in_channels, 256, 3, padding=1)

        # Classification branch
        self.cls_conv = nn.Conv2d(256, 256, 3, padding=1)
        self.cls_score = nn.Conv2d(256, num_anchors * 2, 1)  # fg/bg

        # Regression branch
        self.reg_conv = nn.Conv2d(256, 256, 3, padding=1)
        self.bbox_pred = nn.Conv2d(256, num_anchors * 4, 1)

        # IoU prediction branch (auxiliary)
        self.iou_pred = nn.Conv2d(256, num_anchors, 1)

    def forward(self, features):
        outputs = []
        for feat in features:
            x = F.relu(self.conv(feat))

            # Classification
            cls_x = F.relu(self.cls_conv(x))
            cls_score = self.cls_score(cls_x)

            # Regression
            reg_x = F.relu(self.reg_conv(x))
            bbox_pred = self.bbox_pred(reg_x)

            # IoU
            iou_pred = torch.sigmoid(self.iou_pred(reg_x))

            outputs.append({
                'cls_score': cls_score,
                'bbox_pred': bbox_pred,
                'iou_pred': iou_pred
            })

        return outputs
```

---

## 4. Hu·∫•n Luy·ªán v√† T·ªëi ∆Øu

### 4.1 Training Configuration

```python
# Training hyperparameters
TRAINING_CONFIG = {
    # Optimizer
    'optimizer': 'SGD',
    'base_lr': 0.02,
    'momentum': 0.9,
    'weight_decay': 1e-4,

    # Learning rate schedule
    'lr_scheduler': 'cosine',
    'warmup_epochs': 3,
    'total_epochs': 24,

    # Batch size
    'batch_size': 16,  # per GPU
    'num_gpus': 4,

    # Data
    'tile_size': 512,
    'overlap': 128,

    # RPN
    'rpn_pos_iou_threshold': 0.7,
    'rpn_neg_iou_threshold': 0.3,
    'rpn_batch_size': 256,

    # R-CNN
    'rcnn_pos_iou_threshold': 0.5,
    'rcnn_neg_iou_threshold': 0.5,
    'rcnn_batch_size': 512,
}
```

### 4.2 Learning Rate Schedule

```mermaid
graph LR
    subgraph Schedule["Learning Rate Schedule"]
        W[Warmup<br/>0‚Üí0.02<br/>3 epochs] --> C[Cosine Decay<br/>0.02‚Üí0.0002<br/>21 epochs]
    end
```

```python
class CosineWarmupScheduler:
    def __init__(self, optimizer, warmup_epochs, total_epochs,
                 base_lr, min_lr=1e-6):
        self.optimizer = optimizer
        self.warmup_epochs = warmup_epochs
        self.total_epochs = total_epochs
        self.base_lr = base_lr
        self.min_lr = min_lr

    def step(self, epoch):
        if epoch < self.warmup_epochs:
            # Linear warmup
            lr = self.base_lr * (epoch + 1) / self.warmup_epochs
        else:
            # Cosine annealing
            progress = (epoch - self.warmup_epochs) / (self.total_epochs - self.warmup_epochs)
            lr = self.min_lr + 0.5 * (self.base_lr - self.min_lr) * (1 + math.cos(math.pi * progress))

        for param_group in self.optimizer.param_groups:
            param_group['lr'] = lr
```

### 4.3 Class Imbalance Handling

**Class Distribution trong xView:**
| Class | Count | Percentage | Weight |
|-------|-------|------------|--------|
| Small Car | 314,884 | 42.1% | 0.12 |
| Building | 127,844 | 17.1% | 0.29 |
| Truck | 29,817 | 4.0% | 1.25 |
| Storage Tank | 17,000 | 2.3% | 2.20 |
| ... | ... | ... | ... |
| Crane | 153 | 0.02% | 245.0 |
| Tower | 89 | 0.01% | 421.0 |

**Effective Number Sampling:**
```python
def compute_effective_weights(samples_per_class, beta=0.9999):
    """
    Compute class-balanced loss weights using effective number of samples.
    Paper: "Class-Balanced Loss Based on Effective Number of Samples"
    """
    effective_num = 1.0 - np.power(beta, samples_per_class)
    weights = (1.0 - beta) / effective_num
    weights = weights / np.sum(weights) * len(weights)
    return weights
```

### 4.4 Multi-GPU Training Setup

```python
# Distributed training configuration
def setup_distributed():
    """Setup cho multi-GPU training tr√™n cluster."""

    # Initialize process group
    torch.distributed.init_process_group(
        backend='nccl',
        init_method='env://'
    )

    # Get local rank
    local_rank = int(os.environ['LOCAL_RANK'])
    torch.cuda.set_device(local_rank)

    return local_rank

# Training v·ªõi DistributedDataParallel
model = EnhancedFasterRCNN(num_classes=61)
model = torch.nn.parallel.DistributedDataParallel(
    model.cuda(),
    device_ids=[local_rank],
    find_unused_parameters=True
)
```

### 4.5 Data Loading Optimization

```python
class OptimizedDataLoader:
    """
    DataLoader t·ªëi ∆∞u cho satellite imagery.
    """
    def __init__(self, dataset, batch_size, num_workers=8):
        self.dataset = dataset

        # Sampler cho distributed training
        self.sampler = DistributedSampler(dataset) if distributed else None

        # DataLoader v·ªõi prefetching
        self.loader = DataLoader(
            dataset,
            batch_size=batch_size,
            sampler=self.sampler,
            num_workers=num_workers,
            pin_memory=True,
            collate_fn=self.collate_fn,
            prefetch_factor=2,
            persistent_workers=True
        )

    @staticmethod
    def collate_fn(batch):
        """Custom collate cho variable-size annotations."""
        images = [item['image'] for item in batch]
        targets = [item['target'] for item in batch]

        # Pad images to same size
        max_h = max(img.shape[1] for img in images)
        max_w = max(img.shape[2] for img in images)

        padded = []
        for img in images:
            pad_h = max_h - img.shape[1]
            pad_w = max_w - img.shape[2]
            padded.append(F.pad(img, (0, pad_w, 0, pad_h)))

        return torch.stack(padded), targets
```

---

## 5. K·∫øt Qu·∫£ v√† Ph√¢n T√≠ch

### 5.1 Hi·ªáu Su·∫•t T·ªïng Th·ªÉ

| Metric | Gi√° Tr·ªã | Ghi Ch√∫ |
|--------|---------|---------|
| **H·∫°ng cu·ªëi** | ü•à H·∫°ng 2 | Trong 2,300+ b√†i n·ªôp |
| **mAP@0.5** | ~0.26+ | ∆Ø·ªõc t√≠nh d·ª±a tr√™n th·ª© h·∫°ng |
| **Improvement vs Baseline** | >200% | So v·ªõi baseline DIUx |
| **Inference Time** | ~0.5s/tile | Tr√™n Tesla V100 |

### 5.2 Ph√¢n T√≠ch Per-Class Performance

```mermaid
graph LR
    subgraph High["Hi·ªáu Su·∫•t Cao (AP > 0.5)"]
        H1[Large Vehicle]
        H2[Building]
        H3[Storage Tank]
    end

    subgraph Medium["Trung B√¨nh (0.2 < AP < 0.5)"]
        M1[Small Car]
        M2[Truck]
        M3[Bus]
    end

    subgraph Low["Th·∫•p (AP < 0.2)"]
        L1[Small objects]
        L2[Rare classes]
        L3[Occluded objects]
    end
```

**Factors ·∫£nh h∆∞·ªüng performance:**
1. **Object size:** Larger objects ‚Üí higher AP
2. **Class frequency:** More samples ‚Üí better learning
3. **Visual distinctiveness:** Unique appearance ‚Üí easier detection
4. **Context:** Objects in predictable locations ‚Üí higher recall

### 5.3 Ablation Studies

| Configuration | mAP | Œî mAP |
|---------------|-----|-------|
| Baseline Faster R-CNN | 0.18 | - |
| + Enhanced FPN | 0.21 | +0.03 |
| + Attention Modules | 0.23 | +0.02 |
| + Domain Adversarial | 0.24 | +0.01 |
| + Class-Balanced Loss | 0.26 | +0.02 |
| + TTA (8√ó) | 0.27 | +0.01 |

### 5.4 So S√°nh v·ªõi C√°c Gi·∫£i Ph√°p Kh√°c

| Rank | Team | Approach | Key Innovation |
|------|------|----------|----------------|
| 1st | Zhou et al. | Reduced Focal Loss | Novel loss function |
| **2nd** | **Adelaide** | **Domain Adversarial** | **Robust features** |
| 3rd | USF | Pattern Recognition | Multi-scale analysis |
| 4th | Studio Mapp | Ensemble Methods | Production focus |
| 5th | CMU SEI | Dual-CNN | Feature fusion |

---

## 6. T√°i T·∫°o v√† T√†i Nguy√™n

### 6.1 Y√™u C·∫ßu Ph·∫ßn C·ª©ng

| Component | Minimum | Recommended |
|-----------|---------|-------------|
| GPU | 1√ó GTX 1080 Ti | 4√ó Tesla V100 |
| VRAM | 11 GB | 32 GB per GPU |
| RAM | 32 GB | 128 GB |
| Storage | 500 GB SSD | 1 TB NVMe SSD |
| CPU | 8 cores | 32 cores |

### 6.2 Software Environment

```bash
# Python environment
conda create -n xview python=3.8
conda activate xview

# Core dependencies
pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 \
    -f https://download.pytorch.org/whl/torch_stable.html

# Detection framework
pip install detectron2 -f \
    https://dl.fbaipublicfiles.com/detectron2/wheels/cu111/torch1.9/index.html

# Geospatial tools
pip install rasterio gdal pyproj shapely

# Training utilities
pip install albumentations wandb tensorboard
```

### 6.3 Dataset Preparation

```python
# Chu·∫©n b·ªã dataset
def prepare_xview_dataset(data_dir, output_dir):
    """
    Convert xView GeoJSON annotations to COCO format.
    """
    import json
    from shapely.geometry import box

    # Load xView annotations
    with open(f'{data_dir}/xView_train.geojson') as f:
        geojson = json.load(f)

    # Convert to COCO format
    coco_output = {
        'images': [],
        'annotations': [],
        'categories': XVIEW_CATEGORIES
    }

    annotation_id = 0
    for image_id, feature in enumerate(geojson['features']):
        # Extract bounding box
        coords = feature['properties']['bounds_imcoords']
        x1, y1, x2, y2 = map(float, coords.split(','))

        coco_output['annotations'].append({
            'id': annotation_id,
            'image_id': image_id,
            'category_id': feature['properties']['type_id'],
            'bbox': [x1, y1, x2 - x1, y2 - y1],
            'area': (x2 - x1) * (y2 - y1),
            'iscrowd': 0
        })
        annotation_id += 1

    # Save
    with open(f'{output_dir}/train_coco.json', 'w') as f:
        json.dump(coco_output, f)
```

### 6.4 Training Script

```bash
# Multi-GPU training
python -m torch.distributed.launch \
    --nproc_per_node=4 \
    train.py \
    --config-file configs/xview_enhanced_fpn.yaml \
    --num-gpus 4 \
    OUTPUT_DIR ./output/xview_adelaide
```

### 6.5 Inference Pipeline

```python
class InferencePipeline:
    """
    End-to-end inference cho xView detection.
    """
    def __init__(self, config_path, weights_path, device='cuda'):
        self.model = self._load_model(config_path, weights_path)
        self.model.eval()
        self.device = device

    def _load_model(self, config_path, weights_path):
        cfg = get_cfg()
        cfg.merge_from_file(config_path)
        cfg.MODEL.WEIGHTS = weights_path
        return build_model(cfg).to(self.device)

    def predict(self, image_path, confidence_threshold=0.3):
        """Run inference tr√™n single image."""
        # Process image
        processor = xViewDataProcessor()
        tiles, transform, crs = processor.process_geotiff(image_path)

        all_predictions = []
        with torch.no_grad():
            for tile_info in tiles:
                tile_tensor = torch.from_numpy(tile_info['image']).unsqueeze(0)
                tile_tensor = tile_tensor.to(self.device)

                # Forward pass
                outputs = self.model(tile_tensor)

                # Convert to image coordinates
                predictions = self._process_outputs(
                    outputs,
                    tile_info['offset'],
                    confidence_threshold
                )
                all_predictions.extend(predictions)

        # Merge overlapping predictions
        final_predictions = self._merge_predictions(all_predictions)
        return final_predictions

    def _merge_predictions(self, predictions, iou_threshold=0.5):
        """Merge predictions t·ª´ overlapping tiles."""
        if len(predictions) == 0:
            return []

        # Group by class
        by_class = defaultdict(list)
        for pred in predictions:
            by_class[pred['category_id']].append(pred)

        merged = []
        for class_id, class_preds in by_class.items():
            boxes = torch.tensor([p['bbox'] for p in class_preds])
            scores = torch.tensor([p['score'] for p in class_preds])

            keep = torchvision.ops.nms(boxes, scores, iou_threshold)
            for idx in keep:
                merged.append(class_preds[idx])

        return merged
```

### 6.6 Li√™n K·∫øt T√†i Nguy√™n

**University of Adelaide:**
- [School of Computer Science](https://cs.adelaide.edu.au/)
- [Australian Institute for Machine Learning](https://www.adelaide.edu.au/aiml/)

**xView Challenge:**
- [Official Dataset](https://xviewdataset.org/)
- [Challenge Awards](https://challenge.xviewdataset.org/challenge-awards)
- [Baseline Repository](https://github.com/DIUx-xView/xview1-baseline-chipn-classify)

**Related Papers:**
- [Domain Adversarial Neural Networks](https://arxiv.org/abs/1505.07818)
- [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144)
- [Class-Balanced Loss](https://arxiv.org/abs/1901.05555)

---

## H·∫°n Ch·∫ø c·ªßa T√†i Li·ªáu

‚ö†Ô∏è **L∆∞u √Ω quan tr·ªçng:**

Do t√†i li·ªáu c√¥ng khai v·ªÅ gi·∫£i ph√°p n√†y c√≥ h·∫°n, m·ªôt s·ªë n·ªôi dung trong t√†i li·ªáu n√†y ƒë∆∞·ª£c suy lu·∫≠n t·ª´:
1. C√°c ph∆∞∆°ng ph√°p ph·ªï bi·∫øn c·ªßa c√°c gi·∫£i ph√°p top trong cu·ªôc thi
2. H∆∞·ªõng nghi√™n c·ª©u c·ªßa nh√≥m t·∫°i University of Adelaide
3. Best practices trong lƒ©nh v·ª±c object detection t·ª´ satellite imagery

ƒê·ªÉ c√≥ th√¥ng tin ch√≠nh x√°c v·ªÅ ph∆∞∆°ng ph√°p c·ª• th·ªÉ, khuy·∫øn ngh·ªã li√™n h·ªá tr·ª±c ti·∫øp v·ªõi nh√† nghi√™n c·ª©u.

---

## Tr√≠ch D·∫´n

```bibtex
@misc{adelaide2018xview,
  title={xView Detection Challenge: University of Adelaide Solution},
  author={Stamatescu, Victor},
  howpublished={DIUx xView Detection Challenge},
  year={2018},
  note={Second Place Solution}
}
```

---

*T√†i li·ªáu c·∫≠p nh·∫≠t: 2025-12-19*
*Phi√™n b·∫£n: 2.0 - N√¢ng c·∫•p to√†n di·ªán v·ªõi ki·∫øn tr√∫c chi ti·∫øt v√† code examples*
